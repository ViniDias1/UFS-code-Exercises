"id","issue_id","github_id","number","title","body","state","tema_relacionado","labels","comments_count","created_at","updated_at","closed_at","user_login","assignee","milestone","html_url","tempo_resolucao_dias"
"14","2781522863","2781522863","144604","Fix broken YAML template after #144574","The YAML syntax is wrong and GitHub complains about it https://github.com/pytorch/pytorch/blob/main/.github/ISSUE_TEMPLATE/pt2-bug-report.yml","closed","Refatoração","{Merged,""topic: not user facing"",test-config/default}",3,"2025-01-11 02:05:16","2025-01-11 05:10:12","2025-01-11 05:09:10","huydhn",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144604",0
"33","2678306271","2678306271","141215","Rewrite FloorDiv  Expands (x + y) // b into x // b + y // b in iterative way","````
        # Expands (x + y) // b into x // b + y // b.
        # This only works if floor is an identity, i.e. x / b is an integer.
        for term in sympy.Add.make_args(base):
            quotient = term / divisor
            if quotient.is_integer and isinstance(divisor, sympy.Integer):
                # NB: this is correct even if the divisor is not an integer, but it
                # creates rational expressions that cause problems with dynamic
                # shapes.
                return FloorDiv(base - term, divisor) + quotient
````

this part is causing max depth recursion error. 
We should rewrite it in iterative way for both performance and to fix the issue.
when this happen we have a long expression
3*s1 + 3*s2 + 3*s4 .......X times
we end up calling the constructor FloorDiv X times .

What we shall do instead is found all terms that satisfy      quotient = term / divisor and quotient.is_integer 
and do base - {all those terms } all at once. 

Furthermore, we can do some caching potentially here, 
the pattern here is 

3*s1/3 
..
(3*s1 + 3*s2 )/ 3
...
(3*s1 + 3*s2+ 3*s4 )/ 3
....

this address https://github.com/pytorch/pytorch/issues/138729


repo:
run the following on top of https://github.com/pytorch/pytorch/pull/140882
````

import time
from torch._inductor.utils import clear_inductor_caches, fresh_inductor_cache

import torch
import sys
print(sys.getrecursionlimit())


if __name__ == ""__main__"":
    def make_val(n=10):
        values = torch.arange(random.randint(1, 1000), dtype=torch.float)
        return values

    @torch.compile(fullgraph=True, dynamic=True)
    def consolidate(tensors):
        start = 0
        lengths = []
        swaps = []
        origs = []


        def view_and_pad(tensor: torch.Tensor, lengths=lengths) -> torch.Tensor:
            nonlocal start

            origs.append(tensor)
            swap = tensor.view(-1).view(torch.uint8)
            # result must always have a multiple of 8 elements
            pad = swap.numel() % 8
            swap = torch.cat([swap, swap.new_zeros(8 - pad)])

            n = swap.numel()
            start = start + n
            lengths.append(n)
            swaps.append(swap)


        for val in list(tensors):
            view_and_pad(val)

        filesize = start
        storage = torch.empty(
            filesize,
            dtype=torch.uint8,
        )
        torch.cat(swaps, out=storage)
        swaps = storage.split(lengths)


        def view_old_as_new(
            v: torch.Tensor, oldv: torch.Tensor
        ) -> torch.Tensor:
            if oldv is None:
                return v
            v = v.view(oldv.dtype)
            if v.numel() > oldv.numel():
                return v[: oldv.numel()].view(oldv.shape)
            return v.view(oldv.shape)

        result = [
            view_old_as_new(v, oldv)
            for (v, oldv) in zip(swaps, origs, strict=True)
        ]
        return result

    with fresh_inductor_cache():
        for n in [265,]:
            torch.compiler.reset()
            tensors = [make_val() for _ in range(n)]
            t0 = time.time()
            results = consolidate(tensors)
            print(n, time.time()-t0)

        assert len(set(t.untyped_storage().data_ptr() for t in results)) == 1

````
cc @chauhang @penguinwu @ezyang @bobrenjc93","closed","Refatoração","{triaged,""oncall: pt2"",""module: dynamic shapes""}",0,"2024-11-21 07:04:32","2024-12-06 22:31:52","2024-12-06 22:31:52","laithsakka","laithsakka",NULL,"https://github.com/pytorch/pytorch/issues/141215",15
"43","2645725735","2645725735","140209","[doc] fix grammar in ""Extending Torch""",NULL,"closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-11-09 06:53:07","2024-12-14 02:11:57","2024-11-13 05:34:45","ssnl",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140209",4
"70","2743339496","2743339496","143316","Enable swap on all Linux jobs","A swapfile on Linux runner has been prepared by https://github.com/pytorch/test-infra/pull/6058.  So this PR does 2 things:

* Start using the swapfile on all Linux build and test jobs
* Testing the rollout https://github.com/pytorch-labs/pytorch-gha-infra/pull/582

### Testing

Run `swapon` inside the container and the swapfile shows up correctly:

```
jenkins@259dfb0a314c:~/workspace$ swapon
NAME      TYPE SIZE USED PRIO
/swapfile file   3G 256K   -2
```","closed","Refatoração","{Merged,""topic: not user facing"",test-config/default,no-runner-experiments}",3,"2024-12-16 20:30:39","2024-12-17 02:13:28","2024-12-17 02:12:26","huydhn",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143316",1
"81","2788147618","2788147618","144792","massive number of runtime asserts can hamper compile times","internal xref: https://fb.workplace.com/groups/1075192433118967/permalink/1585006445470894/

We're spending ~2 hours in one job cranking out a few thousand runtime asserts. The relevant bit is this section of the logs:
```
#  .... 4000 lines of runtime asserts
[trainers0]:[rank0]:I0114 06:02:10.688613 5302 torch/fx/experimental/symbolic_shapes.py:6328] [6/0] runtime_assert u0 + u1 + u10 + u100 + u1000 +
[trainers0]:[rank0]:I0114 06:02:25.990159 5302 torch/fx/experimental/symbolic_shapes.py:6328] [6/0] runtime_assert u0 + u1 + u10 + u100 + u1000 +
[trainers0]:[rank0]:I0114 06:02:41.910004 5302 torch/fx/experimental/symbolic_shapes.py:6328] [6/0] runtime_assert u0 + u1 + u10 + u100 + u1000 +
[trainers0]:[rank0]:I0114 06:02:57.359968 5302 torch/fx/experimental/symbolic_shapes.py:6328] [6/0] runtime_assert u0 + u1 + u10 + u100 + u1000 +
[trainers0]:[rank0]:I0114 06:03:12.918750 5302 torch/fx/experimental/symbolic_shapes.py:6328] [6/0] runtime_assert u0 + u1 + u10 + u100 + u1000 +
[trainers0]:[rank0]:I0114 06:03:28.610256 5302 torch/fx/experimental/symbolic_shapes.py:6328] [6/0] runtime_assert u0 + u1 + u10 + u100 + u1000 +
```

full paste here: P1712262166

you can see ~15 seconds between each log, indicating that once we have a few thousand runtime asserts, there is probably some quadratic behavior going on that slows compile times.

The runtime asserts themselves are also quite big and contain many symbols. The last one:
```
runtime_assert u0 + u1 + u10 + u100 + u1000 + u1001 + u1002 + u1003 + u1004 + u1005 + u1006 + u1007 + u1008 + u1009 + u101 + u1010 + u1011 + u1012 + u1013 + u1014 + u1015 + u1016 + u1017 + u1018 + u1019 + u102 + u1020 + u1021 + u1022 + u1023 + u1024 + u1025 + u1026 + u1027 + u1028 + u1029 + u103 + u1030 + u1031 + u1032 + u1033 + u1034 + u1035 + u1036 + u1037 + u1038 + u1039 + u104 + u1040 + u1041 + u1042 + u1043 + u1044 + u1045 + u1046 + u1047 + u1048 + u1049 + u105 + u1050 + u1051 + u1052 + u1053 + u1054 + u1055 + u1056 + u1057 + u1058 + u1059 + u106 + u1060 + u1061 + u1062 + u1063 + u1064 + u1065 + u1066 + u1067 + u1068 + u1069 + u107 + u1070 + u1071 + u1072 + u1073 + u1074 + u1075 + u1076 + u1077 + u1078 + u1079 + u108 + u1080 + u1081 + u1082 + u1083 + u1084 + u1085 + u1086 + u1087 + u1088 + u1089 + u109 + u1090 + u1091 + u1092 + u1093 + u1094 + u1095 + u1096 + u1097 + u1098 + u1099 + u11 + u110 + u1100 + u1101 + u1102 + u1103 + u1104 + u1105 + u1106 + u1107 + u1108 + u1109 + u111 + u1110 + u1111 + u1112 + u1113 + u1114 + u1115 + u1116 + u1117 + u1118 + u1119 + u112 + u1120 + u1121 + u1122 + u1123 + u1124 + u1125 + u1126 + u1127 + u1128 + u1129 + u113 + u1130 + u1131 + u1132 + u1133 + u1134 + u1135 + u1136 + u1137 + u1138 + u1139 + u114 + u1140 + u1141 + u1142 + u1143 + u1144 + u1145 + u1146 + u1147 + u1148 + u1149 + u115 + u1150 + u1151 + u1152 + u1153 + u1154 + u1155 + u1156 + u1157 + u1158 + u1159 + u116 + u1160 + u1161 + u1162 + u1163 + u1164 + u1165 + u117 + u118 + u119 + u12 + u120 + u121 + u122 + u123 + u124 + u125 + u126 + u127 + u128 + u129 + u13 + u130 + u131 + u132 + u133 + u134 + u135 + u136 + u137 + u138 + u139 + u14 + u140 + u141 + u142 + u143 + u144 + u145 + u146 + u147 + u148 + u149 + u15 + u150 + u151 + u152 + u153 + u154 + u155 + u156 + u157 + u158 + u159 + u16 + u160 + u161 + u162 + u163 + u164 + u165 + u166 + u167 + u168 + u169 + u17 + u170 + u171 + u172 + u173 + u174 + u175 + u176 + u177 + u178 + u179 + u18 + u180 + u181 + u182 + u183 + u184 + u185 + u186 + u187 + u188 + u189 + u19 + u190 + u191 + u192 + u193 + u194 + u195 + u196 + u197 + u198 + u199 + u2 + u20 + u200 + u201 + u202 + u203 + u204 + u205 + u206 + u207 + u208 + u209 + u21 + u210 + u211 + u212 + u213 + u214 + u215 + u216 + u217 + u218 + u219 + u22 + u220 + u221 + u222 + u223 + u224 + u225 + u226 + u227 + u228 + u229 + u23 + u230 + u231 + u232 + u233 + u234 + u235 + u236 + u237 + u238 + u239 + u24 + u240 + u241 + u242 + u243 + u244 + u245 + u246 + u247 + u248 + u249 + u25 + u250 + u251 + u252 + u253 + u254 + u255 + u256 + u257 + u258 + u259 + u26 + u260 + u261 + u262 + u263 + u264 + u265 + u266 + u267 + u268 + u269 + u27 + u270 + u271 + u272 + u273 + u274 + u275 + u276 + u277 + u278 + u279 + u28 + u280 + u281 + u282 + u283 + u284 + u285 + u286 + u287 + u288 + u289 + u29 + u290 + u291 + u292 + u293 + u294 + u295 + u296 + u297 + u298 + u299 + u3 + u30 + u300 + u301 + u302 + u303 + u304 + u305 + u306 + u307 + u308 + u309 + u31 + u310 + u311 + u312 + u313 + u314 + u315 + u316 + u317 + u318 + u319 + u32 + u320 + u321 + u322 + u323 + u324 + u325 + u326 + u327 + u328 + u329 + u33 + u330 + u331 + u332 + u333 + u334 + u335 + u336 + u337 + u338 + u339 + u34 + u340 + u341 + u342 + u343 + u344 + u345 + u346 + u347 + u348 + u349 + u35 + u350 + u351 + u352 + u353 + u354 + u355 + u356 + u357 + u358 + u359 + u36 + u360 + u361 + u362 + u363 + u364 + u365 + u366 + u367 + u368 + u369 + u37 + u370 + u371 + u372 + u373 + u374 + u375 + u376 + u377 + u378 + u379 + u38 + u380 + u381 + u382 + u383 + u384 + u385 + u386 + u387 + u388 + u389 + u39 + u390 + u391 + u392 + u393 + u394 + u395 + u396 + u397 + u398 + u399 + u4 + u40 + u400 + u401 + u402 + u403 + u404 + u405 + u406 + u407 + u408 + u409 + u41 + u410 + u411 + u412 + u413 + u414 + u415 + u416 + u417 + u418 + u419 + u42 + u420 + u421 + u422 + u423 + u424 + u425 + u426 + u427 + u428 + u429 + u43 + u430 + u431 + u432 + u433 + u434 + u435 + u436 + u437 + u438 + u439 + u44 + u440 + u441 + u442 + u443 + u444 + u445 + u446 + u447 + u448 + u449 + u45 + u450 + u451 + u452 + u453 + u454 + u455 + u456 + u457 + u458 + u459 + u46 + u460 + u461 + u462 + u463 + u464 + u465 + u466 + u467 + u468 + u469 + u47 + u470 + u471 + u472 + u473 + u474 + u475 + u476 + u477 + u478 + u479 + u48 + u480 + u481 + u482 + u483 + u484 + u485 + u486 + u487 + u488 + u489 + u49 + u490 + u491 + u492 + u493 + u494 + u495 + u496 + u497 + u498 + u499 + u5 + u50 + u500 + u501 + u502 + u503 + u504 + u505 + u506 + u507 + u508 + u509 + u51 + u510 + u511 + u512 + u513 + u514 + u515 + u516 + u517 + u518 + u519 + u52 + u520 + u521 + u522 + u523 + u524 + u525 + u526 + u527 + u528 + u529 + u53 + u530 + u531 + u532 + u533 + u534 + u535 + u536 + u537 + u538 + u539 + u54 + u540 + u541 + u542 + u543 + u544 + u545 + u546 + u547 + u548 + u549 + u55 + u550 + u551 + u552 + u553 + u554 + u555 + u556 + u557 + u558 + u559 + u56 + u560 + u561 + u562 + u563 + u564 + u565 + u566 + u567 + u568 + u569 + u57 + u570 + u571 + u572 + u573 + u574 + u575 + u576 + u577 + u578 + u579 + u58 + u580 + u581 + u582 + u583 + u584 + u585 + u586 + u587 + u588 + u589 + u59 + u590 + u591 + u592 + u593 + u594 + u595 + u596 + u597 + u598 + u599 + u6 + u60 + u600 + u601 + u602 + u603 + u604 + u605 + u606 + u607 + u608 + u609 + u61 + u610 + u611 + u612 + u613 + u614 + u615 + u616 + u617 + u618 + u619 + u62 + u620 + u621 + u622 + u623 + u624 + u625 + u626 + u627 + u628 + u629 + u63 + u630 + u631 + u632 + u633 + u634 + u635 + u636 + u637 + u638 + u639 + u64 + u640 + u641 + u642 + u643 + u644 + u645 + u646 + u647 + u648 + u649 + u65 + u650 + u651 + u652 + u653 + u654 + u655 + u656 + u657 + u658 + u659 + u66 + u660 + u661 + u662 + u663 + u664 + u665 + u666 + u667 + u668 + u669 + u67 + u670 + u671 + u672 + u673 + u674 + u675 + u676 + u677 + u678 + u679 + u68 + u680 + u681 + u682 + u683 + u684 + u685 + u686 + u687 + u688 + u689 + u69 + u690 + u691 + u692 + u693 + u694 + u695 + u696 + u697 + u698 + u699 + u7 + u70 + u700 + u701 + u702 + u703 + u704 + u705 + u706 + u707 + u708 + u709 + u71 + u710 + u711 + u712 + u713 + u714 + u715 + u716 + u717 + u718 + u719 + u72 + u720 + u721 + u722 + u723 + u724 + u725 + u726 + u727 + u728 + u729 + u73 + u730 + u731 + u732 + u733 + u734 + u735 + u736 + u737 + u738 + u739 + u74 + u740 + u741 + u742 + u743 + u744 + u745 + u746 + u747 + u748 + u749 + u75 + u750 + u751 + u752 + u753 + u754 + u755 + u756 + u757 + u758 + u759 + u76 + u760 + u761 + u762 + u763 + u764 + u765 + u766 + u767 + u768 + u769 + u77 + u770 + u771 + u772 + u773 + u774 + u775 + u776 + u777 + u778 + u779 + u78 + u780 + u781 + u782 + u783 + u784 + u785 + u786 + u787 + u788 + u789 + u79 + u790 + u791 + u792 + u793 + u794 + u795 + u796 + u797 + u798 + u799 + u8 + u80 + u800 + u801 + u802 + u803 + u804 + u805 + u806 + u807 + u808 + u809 + u81 + u810 + u811 + u812 + u813 + u814 + u815 + u816 + u817 + u818 + u819 + u82 + u820 + u821 + u822 + u823 + u824 + u825 + u826 + u827 + u828 + u829 + u83 + u830 + u831 + u832 + u833 + u834 + u835 + u836 + u837 + u838 + u839 + u84 + u840 + u841 + u842 + u843 + u844 + u845 + u846 + u847 + u848 + u849 + u85 + u850 + u851 + u852 + u853 + u854 + u855 + u856 + u857 + u858 + u859 + u86 + u860 + u861 + u862 + u863 + u864 + u865 + u866 + u867 + u868 + u869 + u87 + u870 + u871 + u872 + u873 + u874 + u875 + u876 + u877 + u878 + u879 + u88 + u880 + u881 + u882 + u883 + u884 + u885 + u886 + u887 + u888 + u889 + u89 + u890 + u891 + u892 + u893 + u894 + u895 + u896 + u897 + u898 + u899 + u9 + u90 + u900 + u901 + u902 + u903 + u904 + u905 + u906 + u907 + u908 + u909 + u91 + u910 + u911 + u912 + u913 + u914 + u915 + u916 + u917 + u918 + u919 + u92 + u920 + u921 + u922 + u923 + u924 + u925 + u926 + u927 + u928 + u929 + u93 + u930 + u931 + u932 + u933 + u934 + u935 + u936 + u937 + u938 + u939 + u94 + u940 + u941 + u942 + u943 + u944 + u945 + u946 + u947 + u948 + u949 + u95 + u950 + u951 + u952 + u953 + u954 + u955 + u956 + u957 + u958 + u959 + u96 + u960 + u961 + u962 + u963 + u964 + u965 + u966 + u967 + u968 + u969 + u97 + u970 + u971 + u972 + u973 + u974 + u975 + u976 + u977 + u978 + u979 + u98 + u980 + u981 + u982 + u983 + u984 + u985 + u986 + u987 + u988 + u989 + u99 + u990 + u991 + u992 + u993 + u994 + u995 + u996 + u997 + u998 + u999 <= 487233 - u1166 [guard added] prob_user_split = torch.split(class_prob, num_objs_per_user.tolist())  # <torch_package_0>.dper3_models/ads_ranking/model_impl/sparse_nn/hiearchical_pre_norm_pma.py:149 in get_topk (_ops.py:801 in decompose), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""u0 + u1 + u10 + u100 + u1000 + u1001 + u1002 + u1003 + u1004 + u1005 + u1006 + u1007 + u1008 + u1009 + u101 + u1010 + u1011 + u1012 + u1013 + u1014 + u1015 + u1016 + u1017 + u1018 + u1019 + u102 + u1020 + u1021 + u1022 + u1023 + u1024 + u1025 + u1026 + u1027 + u1028 + u1029 + u103 + u1030 + u1031 + u1032 + u1033 + u1034 + u1035 + u1036 + u1037 + u1038 + u1039 + u104 + u1040 + u1041 + u1042 + u1043 + u1044 + u1045 + u1046 + u1047 + u1048 + u1049 + u105 + u1050 + u1051 + u1052 + u1053 + u1054 + u1055 + u1056 + u1057 + u1058 + u1059 + u106 + u1060 + u1061 + u1062 + u1063 + u1064 + u1065 + u1066 + u1067 + u1068 + u1069 + u107 + u1070 + u1071 + u1072 + u1073 + u1074 + u1075 + u1076 + u1077 + u1078 + u1079 + u108 + u1080 + u1081 + u1082 + u1083 + u1084 + u1085 + u1086 + u1087 + u1088 + u1089 + u109 + u1090 + u1091 + u1092 + u1093 + u1094 + u1095 + u1096 + u1097 + u1098 + u1099 + u11 + u110 + u1100 + u1101 + u1102 + u1103 + u1104 + u1105 + u1106 + u1107 + u1108 + u1109 + u111 + u1110 + u1111 + u1112 + u1113 + u1114 + u1115 + u1116 + u1117 + u1118 + u1119 + u112 + u1120 + u1121 + u1122 + u1123 + u1124 + u1125 + u1126 + u1127 + u1128 + u1129 + u113 + u1130 + u1131 + u1132 + u1133 + u1134 + u1135 + u1136 + u1137 + u1138 + u1139 + u114 + u1140 + u1141 + u1142 + u1143 + u1144 + u1145 + u1146 + u1147 + u1148 + u1149 + u115 + u1150 + u1151 + u1152 + u1153 + u1154 + u1155 + u1156 + u1157 + u1158 + u1159 + u116 + u1160 + u1161 + u1162 + u1163 + u1164 + u1165 + u117 + u118 + u119 + u12 + u120 + u121 + u122 + u123 + u124 + u125 + u126 + u127 + u128 + u129 + u13 + u130 + u131 + u132 + u133 + u134 + u135 + u136 + u137 + u138 + u139 + u14 + u140 + u141 + u142 + u143 + u144 + u145 + u146 + u147 + u148 + u149 + u15 + u150 + u151 + u152 + u153 + u154 + u155 + u156 + u157 + u158 + u159 + u16 + u160 + u161 + u162 + u163 + u164 + u165 + u166 + u167 + u168 + u169 + u17 + u170 + u171 + u172 + u173 + u174 + u175 + u176 + u177 + u178 + u179 + u18 + u180 + u181 + u182 + u183 + u184 + u185 + u186 + u187 + u188 + u189 + u19 + u190 + u191 + u192 + u193 + u194 + u195 + u196 + u197 + u198 + u199 + u2 + u20 + u200 + u201 + u202 + u203 + u204 + u205 + u206 + u207 + u208 + u209 + u21 + u210 + u211 + u212 + u213 + u214 + u215 + u216 + u217 + u218 + u219 + u22 + u220 + u221 + u222 + u223 + u224 + u225 + u226 + u227 + u228 + u229 + u23 + u230 + u231 + u232 + u233 + u234 + u235 + u236 + u237 + u238 + u239 + u24 + u240 + u241 + u242 + u243 + u244 + u245 + u246 + u247 + u248 + u249 + u25 + u250 + u251 + u252 + u253 + u254 + u255 + u256 + u257 + u258 + u259 + u26 + u260 + u261 + u262 + u263 + u264 + u265 + u266 + u267 + u268 + u269 + u27 + u270 + u271 + u272 + u273 + u274 + u275 + u276 + u277 + u278 + u279 + u28 + u280 + u281 + u282 + u283 + u284 + u285 + u286 + u287 + u288 + u289 + u29 + u290 + u291 + u292 + u293 + u294 + u295 + u296 + u297 + u298 + u299 + u3 + u30 + u300 + u301 + u302 + u303 + u304 + u305 + u306 + u307 + u308 + u309 + u31 + u310 + u311 + u312 + u313 + u314 + u315 + u316 + u317 + u318 + u319 + u32 + u320 + u321 + u322 + u323 + u324 + u325 + u326 + u327 + u328 + u329 + u33 + u330 + u331 + u332 + u333 + u334 + u335 + u336 + u337 + u338 + u339 + u34 + u340 + u341 + u342 + u343 + u344 + u345 + u346 + u347 + u348 + u349 + u35 + u350 + u351 + u352 + u353 + u354 + u355 + u356 + u357 + u358 + u359 + u36 + u360 + u361 + u362 + u363 + u364 + u365 + u366 + u367 + u368 + u369 + u37 + u370 + u371 + u372 + u373 + u374 + u375 + u376 + u377 + u378 + u379 + u38 + u380 + u381 + u382 + u383 + u384 + u385 + u386 + u387 + u388 + u389 + u39 + u390 + u391 + u392 + u393 + u394 + u395 + u396 + u397 + u398 + u399 + u4 + u40 + u400 + u401 + u402 + u403 + u404 + u405 + u406 + u407 + u408 + u409 + u41 + u410 + u411 + u412 + u413 + u414 + u415 + u416 + u417 + u418 + u419 + u42 + u420 + u421 + u422 + u423 + u424 + u425 + u426 + u427 + u428 + u429 + u43 + u430 + u431 + u432 + u433 + u434 + u435 + u436 + u437 + u438 + u439 + u44 + u440 + u441 + u442 + u443 + u444 + u445 + u446 + u447 + u448 + u449 + u45 + u450 + u451 + u452 + u453 + u454 + u455 + u456 + u457 + u458 + u459 + u46 + u460 + u461 + u462 + u463 + u464 + u465 + u466 + u467 + u468 + u469 + u47 + u470 + u471 + u472 + u473 + u474 + u475 + u476 + u477 + u478 + u479 + u48 + u480 + u481 + u482 + u483 + u484 + u485 + u486 + u487 + u488 + u489 + u49 + u490 + u491 + u492 + u493 + u494 + u495 + u496 + u497 + u498 + u499 + u5 + u50 + u500 + u501 + u502 + u503 + u504 + u505 + u506 + u507 + u508 + u509 + u51 + u510 + u511 + u512 + u513 + u514 + u515 + u516 + u517 + u518 + u519 + u52 + u520 + u521 + u522 + u523 + u524 + u525 + u526 + u527 + u528 + u529 + u53 + u530 + u531 + u532 + u533 + u534 + u535 + u536 + u537 + u538 + u539 + u54 + u540 + u541 + u542 + u543 + u544 + u545 + u546 + u547 + u548 + u549 + u55 + u550 + u551 + u552 + u553 + u554 + u555 + u556 + u557 + u558 + u559 + u56 + u560 + u561 + u562 + u563 + u564 + u565 + u566 + u567 + u568 + u569 + u57 + u570 + u571 + u572 + u573 + u574 + u575 + u576 + u577 + u578 + u579 + u58 + u580 + u581 + u582 + u583 + u584 + u585 + u586 + u587 + u588 + u589 + u59 + u590 + u591 + u592 + u593 + u594 + u595 + u596 + u597 + u598 + u599 + u6 + u60 + u600 + u601 + u602 + u603 + u604 + u605 + u606 + u607 + u608 + u609 + u61 + u610 + u611 + u612 + u613 + u614 + u615 + u616 + u617 + u618 + u619 + u62 + u620 + u621 + u622 + u623 + u624 + u625 + u626 + u627 + u628 + u629 + u63 + u630 + u631 + u632 + u633 + u634 + u635 + u636 + u637 + u638 + u639 + u64 + u640 + u641 + u642 + u643 + u644 + u645 + u646 + u647 + u648 + u649 + u65 + u650 + u651 + u652 + u653 + u654 + u655 + u656 + u657 + u658 + u659 + u66 + u660 + u661 + u662 + u663 + u664 + u665 + u666 + u667 + u668 + u669 + u67 + u670 + u671 + u672 + u673 + u674 + u675 + u676 + u677 + u678 + u679 + u68 + u680 + u681 + u682 + u683 + u684 + u685 + u686 + u687 + u688 + u689 + u69 + u690 + u691 + u692 + u693 + u694 + u695 + u696 + u697 + u698 + u699 + u7 + u70 + u700 + u701 + u702 + u703 + u704 + u705 + u706 + u707 + u708 + u709 + u71 + u710 + u711 + u712 + u713 + u714 + u715 + u716 + u717 + u718 + u719 + u72 + u720 + u721 + u722 + u723 + u724 + u725 + u726 + u727 + u728 + u729 + u73 + u730 + u731 + u732 + u733 + u734 + u735 + u736 + u737 + u738 + u739 + u74 + u740 + u741 + u742 + u743 + u744 + u745 + u746 + u747 + u748 + u749 + u75 + u750 + u751 + u752 + u753 + u754 + u755 + u756 + u757 + u758 + u759 + u76 + u760 + u761 + u762 + u763 + u764 + u765 + u766 + u767 + u768 + u769 + u77 + u770 + u771 + u772 + u773 + u774 + u775 + u776 + u777 + u778 + u779 + u78 + u780 + u781 + u782 + u783 + u784 + u785 + u786 + u787 + u788 + u789 + u79 + u790 + u791 + u792 + u793 + u794 + u795 + u796 + u797 + u798 + u799 + u8 + u80 + u800 + u801 + u802 + u803 + u804 + u805 + u806 + u807 + u808 + u809 + u81 + u810 + u811 + u812 + u813 + u814 + u815 + u816 + u817 + u818 + u819 + u82 + u820 + u821 + u822 + u823 + u824 + u825 + u826 + u827 + u828 + u829 + u83 + u830 + u831 + u832 + u833 + u834 + u835 + u836 + u837 + u838 + u839 + u84 + u840 + u841 + u842 + u843 + u844 + u845 + u846 + u847 + u848 + u849 + u85 + u850 + u851 + u852 + u853 + u854 + u855 + u856 + u857 + u858 + u859 + u86 + u860 + u861 + u862 + u863 + u864 + u865 + u866 + u867 + u868 + u869 + u87 + u870 + u871 + u872 + u873 + u874 + u875 + u876 + u877 + u878 + u879 + u88 + u880 + u881 + u882 + u883 + u884 + u885 + u886 + u887 + u888 + u889 + u89 + u890 + u891 + u892 + u893 + u894 + u895 + u896 + u897 + u898 + u899 + u9 + u90 + u900 + u901 + u902 + u903 + u904 + u905 + u906 + u907 + u908 + u909 + u91 + u910 + u911 + u912 + u913 + u914 + u915 + u916 + u917 + u918 + u919 + u92 + u920 + u921 + u922 + u923 + u924 + u925 + u926 + u927 + u928 + u929 + u93 + u930 + u931 + u932 + u933 + u934 + u935 + u936 + u937 + u938 + u939 + u94 + u940 + u941 + u942 + u943 + u944 + u945 + u946 + u947 + u948 + u949 + u95 + u950 + u951 + u952 + u953 + u954 + u955 + u956 + u957 + u958 + u959 + u96 + u960 + u961 + u962 + u963 + u964 + u965 + u966 + u967 + u968 + u969 + u97 + u970 + u971 + u972 + u973 + u974 + u975 + u976 + u977 + u978 + u979 + u98 + u980 + u981 + u982 + u983 + u984 + u985 + u986 + u987 + u988 + u989 + u99 + u990 + u991 + u992 + u993 + u994 + u995 + u996 + u997 + u998 + u999 <= 487233 - u1166""
```

cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @chauhang @penguinwu @bobrenjc93","closed","Refatoração","{""high priority"",""triage review"",""oncall: pt2"",""module: dynamic shapes""}",5,"2025-01-14 19:43:21","2025-01-18 19:03:12","2025-01-18 19:03:12","bdhirsh","laithsakka",NULL,"https://github.com/pytorch/pytorch/issues/144792",4
"93","2657074594","2657074594","140628","remove ptx test","test if removing PTX will reduce the binary size

cc @nWEIdia","closed","Refatoração","{""open source"",Stale,ciflow/binaries}",5,"2024-11-13 23:13:55","2025-01-19 21:19:06","2025-01-19 21:19:05","tinglvv",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140628",67
"100","2619838615","2619838615","139133","[executorch hash update] update the pinned executorch hash","This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned executorch hash.","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",ciflow/inductor}",18,"2024-10-29 00:24:23","2024-12-02 02:13:22","2024-11-02 00:14:49","pytorchupdatebot",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139133",4
"112","2618061508","2618061508","139058","[3/N] Don't skip ASAN on some tests","Fixes #ISSUE_NUMBER","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",7,"2024-10-28 10:54:23","2024-10-29 02:32:16","2024-10-28 23:57:25","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139058",0
"123","2598622218","2598622218","138382","[4/N] Fix clang-tidy warnings in torch/csrc/api/","Follows #138328","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""release notes: cpp""}",6,"2024-10-19 00:56:08","2024-10-23 02:29:00","2024-10-19 13:32:53","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138382",0
"132","2726001601","2726001601","142351","Corrected description of AMSGrad algorithm","Fixes #142323","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""release notes: optim""}",5,"2024-12-09 05:08:35","2024-12-19 16:25:26","2024-12-19 16:24:23","Tony-Y",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142351",10
"190","2702310418","2702310418","141773","[AOTI] Clean up temporary files generated by AOTI package loader.","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #141773

Fix #141772","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",ciflow/inductor}",5,"2024-11-28 14:21:08","2025-01-06 02:08:54","2024-12-06 11:46:50","etaf","etaf",NULL,"https://github.com/pytorch/pytorch/pull/141773",8
"237","2619118771","2619118771","139089","Remove Python 3.8 from README",NULL,"closed","Refatoração","{Merged,""topic: not user facing""}",3,"2024-10-28 17:52:52","2024-11-28 02:15:08","2024-10-28 18:12:16","kit1980",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139089",0
"242","2625804587","2625804587","139368","Fix the example of fx/interpreter","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139368



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""release notes: fx"",fx}",5,"2024-10-31 02:11:12","2024-12-01 02:21:23","2024-10-31 05:12:45","FFFrog",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139368",0
"247","2632736309","2632736309","139622","DISABLED test_comprehensive_exp_cuda_bool (__main__.TestInductorOpInfoCUDA)","Platforms: inductor

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_comprehensive_exp_cuda_bool&suite=TestInductorOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/32466610584).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_comprehensive_exp_cuda_bool`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 1152, in test_wrapper
    return test(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 1434, in only_fn
    return fn(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 2194, in wrapper
    fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 1229, in dep_fn
    return fn(slf, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 1229, in dep_fn
    return fn(slf, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 1229, in dep_fn
    return fn(slf, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 1587, in wrapper
    fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 1523, in wrapper
    fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/unittest/mock.py"", line 1395, in patched
    return func(*newargs, **newkeywargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py"", line 955, in inner
    raise e
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py"", line 947, in inner
    fn(self, device, dtype, op)
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py"", line 1193, in test_comprehensive
    raise e
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py"", line 1153, in test_comprehensive
    self.check_model_gpu(
  File ""/opt/conda/envs/py_3.12/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor.py"", line 606, in check_model_gpu
    check_model(
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor.py"", line 447, in check_model
    actual = run(*example_inputs, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py"", line 556, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py"", line 1446, in __call__
    return self._torchdynamo_orig_callable(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py"", line 549, in __call__
    return _compile(
           ^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py"", line 982, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py"", line 708, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_utils_internal.py"", line 95, in wrapper_function
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py"", line 743, in _compile_inner
    out_code = transform_code_object(code, transform)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py"", line 1348, in transform_code_object
    transformations(instructions, code_options)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py"", line 233, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py"", line 662, in transform
    tracer.run()
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py"", line 2909, in run
    super().run()
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py"", line 1115, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py"", line 1027, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py"", line 3100, in RETURN_VALUE
    self._return(inst)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py"", line 3085, in _return
    self.output.compile_subgraph(
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py"", line 1140, in compile_subgraph
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py"", line 1411, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py"", line 1460, in call_user_compiler
    return self._call_user_compiler(gm)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py"", line 1509, in _call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py"", line 1490, in _call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py"", line 130, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py"", line 130, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor.py"", line 439, in compile_fx_wrapper
    return compile_fx(model_, example_inputs_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py"", line 1707, in compile_fx
    return aot_autograd(
           ^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/backends/common.py"", line 72, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py"", line 1103, in aot_module_simplified
    compiled_fn = dispatch_and_compile()
                  ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py"", line 1079, in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py"", line 527, in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py"", line 778, in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
                               ^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py"", line 196, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py"", line 1524, in fw_compiler_base
    return _fw_compiler_base(model, example_inputs, is_inference)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py"", line 1593, in _fw_compiler_base
    return inner_compile(
           ^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py"", line 587, in compile_fx_inner
    return wrap_compiler_debug(_compile_fx_inner, compiler_name=""inductor"")(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/repro/after_aot.py"", line 100, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py"", line 744, in _compile_fx_inner
    compiled_graph = FxGraphCache.load(
                     ^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py"", line 1507, in load
    compiled_graph = compile_fx_fn(
                     ^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py"", line 651, in codegen_and_compile
    compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py"", line 962, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
                  ^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/graph.py"", line 2036, in compile_to_fn
    return self.compile_to_module().call
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/graph.py"", line 1955, in compile_to_module
    return self._compile_to_module()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/graph.py"", line 1990, in _compile_to_module
    mod = PyCodeCache.load_by_key_path(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py"", line 3026, in load_by_key_path
    mod = _reload_python_module(key, path)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/runtime/compile_tasks.py"", line 45, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File ""/tmp/tmplxeo8bv4/by/cbyovvfte3ygwntgffdoy3k3jtbpfw7scndbyy66f677ccdxqrjw.py"", line 78, in <module>
    async_compile.wait(globals())
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/async_compile.py"", line 312, in wait
    scope[key] = result.result()
                 ^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py"", line 3505, in result
    self.kernel.precompile()
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py"", line 274, in precompile
    compiled_binary, launcher = self._precompile_config(
                                ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py"", line 489, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/var/lib/jenkins/triton/python/triton/compiler/compiler.py"", line 276, in compile
    module = src.make_ir(options, codegen_fns, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/var/lib/jenkins/triton/python/triton/compiler/compiler.py"", line 113, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/inspect.py"", line 1267, in getsourcelines
    lines, lnum = findsource(object)
                  ^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/inspect.py"", line 1096, in findsource
    raise OSError('could not get source code')
torch._dynamo.exc.BackendCompilerFailed: backend='compile_fx_wrapper' raised:
OSError: could not get source code

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 3052, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 3052, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 460, in instantiated_test
    result = test(self, **param_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 1587, in wrapper
    fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 1164, in test_wrapper
    raise e_tracked from e
Exception: Caused by sample input at index 0: SampleInput(input=Tensor[size=(20,), device=""cuda:0"", dtype=torch.bool], args=(), kwargs={}, broadcasts_input=False, name='')

To execute this test, run the following from the base repo dir:
    PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=0 python test/inductor/test_torchinductor_opinfo.py TestInductorOpInfoCUDA.test_comprehensive_exp_cuda_bool

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `inductor/test_torchinductor_opinfo.py`

cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @clee2000 @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @aakhundov","closed","Refatoração","{""high priority"",triaged,""module: flaky-tests"",skipped,""oncall: pt2"",""module: inductor""}",2,"2024-11-04 12:50:59","2024-12-12 17:38:46","2024-12-12 17:38:46","pytorch-bot[bot]","masnesral",NULL,"https://github.com/pytorch/pytorch/issues/139622",38
"251","2634569079","2634569079","139734","DISABLED test_no_grad_copy (__main__.TestAutograd)","Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_no_grad_copy&suite=TestAutograd&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/32508596415).

Over the past 3 hours, it has been determined flaky in 17 workflow(s) with 17 failures and 17 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_no_grad_copy`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/test/test_autograd.py"", line 5599, in test_no_grad_copy
    self.assertTrue(p_a == p_g or p_b == p_g)
  File ""/opt/conda/envs/py_3.12/lib/python3.12/unittest/case.py"", line 727, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_DYNAMO=1 python test/test_autograd.py TestAutograd.test_no_grad_copy

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `test_autograd.py`

cc @ezyang @albanD @gqchen @pearu @nikitaved @soulitzer @Varal7 @xmfan @clee2000 @wdvr @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @amjames","closed","Refatoração","{""module: autograd"",triaged,""module: flaky-tests"",skipped,""oncall: pt2"",""module: dynamo""}",12,"2024-11-05 06:43:19","2025-01-22 21:58:44","2025-01-22 21:57:41","pytorch-bot[bot]","StrongerXi",NULL,"https://github.com/pytorch/pytorch/issues/139734",78
"253","2637095441","2637095441","139854","[aot autograd] functionalize bwd prologue and epilogue","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)","closed","Refatoração","{ciflow/inductor}",2,"2024-11-06 05:39:58","2025-01-20 02:05:08","2024-12-21 01:11:46","xmfan",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139854",45
"298","2721737754","2721737754","142185","skip test dynamo for aot_dispatch tests on ci","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #141524
* __->__ #142185

A lot of tests in test_aotdispatch.py is not meaningful (from user's perspective) when we run with dynamo. So we skip them.","closed","Refatoração","{Merged,Reverted,""topic: not user facing"",ci-no-td}",4,"2024-12-06 00:15:05","2025-01-11 02:10:49","2024-12-11 18:47:06","ydwu4",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142185",5
"328","2745316176","2745316176","143389","Fix unused variables in test_serialize_sym_float","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143389","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-12-17 15:57:02","2025-01-18 02:04:23","2024-12-17 18:55:17","rec",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143389",0
"319","2736316562","2736316562","143119","Expose sharedMemPerMultiprocessor device property to python",NULL,"closed","Refatoração","{""open source"",Merged,ciflow/trunk,""release notes: python_frontend""}",3,"2024-12-12 16:05:44","2024-12-13 16:55:02","2024-12-13 16:53:59","peterbell10",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143119",1
"320","2735651964","2735651964","143113","check tensor grad requirement before dispatching","Fixes #143071 

Before, dispatching disregarded whether tensor had a requires_grad or not. Now if we dispatch such a tensor, it will ask user to detach the tensor first so we avoid unexpected errors such as the one listed in the issue. This is already done for numpy. For example if we do this:
```
import numpy as np
import torch


x = torch.tensor(2.0, requires_grad=True)
np.log2(x)
```
It gives an error:
`RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.`

But for:
```
import math
import torch


x = torch.tensor(2.0, requires_grad=True)
math.log2(x)
```
No error","closed","Refatoração","{triaged,""open source""}",3,"2024-12-12 11:38:17","2024-12-14 07:10:53","2024-12-14 07:10:53","Isalia20",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143113",2
"321","2739411458","2739411458","143248","try root fix for FP8 tensor","Fixes #143194



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",triaged,""open source"",Merged,ciflow/trunk,""release notes: distributed (fsdp)""}",7,"2024-12-14 00:42:57","2024-12-20 01:58:24","2024-12-20 01:57:21","mayank31398",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143248",6
"343","2767835554","2767835554","144153","Update copyright year to 2025",NULL,"closed","Refatoração","{triaged,""open source"",""topic: not user facing""}",3,"2025-01-03 16:19:42","2025-01-10 16:57:46","2025-01-10 16:31:53","kuraga",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144153",7
"347","2776463102","2776463102","144432","[ROCm] hipblaslt rowwise f8 gemm","hipblaslt added rowwise f8 gemm support.  Integrate with scaled_mm.

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",triaged,""open source"",Merged,ciflow/trunk,""release notes: rocm"",""release notes: nn"",ciflow/rocm}",14,"2025-01-08 22:25:26","2025-01-16 02:54:49","2025-01-15 18:23:47","jeffdaily",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144432",7
"374","2805498992","2805498992","145418","[BE] Type annotate metrics.py","cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",3,"2025-01-22 22:47:58","2025-01-23 18:15:04","2025-01-23 18:14:02","BoyuanFeng",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145418",1
"1","2818558830","2818558830","145934","[Break XPU] Fix Inductor cuda bias UT","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145934

# Motivation
[Break XPU] inductor ut: `inductor/test_inplace_padding.py::InplacePaddingTest::test_pad_non_zero - RuntimeError: Expected to find ""empty_strided_cuda((2048, 2048), (2048, 1), torch.float32).as_strided((2048, 2047), (2048, 1))"" but did not find it`

With this PR, `test_pad_non_zero` will pass on XPU.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor,ciflow/xpu}",7,"2025-01-29 15:34:45","2025-01-31 01:40:46","2025-01-31 01:39:41","guangyey",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145934",2
"271","2670434246","2670434246","140995","[ONNX] How exporter handles higher order ops (HOP)","> [!NOTE]
> This is a design doc for HOP support in the ONNX exporter.

In [PyTorch export IR](https://pytorch.org/docs/stable/export.ir_spec.html), control flows and special ops like `cond`, `scan` and `wrap_with_autocast` are represented as higher order ops (HOP) which take functions (represented as local GraphModules) as inputs.

As the functions called by the HOPs are pure and do not close over values from outer naming scopes, they are different from typical ONNX subgraphs (that can reference outer scoped values). To allow constructing arguments to an HOP without any information from the outer scope, it is easier and more straightforward to represent the subgraphs as ONNX functions than as ONNX graphs.

## Nested initializers

The GraphModules in the export IR are pure. All initializers are provided as inputs. So there will be no nested initializers.

## Nested subgraphs

The GraphModules can be nested, meaning a local GraphModule can in turn have an HOP that takes another GraphModule, local to the submodule. These nested GraphModules have unique names in there own Python naming scopes, but they can have conflicting names if we move them to the same scope. This is important because there is only one naming scope for ONNX model local functions. If we translate all GraphModules in the exported program and list them to the same scope, we need to ensure names do not collide and can be referenced correctly.

As a concrete example, consider the following model, where a branch of the cond operator is in turn calling another cond operator:


```py
        class Submodule(torch.nn.Module):
            def __init__(self):
                super().__init__()
                # Nested weight
                self.weight = torch.nn.Parameter(torch.tensor([100.0]))

            def forward(self, x):
                def true_fn(x):
                    return x * self.weight

                def false_fn(x):
                    return x / self.weight

                y = torch.cond(x.sum() <= 0, true_fn, false_fn, [x])
                return y

        class CondModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.submodule = Submodule()
                self.weight = torch.nn.Parameter(torch.tensor([42.0]))

            def forward(self, x):
                def true_fn(x):
                    return self.submodule(x)

                def false_fn(x):
                    return x - self.weight

                y = torch.cond(x.sum() > 0, true_fn, false_fn, [x])
                return y

```

The exported program looks like this:


```py
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_weight: ""f32[1]"", p_submodule_weight: ""f32[1]"", x: ""i64[2]""):
            sum_1: ""i64[]"" = torch.ops.aten.sum.default(x)
            gt: ""b8[]"" = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None
            
            true_graph_0 = self.true_graph_0
            false_graph_0 = self.false_graph_0
            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [x, p_submodule_weight, p_weight]);  gt = true_graph_0 = false_graph_0 = x = p_submodule_weight = p_weight = None
            getitem: ""f32[2]"" = cond[0];  cond = None
            return (getitem,)
            
        class true_graph_0(torch.nn.Module):
            def forward(self, x: ""i64[2]"", p_submodule_weight: ""f32[1]"", p_weight: ""f32[1]""):
                 # File: <eval_with_key>.50:6 in forward, code: sum_1 = l_args_3_0__1.sum()
                sum_1: ""i64[]"" = torch.ops.aten.sum.default(x)
                
                 # File: <eval_with_key>.50:7 in forward, code: le = sum_1.le(0);  sum_1 = None
                le: ""b8[]"" = torch.ops.aten.le.Scalar(sum_1, 0);  sum_1 = None
                
                true_graph_0 = self.true_graph_0
                false_graph_0 = self.false_graph_0
                cond = torch.ops.higher_order.cond(le, true_graph_0, false_graph_0, [p_submodule_weight, x]);  le = true_graph_0 = false_graph_0 = p_submodule_weight = x = None
                getitem: ""f32[2]"" = cond[0];  cond = None
                return (getitem,)
                
            class true_graph_0(torch.nn.Module):
                def forward(self, p_submodule_weight: ""f32[1]"", x: ""i64[2]""):
                    mul: ""f32[2]"" = torch.ops.aten.mul.Tensor(x, p_submodule_weight);  x = p_submodule_weight = None
                    return (mul,)
                    
            class false_graph_0(torch.nn.Module):
                def forward(self, p_submodule_weight: ""f32[1]"", x: ""i64[2]""):
                    div: ""f32[2]"" = torch.ops.aten.div.Tensor(x, p_submodule_weight);  x = p_submodule_weight = None
                    return (div,)
                    
        class false_graph_0(torch.nn.Module):
            def forward(self, x: ""i64[2]"", p_submodule_weight: ""f32[1]"", p_weight: ""f32[1]""):
                sub: ""f32[2]"" = torch.ops.aten.sub.Tensor(x, p_weight);  x = p_weight = None
                return (sub,)

```

The root graph defines true_graph_0, which defines another true_graph_0 in its own naming scope. Using graph_module.named_modules(), we get a list of modules, including the nested ones: `["""", true_graph_0, true_graph_0.true_graph_0, true_graph_0.false_graph_0, false_graph_0]`.

Note that the names provided by `name_modules()` are already scoped. We can recover the local object name and the naming scope it’s in by simply splitting on the last dot in the string.

Each of these graphs will have a value naming scope that has unique names for its immediate subgraphs and values:


```py
{
    """": true_graph_0, false_graph_0, <…values>
    ""true_graph_0"": true_graph_0, false_graph_0, <…values>
    ""true_graph_0.true_graph_0"": <…values>
    ""true_graph_0.false_graph_0"": <…values>
    ""false_graph_0"": <…values>
}
```

We can thus construct an ONNX function for each of the non-root Graphs in the reversed order, such that the inner most graph is first constructed and made available in the naming scope of the outer graph before the outer graph is constructed.

This motivates the following data structure for storing scoped graphs: `dict[<scope name>, dict[<graph name in scope>, <ir.Graph>]]`. 

The way we fill in the data structure is the following:


```py
scoped_subgraphs = dict()  # some defaultdict
for (module_name, module) in reversed(named_modules()):
    If module_name == """":
        break
    onnx_function = translate(module, scoped_graphs[module_name])
    parent_scope, local_module_name = module_name.rsplit(""."", 1)
    scoped_graphs[parent_scope][local_module_name] = onnx_function
# Translate the top graph
top_graph = translate_top_graph(ep.graph, scoped_graphs[""""])
# Finally construct the model and collect all onnx functions to the model
…
```

Within `translate`, all values are created as inputs to the module or by nodes in the module. Any get_attr node will obtain the corresponding ONNX function from the provided scoped_graph. From the ONNX function it is possible to create a single node that calls the function. Since we know the number of outputs the function has, we can also handle multi-output cases. (This assumes the output is not variadic).


## Implementing specific operators

Using `cond` as an example, the dispatcher dispatches to an implementation of 


```py
@traced
def cond_impl(cond: ir.Value, true_func: ir.Function, false_func, inputs: list[ir.Value]):
    # build_node is a helper that creates a node that calls the given function
    return op.If(conf, then_graph=ir.Graph(…, [build_node(true_func, inputs)]), else_graph=ir.Graph(..., [build_node(false_func, inputs)], num_outputs=len(true_func.outputs))
```

Similarly, autocast can be implemented as

```py
@traced
def autocast_impl(device, _arg, _arg2, _arg3, func: ir.Function, *args):
    # Add custom casting logic for the device, potentially modifying the graph in func
    return call_func_as_onnxscript_func(func, *args)

```

This shows the mechanism for supporting various HOPs is general, and we can assume whenever we have a get_attr node as input, it is already translated into an ONNX IR function.","closed","Refatoração","{""module: onnx"",triaged}",0,"2024-11-19 01:15:33","2024-11-21 18:08:48","2024-11-21 03:02:48","justinchuby","justinchuby",NULL,"https://github.com/pytorch/pytorch/issues/140995",2
"2","2818554953","2818554953","145933","Better hop_db comment; move test to a non-export test file","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145933

Goal is for people to better test their HOPs.

Test Plan:
- tests","closed","Refatoração","{ciflow/trunk}",2,"2025-01-29 15:33:07","2025-01-29 16:40:06","2025-01-29 16:40:06","zou3519",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145933",0
"3","2802535990","2802535990","145285","Updates NCCL user buffer registration test for NCCL 2.24.3","NCCL 2.24.3 changed the content of the debug output for NVLS registration. We use this debug output in our test suite to check if NVLS was successfully registered or not. Hence we need to specialize for the NCCL version in the test.

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145285



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",""open source"",Merged,ciflow/trunk,""topic: not user facing""}",7,"2025-01-21 18:28:05","2025-01-28 00:25:59","2025-01-28 00:24:55","syed-ahmed",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145285",7
"4","2802518264","2802518264","145284","[dynamo] `torch.compile` ICE on using a sourceless unspecialized NN module as branching condition","### 🐛 Describe the bug

This was exposed by #144906; minimal repro:

```python
import torch

class Cache(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.key_cache = []

    def __len__(self):
        return len(self.key_cache)

@torch.compile(backend=""eager"")
def f(x):
    cache = Cache()
    if cache:
        return x + 1
    return x + 2

f(torch.ones(1))
```

### Error logs

```
Traceback (most recent call last):
  File ""/Users/ryanguo99/Documents/work/scratch/test-cond.py"", line 18, in <module>
    f(torch.ones(1))
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/eval_frame.py"", line 576, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/convert_frame.py"", line 1400, in __call__
    return self._torchdynamo_orig_callable(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/convert_frame.py"", line 1184, in __call__
    result = self._inner_convert(
             ^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/convert_frame.py"", line 565, in __call__
    return _compile(
           ^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/convert_frame.py"", line 1048, in _compile
    raise InternalTorchDynamoError(
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/convert_frame.py"", line 997, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_utils_internal.py"", line 95, in wrapper_function
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/convert_frame.py"", line 726, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/convert_frame.py"", line 760, in _compile_inner
    out_code = transform_code_object(code, transform)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/bytecode_transformation.py"", line 1403, in transform_code_object
    transformations(instructions, code_options)
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/convert_frame.py"", line 236, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/convert_frame.py"", line 680, in transform
    tracer.run()
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/symbolic_convert.py"", line 2913, in run
    super().run()
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/symbolic_convert.py"", line 1083, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/symbolic_convert.py"", line 993, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/_dynamo/symbolic_convert.py"", line 598, in inner
    if truth_fn(mod):
       ^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/scratch/test-cond.py"", line 9, in __len__
    return len(self.key_cache)
               ^^^^^^^^^^^^^^
  File ""/Users/ryanguo99/Documents/work/pytorch/torch/nn/modules/module.py"", line 1938, in __getattr__
    raise AttributeError(
torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'Cache' object has no attribute 'key_cache'

from user code:
   File ""/Users/ryanguo99/Documents/work/scratch/test-cond.py"", line 14, in f
    if cache:
```

### Versions

MacOS, Python 3.12.5, main 18638b91fe3.

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @amjames","closed","Refatoração","{triaged,""oncall: pt2"",""module: dynamo"",dynamo-triage-jan2025}",3,"2025-01-21 18:18:00","2025-01-28 19:45:20","2025-01-28 19:45:20","StrongerXi","StrongerXi",NULL,"https://github.com/pytorch/pytorch/issues/145284",7
"5","2802505066","2802505066","145283","[BE][export] add ""+export"" logging to de/serialization","adds de/serialization debug logging to `TORCH_LOGS=""+dynamic""`","closed","Refatoração","{Merged,ciflow/trunk,ciflow/inductor,""release notes: export""}",7,"2025-01-21 18:10:23","2025-01-23 19:48:53","2025-01-23 19:47:51","pianpwk",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145283",2
"6","2802335734","2802335734","145280","Use TORCH_CHECK instead of std::runtime_error in stack.h and ivalue.h","TORCH_CHECK will preserve the stacktrace for when TORCH_CPP_SHOW_STACKTRACES=1, whereas std::runtime_error will not.

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145280","closed","Refatoração","{better-engineering,Merged,ciflow/trunk,""topic: not user facing""}",3,"2025-01-21 16:44:27","2025-01-21 23:04:13","2025-01-21 21:59:02","janeyx99",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145280",0
"7","2633775875","2633775875","139657","Migrate inductor-perf-test-nightly.yml to use linux.aws.a100","We are deprecating the `linux.gcp.a100` runners, in favor of its AWS counterpart labeled `linux.aws.a100` given the fact it is cheaper and should perform just as well.","closed","Refatoração","{Merged,""topic: not user facing""}",7,"2024-11-04 20:30:10","2024-11-06 10:37:04","2024-11-05 20:24:29","jeanschmidt","jeanschmidt",NULL,"https://github.com/pytorch/pytorch/pull/139657",1
"8","2633608355","2633608355","139650","[FSDP2] Make module-to-state mapping use weakrefs","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139650

Without this, `del model` does not free memory of a module with FSDP2 applied.

cc @H-Huang @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,ciflow/trunk,ciflow/periodic,ciflow/inductor,""release notes: distributed (fsdp2)""}",8,"2024-11-04 18:59:50","2024-12-06 02:12:16","2024-11-05 02:16:55","awgu",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139650",1
"9","2633580676","2633580676","139649","Disable foreach tests for complex128 internally","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139649","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-11-04 18:47:05","2024-12-05 02:16:35","2024-11-04 23:24:49","janeyx99",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139649",0
"10","2629706432","2629706432","139505","[BC-Breaking]Remove capture_pre_autograd_graph references in quantization","Summary:
As title

This is a BC-breaking change because graph produced by ""capture_pre_autograd_graph"" cannot be input to quantization anymore. But this is ok, since this API is deprecated for a while and is going to be deleted. We have removed all call sites of it. 


We remove the deprecated API references in code, docs, and tests.


We also removed two tests that specific to capture_pre_autograd_graph API.

Test Plan: CI

Differential Revision: D65351887




cc @ezyang @gchanan","closed","Refatoração","{""module: bc-breaking"",fb-exported,Merged,ciflow/trunk,""release notes: quantization"",""topic: bc breaking""}",38,"2024-11-01 19:12:02","2024-12-13 22:27:27","2024-12-13 22:26:24","yushangdi",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139505",42
"11","2629690038","2629690038","139503","[ONNX] Add opset version to individual nodes when building the graph","Add opset version to individual nodes when building the graph in the new exporter to make it easier to convert opset versions.

cc @titaiwangms @shubhambhokare1","closed","Refatoração","{""module: onnx"",triaged}",0,"2024-11-01 19:01:40","2024-11-21 03:02:48","2024-11-21 03:02:48","justinchuby","justinchuby",NULL,"https://github.com/pytorch/pytorch/issues/139503",20
"12","2781582154","2781582154","144606","[mps/inductor] Add support for exp().","inductor/test_silu now passes after this change.

cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,Reverted,ciflow/trunk,""topic: not user facing"",""module: mps"",ciflow/mps,""module: inductor"",""module: dynamo"",ciflow/inductor,ci-no-td}",12,"2025-01-11 03:30:30","2025-01-12 05:48:16","2025-01-12 00:38:13","dcci",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144606",1
"13","2781567984","2781567984","144605","[inductor] Add unbacked symints binding in ShapeProp","Summary: ShapeProp  doesn't know how to propagate unbacked. Patch it up to propagate unbacked symints like PropagateUnbackedSymInts.

Test Plan:
```
buck run mode/dev-nosan  fbcode//caffe2/test:fx -- -r test_shape_prop_unbacked_sym
```

Differential Revision: D68050073




cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""release notes: fx"",fx}",124,"2025-01-11 02:52:04","2025-01-13 21:31:25","2025-01-13 21:30:22","yushangdi",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144605",2
"15","2781504005","2781504005","144603","Modernize C++ code","Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{""module: cpu"",triaged,""open source"",Merged,ciflow/trunk,""release notes: mobile"",""release notes: quantization"",ciflow/mps,""module: dynamo"",ciflow/inductor}",3,"2025-01-11 01:40:15","2025-01-17 00:26:24","2025-01-17 00:25:21","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144603",6
"16","2781477674","2781477674","144597","[Dynamo] Supports autograd.Function forward returns constant","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #144597

Fixes #144142

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor}",3,"2025-01-11 00:53:05","2025-01-12 03:58:06","2025-01-12 03:53:13","yanboliang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144597",1
"17","2781458424","2781458424","144596","[Pipelining] Refactor common utils from test_pp_dp","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #144734
* __->__ #144596
* #144352

Split test_pp_dp into pp_ddp and pp_fsdp so its a bit more
concise and easier to add CP to the FSDP one.

Realize that 'use_new_runtime' parametrization was not even being used,
removing it saves a bunch of test time. We should migrate schedules to
the new runtime and have them be covered that way.  (And
test_schedule*.py are testing new runtime too).

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,ciflow/trunk,""topic: not user facing"",""module: pipelining""}",3,"2025-01-11 00:18:45","2025-01-14 20:14:27","2025-01-14 20:13:21","wconstab",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144596",3
"18","2781363704","2781363704","144592","[CUDA][TF32] Add some missing TF32 decorators to `test_nn.py`","Original authored by @bilal2vec 

cc @ptrblck @msaroufim @zasdfgbnm","closed","Refatoração","{""module: cuda"",""open source"",Merged,""module: tf32"",ciflow/trunk,""topic: not user facing""}",3,"2025-01-10 22:58:34","2025-01-11 16:22:03","2025-01-11 16:21:01","eqy",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144592",1
"19","2781359048","2781359048","144591","[BE] Enable test_public_bindings on MacOS","I've tried it locally and it works.. (One more reason to xfail rather than skip)","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",3,"2025-01-10 22:55:28","2025-01-12 00:35:51","2025-01-12 00:34:49","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144591",2
"20","2803103195","2803103195","145330","[be] fix flaky test aot_export_ cond caused by free symbol lifting and automatic dynamic shape","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145330

Fixes https://github.com/pytorch/pytorch/issues/139998#issuecomment-2605908426.

It seems to be an issue caused by the interaction between dynamoed hop X automatic dynamic shape X auto_lift_free symbols. The immediate error is that the asserteExpectedInline of the graph can sometimes be different e.g. see https://hud.pytorch.org/flakytest?name=test_aot_export_with_torch_cond&suite=TestAOTExport&limit=100, where sometimes the shapes are lifted as input to the cond and sometimes they're not. 

The root cause of the flakyness is that the two invocations of torch.cond triggers two torch.compile on the same code object ([code](https://github.com/pytorch/pytorch/blob/main/torch/_higher_order_ops/cond.py#L192)), and triggers automatic dynamic shape because in test_aot_export_with_torch_cond, x has shape (3, 4) while the pre_dispatch one has shape (2, 2). Because of we auto lift free symbols for dynamic shaped input, this causes cond sometimes have the shape as arguments and sometimes not.

This PR adds a simple fix by adding a _dynamo.reset before each torch.cond tests. This fixes the error by not triggering automatic dynamic shape.","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",9,"2025-01-22 00:41:07","2025-01-23 18:14:06","2025-01-23 18:13:01","ydwu4",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145330",1
"21","2803080575","2803080575","145327","[utilization] pipeline to create clean db records","upload_utilization_script to generate db-ready-insert records to s3
- generate two files: metadata and timeseries in ossci-utilization buckets
- convert log record to db format ones
- add unit test job for tools/stats/

Related Prs:
setup composite action for data pipeline: https://github.com/pytorch/pytorch/pull/145310
add permission for composite action to access S3 bucket: https://github.com/pytorch-labs/pytorch-gha-infra/pull/595
add insert logic in s3 replicator: https://github.com/pytorch/test-infra/pull/6217","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",5,"2025-01-22 00:20:19","2025-01-29 23:49:56","2025-01-29 23:48:52","yangw-dev",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145327",7
"30","2760246016","2760246016","143885","restore 'unused' variable to fix test_cuda_device_memory_allocated","This PR fix `test_cuda_multigpu.py::TestCudaMultiGPU::test_cuda_device_memory_allocated`
by restoring a deleted 'unused' variable from commit https://github.com/pytorch/pytorch/commit/d8c8ba24404ef892d4d948eb095b69d90b9ba7e6

cc @jithunnair-amd @jeffdaily @pruthvistony","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",5,"2024-12-27 00:23:47","2024-12-27 23:19:18","2024-12-27 23:18:15","dnikolaev-amd",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143885",0
"22","2803025609","2803025609","145322","[S481486] Move MTIA dynamic library loading from __init__.py to a separate module","Summary: As titled

Test Plan:
- Passed CI tests

buck2 test 'fbcode//mode/opt' fbcode//ai_infra/distributed_ai/pyper_local_run/tests/integration_tests:test_icvr_e2e_gpu -- --exact 'ai_infra/distributed_ai/pyper_local_run/tests/integration_tests:test_icvr_e2e_gpu - test_icvr_e2e_gpu (ai_infra.distributed_ai.pyper_local_run.tests.integration_tests.test_icvr_e2e_gpu.TestIcvrE2EGpu)' --run-disabled
```


https://www.internalfb.com/intern/testinfra/testconsole/testrun/9007199320480497/

Differential Revision: D68463242","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""topic: not user facing""}",7,"2025-01-21 23:29:17","2025-01-22 23:40:47","2025-01-22 23:39:45","chaos5958",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145322",1
"23","2595758108","2595758108","138262","Update viable strict workflow","Corresponds to https://github.com/pytorch/test-infra/pull/5775

Tested in https://github.com/pytorch/pytorch/actions/runs/11393196544/job/31700963325?pr=138262 by adding my branch to the environment and pointing the workflow at my test-infra branch and commenting out the parts that did the push + upload record to s3


Versioning would have been good for this...","closed","Refatoração","{Merged,""topic: not user facing""}",3,"2024-10-17 21:09:34","2024-11-18 02:11:01","2024-10-18 17:28:57","clee2000",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138262",1
"60","2653348877","2653348877","140451","[FR] Polish the log message for dtype mismatch and don't exit when too many mismatch","Summary:
1. We don't want to exit with exceptions when there are so many mismatches. We should just break and return.
2. Polish the message of dtype mismatch. This is because dtype of input/output is actually a list not a string. So we don't want to show a list of ['double'] in the output message.

Test Plan:
Testing on the case when we see too many collective dtype mismatch

 {F1958467224}

Differential Revision: D65841830




cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",fb-exported,Merged,ciflow/trunk,""topic: not user facing""}",8,"2024-11-12 21:18:48","2024-11-13 07:25:58","2024-11-13 07:24:55","fduwjj",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140451",1
"24","2595753960","2595753960","138260","Bugfix for passing None args to user defined Triton kernel","This is a PR to fix the following issue: https://github.com/pytorch/pytorch/issues/115344

In short, passing None as an arg. to a Triton kernel would cause problems:

Short repro of the specific bug fixed here:
```
import triton
import triton.language as tl

@triton.autotune( # E: Untyped decorator makes function ""sin_kernel"" untyped  [misc]
    configs=[
        triton.Config({'BLOCK_SIZE': 32}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE': 64}, num_stages=4, num_warps=4),
    ],
    key=['n_elements']
)
@triton.jit # E: Untyped decorator makes function ""sin_kernel"" untyped  [misc]
def sin_kernel( # E: Function is missing a return type annotation  [no-untyped-def]
    in_ptr0,
    out_ptr,
    n_elements,
    BLOCK_SIZE: ""tl.constexpr"",
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    if in_ptr0 is not None:
        x = tl.load(in_ptr0 + offsets, mask=mask)
    else:
        x = 0.
    output = tl.sin(x)
    tl.store(out_ptr + offsets, output, mask=mask)

import torch

def sin_triton(x, out):
    n_elements = out.numel()
    sin_kernel[(n_elements,)](x, out, n_elements)

x = torch.randn(65, device=""cuda"")
out = torch.empty_like(x)
out_compiled = torch.empty_like(x)

sin_triton_compiled = torch.compile(fullgraph=True)(sin_triton)

for first in (x, None):
    sin_triton(first, out)
    sin_triton_compiled(first, out_compiled)
    torch.testing.assert_close(out, out_compiled)
```
I've added a unit test to catch this issue in the future and the tests in ""test/inductor/test_triton_kernels.py""


topic: not user facing

================================================================================
Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138260


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov

Differential Revision: [D64615061](https://our.internmc.facebook.com/intern/diff/D64615061)","closed","Refatoração","{ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",9,"2024-10-17 21:06:36","2024-11-21 02:08:00","2024-10-21 14:56:49","SamGinzburg",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138260",4
"25","2595749670","2595749670","138259","[ROCm] index_put performance improvement","On ROCm, using a non-vectorized index_put kernel provides ~2x perf improvement over the hipified CUDA kernel.  None of the existing unit tests were exercising the large index case so a new unit test was added.

It was also noted that the scale value in the original kernel was hard-coded to 1.0 which would be a no-op, so it was removed from the simplified rocm kernel.

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing"",rocm,ciflow/rocm}",4,"2024-10-17 21:03:30","2024-10-22 15:22:49","2024-10-22 15:21:46","jeffdaily",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138259",5
"26","2595742712","2595742712","138258","more tests passing","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang","closed","Refatoração","{""module: inductor"",ciflow/inductor}",2,"2024-10-17 20:59:21","2024-12-01 02:19:50","2024-10-17 20:59:54","SamGinzburg",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138258",0
"27","2595742509","2595742509","138257","fewer failing tests","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang","closed","Refatoração","{""module: inductor"",ciflow/inductor}",2,"2024-10-17 20:59:14","2024-12-01 02:19:47","2024-10-17 21:00:07","SamGinzburg",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138257",0
"28","2760613728","2760613728","143898","pytorch2.5.1的版本支持这个算子了吗：aclnnFusedInferAttentionScoreV2","pytorch2.5.1的版本支持这个算子了吗：aclnnFusedInferAttentionScoreV2

cc @NmomoN @mengpenghui @fwenguang @cdzhan @1274085042 @PHLens","closed","Refatoração","{""triage review"",""module: PrivateUse1""}",3,"2024-12-27 09:05:29","2025-01-06 18:06:17","2025-01-06 18:06:17","ZWQ2-A11Y",NULL,NULL,"https://github.com/pytorch/pytorch/issues/143898",10
"29","2760568736","2760568736","143897","[Inductor][CPP] Enable Epilogue Fusion for Grouped GEMM Template","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143897
* #143796

**Summary**
In this PR, we enable the epilogues fusion and code generation for Grouped GEMM. Here are the high-level description of how we implement it.

**Fusion**

- The Grouped GEMM Template produces a `Template Buffer` with a `MultiOutputLayout` and a set of `MultiOutput Buffers`, where each buffer corresponds to a specific GEMM.
- During the initial round of fusion, the `Template Buffer` and all associated `MultiOutput Buffers` are fused into a `FusedSchedulerNode` by extending the existing fusion design.
- In subsequent fusion rounds, this `FusedSchedulerNode` can further fuse with its epilogues, following the original fusion design principles.

**Code Gen**
We maintain a list of epilogues and codegen it one by one.

- If any of the GEMM has bias, we create  a extra `bias_add` epilogue and prepend it at first of the epilogue list.
- If any of the GEMM has no epilogue, we create a `to_bf16` copy epilogue and append it at last of the epilogue list. 

**TestPlan**
```
python -u -m pytest -s -v test/inductor/test_cpu_select_algorithm.py -k test_grouped_linear_epilogue
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov @BoyuanFeng","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",3,"2024-12-27 08:20:33","2025-01-14 06:08:57","2025-01-14 06:07:52","leslie-fang-intel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143897",18
"260","2653906186","2653906186","140491","Add `cmake` to requirements.txt","As one can not build PyTorch in clean venv if cmake is not installed","closed","Refatoração","{Merged,""topic: not user facing""}",3,"2024-11-13 02:12:09","2024-12-12 22:31:14","2024-11-13 02:53:27","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140491",0
"31","2760194003","2760194003","143882","Add support for list, tuple and dict in numeric debugger","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143882

Summary:
Previously numeric debugger only supports torch.Tensor, this PR adds support for list, tuple and dict as well

Test Plan:
python test/test_quantization.py -k test_extract_results_from_loggers_list_output

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D67660049](https://our.internmc.facebook.com/intern/diff/D67660049)","closed","Refatoração","{Merged,ciflow/trunk,""release notes: quantization""}",6,"2024-12-26 22:33:39","2025-01-28 02:04:00","2024-12-28 02:10:33","jerryzh168",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143882",2
"32","2678652406","2678652406","141223","[dtensor][random] allow user to manual_seed different seed on device mesh; only sync RNG state in WORLD when manual_seed has not been called","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #141532
* __->__ #141223
* #141220
* #141731

**Summary**
This PR proposes 4 changes to DTensor RNG management:
1. DTensor allows users to eagerly initialize the RNG tracker by calling `torch.distributed.tensor._random.manual_seed`.
2. DTensor `manual_seed` no longer checks the integrity of the `seed` argument. Users are responsible for setting the same seed on all ranks within an SPMD group, but if there are multiple separate SPMD groups (e.g. across pipeline stages), users should set a _different_ seed for each SPMD group. For cases like Pipeline Parallel, users can set different initial seed for pipelining stages by calling
```
world_mesh = init_device_mesh(
    device_type=""cuda"",
    mesh_shape=(2, 2, 2),
    mesh_dim_names=(""pp"", ""dp"", ""tp""),
)
pp_mesh = world_mesh[""pp""]
pp_rank = pp_mesh.get_local_rank()
spmd_mesh = world_mesh[""dp"", ""tp""]._flatten(""spmd"")  # this flattening is only needed if you need to call collective over this mesh
torch.distributed.tensor._random.manual_seed(123+pp_rank, spmd_mesh)
```

In other word, if users want to call `torch.distributed.tensor._random.manual_seed`, they will be responsible for passing in the right value and DTensor won't perform any checks on it. If the current rank is not a part of the mesh, it will use the current device RNG state to initialize.

3. `OffsetBasedRNGTracker` still performs RNG state synchronization by broadcasting the RNG state on rank 0 to `WORLD`. However, calling `torch.distributed.tensor._random.manual_seed` is an exception. In this case, no broadcast will happen.

4. Enforce that the `manual_seed` call only accept ""full mesh"" i.e. the DTensor RNG state on every rank must be set through the call. This makes sure that no rank has its RNG state left uninitialized and the SPMD ranks have their RNG state synchronous.

**Motivation**
tl;dr

1. Lazily initializing DTensor RNG tracker causes hang in non-SPMD code such as Pipeline Parallel.
2. Users may want to set different seed on ranks in one device mesh.
3. We want to keep the old behavior if users prefer not curating the RNG state and want to have DTensor take care of it.

see detail in https://github.com/pytorch/pytorch/issues/140301

**Test**
`pytest test/distributed/_tensor/test_random_ops.py`
`pytest test/distributed/tensor/parallel/test_tp_random_state.py`



cc @wanchaol @tianyu-l @wz337 @d4l3k @H-Huang @awgu @kwen2501 @fegin @fduwjj @wconstab @c-p-i-occ @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,""module: dtensor"",""release notes: distributed (dtensor)""}",5,"2024-11-21 09:10:54","2024-12-30 02:08:24","2024-11-29 07:59:38","XilunWu",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141223",8
"34","2625660572","2625660572","139350","Clean up Android jobs in CI","As an outcome of https://fburl.com/gdoc/voce5o06 and confirm with @iseeyuan, we can now clean up Android lite interpreter jobs on PyTorch CI. There is not much value in running them anymore.

It's stated in https://github.com/pytorch/android-demo-app/blob/master/README.md that ExecuTorch is the replacement now.","closed","Refatoração","{Merged,""topic: not user facing"",test-config/default}",3,"2024-10-31 00:35:26","2024-11-01 21:11:26","2024-11-01 21:10:22","huydhn",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139350",1
"35","2698669914","2698669914","141687","[ROCm] Enable inductor GEMM lowering for gfx11","This check doesn't make sense for some of the AMD gpus since they have the right amount of CUs but multi_processor_count returns WGPs on RDNA while still performing adequately. A lot of tests fail on modern archs due to this check defaulting them to not using the GEMMs backend.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""module: rocm"",""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor,rocm,ciflow/rocm,ciflow/inductor-rocm}",15,"2024-11-27 14:07:21","2024-12-19 11:01:06","2024-12-02 22:13:36","iupaikov-amd",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141687",5
"36","2698350420","2698350420","141679","Support remote caching requiring redis auth","cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",11,"2024-11-27 12:13:43","2024-12-12 17:08:56","2024-12-12 17:07:53","yasyf",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141679",15
"37","2782277352","2782277352","144636","Error loading ""pytorch\torch\lib\shm.dll"" or one of its dependencies when building from source (Windows 11)","### 🐛 Describe the bug

I have built torch from source, on https://github.com/pytorch/pytorch/commit/63569d9745b0530f8d66721e2a462c9f042e6b16 commit, using:

Magma for CUDA 12.6 (2.5.4)
MKL 2025.0 and 2020.2
cudNN 9.6
cuSPARSELt 0.6
cuDSS 0.4

On VS 2022 with MSVC v143

Using

```
cmake .. -GNinja ^
    -DCUDNN_LIBRARY_PATH=""C:/Program Files/NVIDIA/CUDNN/v9.6/lib/12.6/x64/cudnn.lib"" ^
    -DCUDNN_INCLUDE_PATH=""C:/Program Files/NVIDIA/CUDNN/v9.6/include/12.6"" ^
    -DCUSPARSELT_LIBRARY_PATH=""C:/Program Files/NVIDIA cuSPARSELt/v0.6/lib/cusparseLt.lib"" ^
    -DCUSPARSELT_INCLUDE_PATH=""C:/Program Files/NVIDIA cuSPARSELt/v0.6/include"" ^
    -DCUDSS_LIBRARY_PATH=""C:/Program Files/NVIDIA cuDSS/v0.4/lib/12/cudss.lib"" ^
    -DCUDSS_INCLUDE_PATH=""C:/Program Files/NVIDIA cuDSS/v0.4/include"" ^
	-DUSE_CUDA=ON ^
	-DUSE_FLASH_ATTENTION=ON ^
    -DUSE_CUDNN=ON ^
    -DUSE_CUSPARSELT=ON ^
    -DUSE_CUDSS=ON ^
    -DCMAKE_BUILD_TYPE=Release ^
    -DUSE_STATIC_DISPATCH=OFF ^
    -DCMAKE_INSTALL_PREFIX=../torch 
 ```
 
 And then doing
 
```
 cmake --build . --target install --config Release -j 6
 cd ..
 pip install -e .
```

When importing, I get

```
(py310) C:\Users\User\Desktop\pytorch_compile\pytorch\build>python
Python 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\User\Desktop\pytorch_compile\pytorch\torch\__init__.py"", line 274, in <module>
    _load_dll_libraries()
  File ""C:\Users\User\Desktop\pytorch_compile\pytorch\torch\__init__.py"", line 270, in _load_dll_libraries
    raise err
OSError: [WinError 126] No se puede encontrar el módulo especificado. Error loading ""C:\Users\User\Desktop\pytorch_compile\pytorch\torch\lib\shm.dll"" or one of its dependencies.
```

Issue happens with both Python 3.10 and 3.12

Complete log when running cmake is

```
(py310) C:\Users\User\Desktop\pytorch_compile\pytorch\build>cmake .. -GNinja ^
¿Más?     -DCUDNN_LIBRARY_PATH=""C:/Program Files/NVIDIA/CUDNN/v9.6/lib/12.6/x64/cudnn.lib"" ^
¿Más?     -DCUDNN_INCLUDE_PATH=""C:/Program Files/NVIDIA/CUDNN/v9.6/include/12.6"" ^
¿Más?     -DCUSPARSELT_LIBRARY_PATH=""C:/Program Files/NVIDIA cuSPARSELt/v0.6/lib/cusparseLt.lib"" ^
¿Más?     -DCUSPARSELT_INCLUDE_PATH=""C:/Program Files/NVIDIA cuSPARSELt/v0.6/include"" ^
¿Más?     -DCUDSS_LIBRARY_PATH=""C:/Program Files/NVIDIA cuDSS/v0.4/lib/12/cudss.lib"" ^
¿Más?     -DCUDSS_INCLUDE_PATH=""C:/Program Files/NVIDIA cuDSS/v0.4/include"" ^
¿Más? -DUSE_CUDA=ON ^
¿Más? -DUSE_FLASH_ATTENTION=ON ^
¿Más?     -DUSE_CUDNN=ON ^
¿Más?     -DUSE_CUSPARSELT=ON ^
¿Más?     -DUSE_CUDSS=ON ^
¿Más?     -DCMAKE_BUILD_TYPE=Release ^
¿Más? -DUSE_STATIC_DISPATCH=OFF ^
¿Más?     -DCMAKE_INSTALL_PREFIX=../torch
-- The CXX compiler identification is MSVC 19.42.34435.0
-- The C compiler identification is MSVC 19.42.34435.0
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.42.34433/bin/Hostx64/x64/cl.exe - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.42.34433/bin/Hostx64/x64/cl.exe - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Not forcing any particular BLAS to be found
CMake Warning at CMakeLists.txt:422 (message):
  TensorPipe cannot be used on Windows.  Set it to OFF


CMake Warning at CMakeLists.txt:424 (message):
  KleidiAI cannot be used on Windows.  Set it to OFF


-- Performing Test C_HAS_AVX_1
-- Performing Test C_HAS_AVX_1 - Success
-- Performing Test C_HAS_AVX2_1
-- Performing Test C_HAS_AVX2_1 - Success
-- Performing Test C_HAS_AVX512_1
-- Performing Test C_HAS_AVX512_1 - Success
-- Performing Test CXX_HAS_AVX_1
-- Performing Test CXX_HAS_AVX_1 - Success
-- Performing Test CXX_HAS_AVX2_1
-- Performing Test CXX_HAS_AVX2_1 - Success
-- Performing Test CXX_HAS_AVX512_1
-- Performing Test CXX_HAS_AVX512_1 - Success
-- Current compiler supports avx2 extension. Will build perfkernels.
-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS
-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS - Success
-- Current compiler supports avx512f extension. Will build fbgemm.
-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY
-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY - Failed
-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY
-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY - Failed
-- Could not find hardware support for NEON on this machine.
-- No OMAP3 processor on this machine.
-- No OMAP4 processor on this machine.
-- Compiler does not support SVE extension. Will not build perfkernels.
-- Performing Test HAS/UTF_8
-- Performing Test HAS/UTF_8 - Success
-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6 (found version ""12.6"")
-- The CUDA compiler identification is NVIDIA 12.6.85
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/bin/nvcc.exe - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/include (found version ""12.6.85"")
-- PyTorch: CUDA detected: 12.6
-- PyTorch: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/bin/nvcc.exe
-- PyTorch: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6
-- PyTorch: Header version is: 12.6
-- Found Python: C:/Users/User/anaconda3/envs/py310/python.exe (found version ""3.10.16"") found components: Interpreter
CMake Warning at cmake/public/cuda.cmake:140 (message):
  Failed to compute shorthash for libnvrtc.so
Call Stack (most recent call first):
  cmake/Dependencies.cmake:44 (include)
  CMakeLists.txt:865 (include)


-- Found nvtx3: C:/Users/User/Desktop/pytorch_compile/pytorch/third_party/NVTX/c/include
-- Found CUDNN: C:/Program Files/NVIDIA/CUDNN/v9.6/lib/12.6/x64/cudnn.lib
-- Found CUSPARSELT: C:/Program Files/NVIDIA cuSPARSELt/v0.6/lib/cusparseLt.lib
-- Found CUDSS: C:/Program Files/NVIDIA cuDSS/v0.4/lib/12/cudss.lib
-- USE_CUFILE is set to 0. Compiling without cuFile support
-- Autodetected CUDA architecture(s):  8.9 8.9 8.6
-- Added CUDA NVCC flags for: -gencode;arch=compute_89,code=sm_89;-gencode;arch=compute_86,code=sm_86
CMake Warning at cmake/Dependencies.cmake:95 (message):
  Not compiling with XPU.  Could NOT find SYCL.Suppress this warning with
  -DUSE_XPU=OFF.
Call Stack (most recent call first):
  CMakeLists.txt:865 (include)


-- Building using own protobuf under third_party per request.
-- Use custom protobuf build.
CMake Deprecation Warning at third_party/protobuf/cmake/CMakeLists.txt:2 (cmake_minimum_required):
  Compatibility with CMake < 3.5 will be removed from a future version of
  CMake.

  Update the VERSION argument <min> value or use a ...<max> suffix to tell
  CMake that the project does not need compatibility with older versions.


--
-- 3.13.0.0
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - not found
-- Found Threads: TRUE
-- Caffe2 protobuf include directory: $<BUILD_INTERFACE:C:/Users/User/Desktop/pytorch_compile/pytorch/third_party/protobuf/src>$<INSTALL_INTERFACE:include>
-- Trying to find preferred BLAS backend of choice: MKL
-- MKL_THREADING = OMP
-- Looking for sys/types.h
-- Looking for sys/types.h - found
-- Looking for stdint.h
-- Looking for stdint.h - found
-- Looking for stddef.h
-- Looking for stddef.h - found
-- Check size of void*
-- Check size of void* - done
-- Looking for cblas_sgemm
-- Looking for cblas_sgemm - found
-- Looking for cblas_gemm_bf16bf16f32
-- Looking for cblas_gemm_bf16bf16f32 - found
-- Looking for cblas_gemm_f16f16f32
-- Looking for cblas_gemm_f16f16f32 - found
-- MKL libraries: C:/Program Files (x86)/Intel/oneAPI/mkl/2025.0/lib/mkl_intel_lp64_dll.lib;C:/Program Files (x86)/Intel/oneAPI/mkl/2025.0/lib/mkl_intel_thread_dll.lib;C:/Program Files (x86)/Intel/oneAPI/mkl/2025.0/lib/mkl_core_dll.lib;C:/Users/User/anaconda3/envs/py310/Library/lib/libiomp5md.lib
-- MKL include directory: C:/Program Files (x86)/Intel/oneAPI/mkl/2025.0/include
-- MKL OpenMP type: Intel
-- MKL OpenMP library: C:/Users/User/anaconda3/envs/py310/Library/lib/libiomp5md.lib
-- The ASM compiler identification is MSVC
-- Found assembler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.42.34433/bin/Hostx64/x64/cl.exe
-- Building for XNNPACK_TARGET_PROCESSOR: x86_64
-- Generating microkernels.cmake
Duplicate microkernel definition: src\qs8-qc4w-packw\gen\qs8-qc4w-packw-x8c8-gemm-goi-avx256vnni.c and src\qs8-qc4w-packw\gen\qs8-qc4w-packw-x8c8-gemm-goi-avxvnni.c (1th function)
Duplicate microkernel definition: src\qs8-qc4w-packw\gen\qs8-qc4w-packw-x8c8-gemm-goi-avxvnni.c and src\qs8-qc4w-packw\gen\qs8-qc4w-packw-x8c8-gemm-goi-scalar.c
No microkernel found in src\reference\binary-elementwise.cc
No microkernel found in src\reference\packing.cc
No microkernel found in src\reference\unary-elementwise.cc
CMake Warning (dev) at third_party/fbgemm/CMakeLists.txt:93 (find_package):
  Policy CMP0148 is not set: The FindPythonInterp and FindPythonLibs modules
  are removed.  Run ""cmake --help-policy CMP0148"" for policy details.  Use
  the cmake_policy command to set the policy and suppress this warning.

This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found PythonInterp: C:/Users/User/anaconda3/envs/py310/python.exe (found version ""3.10.16"")
-- Performing Test COMPILER_SUPPORTS_AVX512
-- Performing Test COMPILER_SUPPORTS_AVX512 - Success
-- Check OMP with lib C:/Users/User/anaconda3/envs/py310/Library/lib/libiomp5md.lib and flags -openmp:experimental
-- Check OMP with lib C:/Users/User/anaconda3/envs/py310/Library/lib/libiomp5md.lib and flags -openmp:experimental
CMake Warning (dev) at C:/Users/User/anaconda3/envs/py310/Library/share/cmake-3.27/Modules/FindPackageHandleStandardArgs.cmake:438 (message):
  The package name passed to `find_package_handle_standard_args` (OpenMP_C)
  does not match the name of the calling package (OpenMP).  This can lead to
  problems in calling code that expects `find_package` result variables
  (e.g., `_FOUND`) to follow a certain pattern.
Call Stack (most recent call first):
  cmake/Modules/FindOpenMP.cmake:590 (find_package_handle_standard_args)
  third_party/fbgemm/CMakeLists.txt:136 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found OpenMP_C: -openmp:experimental
CMake Warning (dev) at C:/Users/User/anaconda3/envs/py310/Library/share/cmake-3.27/Modules/FindPackageHandleStandardArgs.cmake:438 (message):
  The package name passed to `find_package_handle_standard_args` (OpenMP_CXX)
  does not match the name of the calling package (OpenMP).  This can lead to
  problems in calling code that expects `find_package` result variables
  (e.g., `_FOUND`) to follow a certain pattern.
Call Stack (most recent call first):
  cmake/Modules/FindOpenMP.cmake:590 (find_package_handle_standard_args)
  third_party/fbgemm/CMakeLists.txt:136 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found OpenMP_CXX: -openmp:experimental
-- Found OpenMP: TRUE
CMake Warning at third_party/fbgemm/CMakeLists.txt:138 (message):
  OpenMP found! OpenMP_C_INCLUDE_DIRS =


CMake Warning at third_party/fbgemm/CMakeLists.txt:232 (message):
  ==========


CMake Warning at third_party/fbgemm/CMakeLists.txt:233 (message):
  CMAKE_BUILD_TYPE = Release


CMake Warning at third_party/fbgemm/CMakeLists.txt:234 (message):
  CMAKE_CXX_FLAGS_DEBUG is /Z7 /Ob0 /Od /RTC1 /bigobj


CMake Warning at third_party/fbgemm/CMakeLists.txt:235 (message):
  CMAKE_CXX_FLAGS_RELEASE is /O2 /Ob2 /DNDEBUG /bigobj


CMake Warning at third_party/fbgemm/CMakeLists.txt:236 (message):
  ==========


** AsmJit Summary **
   ASMJIT_DIR=C:/Users/User/Desktop/pytorch_compile/pytorch/third_party/fbgemm/third_party/asmjit
   ASMJIT_TEST=FALSE
   ASMJIT_TARGET_TYPE=SHARED
   ASMJIT_DEPS=
   ASMJIT_LIBS=asmjit
   ASMJIT_CFLAGS=
   ASMJIT_PRIVATE_CFLAGS=-MP;-GF;-Zc:__cplusplus;-Zc:inline;-Zc:strictStrings;-Zc:threadSafeInit-;-W4
   ASMJIT_PRIVATE_CFLAGS_DBG=-GS
   ASMJIT_PRIVATE_CFLAGS_REL=-GS-;-O2;-Oi
CMake Deprecation Warning at third_party/ittapi/CMakeLists.txt:7 (cmake_minimum_required):
  Compatibility with CMake < 3.5 will be removed from a future version of
  CMake.

  Update the VERSION argument <min> value or use a ...<max> suffix to tell
  CMake that the project does not need compatibility with older versions.


CMake Deprecation Warning at third_party/FP16/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):
  Compatibility with CMake < 3.5 will be removed from a future version of
  CMake.

  Update the VERSION argument <min> value or use a ...<max> suffix to tell
  CMake that the project does not need compatibility with older versions.


CMake Deprecation Warning at third_party/psimd/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):
  Compatibility with CMake < 3.5 will be removed from a future version of
  CMake.

  Update the VERSION argument <min> value or use a ...<max> suffix to tell
  CMake that the project does not need compatibility with older versions.


-- Using third party subdirectory Eigen.
-- Found Python: C:/Users/User/anaconda3/envs/py310/python.exe (found version ""3.10.16"") found components: Interpreter Development.Module NumPy
-- Using third_party/pybind11.
-- pybind11 include dirs: C:/Users/User/Desktop/pytorch_compile/pytorch/cmake/../third_party/pybind11/include
-- Could NOT find OpenTelemetryApi (missing: OpenTelemetryApi_INCLUDE_DIRS)
-- Using third_party/opentelemetry-cpp.
-- opentelemetry api include dirs: C:/Users/User/Desktop/pytorch_compile/pytorch/cmake/../third_party/opentelemetry-cpp/api/include
-- Could NOT find MPI_C (missing: MPI_C_LIB_NAMES MPI_C_HEADER_DIR MPI_C_WORKS)
-- Could NOT find MPI_CXX (missing: MPI_CXX_LIB_NAMES MPI_CXX_HEADER_DIR MPI_CXX_WORKS)
-- Could NOT find MPI (missing: MPI_C_FOUND MPI_CXX_FOUND)
CMake Warning at cmake/Dependencies.cmake:945 (message):
  Not compiling with MPI.  Suppress this warning with -DUSE_MPI=OFF
Call Stack (most recent call first):
  CMakeLists.txt:865 (include)


-- Adding OpenMP CXX_FLAGS: -openmp:experimental
-- Will link against OpenMP libraries: C:/Users/User/anaconda3/envs/py310/Library/lib/libiomp5md.lib
-- Found CUB: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/include
CMake Deprecation Warning at third_party/gloo/CMakeLists.txt:1 (cmake_minimum_required):
  Compatibility with CMake < 3.5 will be removed from a future version of
  CMake.

  Update the VERSION argument <min> value or use a ...<max> suffix to tell
  CMake that the project does not need compatibility with older versions.


CMake Warning (dev) at third_party/gloo/CMakeLists.txt:21 (option):
  Policy CMP0077 is not set: option() honors normal variables.  Run ""cmake
  --help-policy CMP0077"" for policy details.  Use the cmake_policy command to
  set the policy and suppress this warning.

  For compatibility with older versions of CMake, option is clearing the
  normal variable 'BUILD_BENCHMARK'.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at third_party/gloo/CMakeLists.txt:35 (option):
  Policy CMP0077 is not set: option() honors normal variables.  Run ""cmake
  --help-policy CMP0077"" for policy details.  Use the cmake_policy command to
  set the policy and suppress this warning.

  For compatibility with older versions of CMake, option is clearing the
  normal variable 'USE_NCCL'.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at third_party/gloo/CMakeLists.txt:36 (option):
  Policy CMP0077 is not set: option() honors normal variables.  Run ""cmake
  --help-policy CMP0077"" for policy details.  Use the cmake_policy command to
  set the policy and suppress this warning.

  For compatibility with older versions of CMake, option is clearing the
  normal variable 'USE_RCCL'.
This warning is for project developers.  Use -Wno-dev to suppress it.

-- MSVC detected
-- Set USE_REDIS OFF
-- Set USE_IBVERBS OFF
-- Set USE_NCCL OFF
-- Set USE_RCCL OFF
-- Set USE_LIBUV ON
-- Only USE_LIBUV is supported on Windows
-- Enabling sccache for CXX
-- Enabling sccache for C
-- Gloo build as SHARED library
CMake Warning (dev) at third_party/gloo/cmake/Cuda.cmake:109 (find_package):
  Policy CMP0074 is not set: find_package uses <PackageName>_ROOT variables.
  Run ""cmake --help-policy CMP0074"" for policy details.  Use the cmake_policy
  command to set the policy and suppress this warning.

  CMake variable CUDAToolkit_ROOT is set to:

    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6

  For compatibility, CMake is ignoring the variable.
Call Stack (most recent call first):
  third_party/gloo/cmake/Dependencies.cmake:115 (include)
  third_party/gloo/CMakeLists.txt:111 (include)
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/include (found suitable version ""12.6.85"", minimum required is ""7.0"")
-- CUDA detected: 12.6.85
CMake Warning (dev) at third_party/onnx/CMakeLists.txt:106 (find_package):
  Policy CMP0148 is not set: The FindPythonInterp and FindPythonLibs modules
  are removed.  Run ""cmake --help-policy CMP0148"" for policy details.  Use
  the cmake_policy command to set the policy and suppress this warning.

This warning is for project developers.  Use -Wno-dev to suppress it.

Generated: C:/Users/User/Desktop/pytorch_compile/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch-ml.proto
Generated: C:/Users/User/Desktop/pytorch_compile/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch-ml.proto
Generated: C:/Users/User/Desktop/pytorch_compile/pytorch/build/third_party/onnx/onnx/onnx-data_onnx_torch.proto
--
-- ******** Summary ********
--   CMake version                     : 3.27.4
--   CMake command                     : C:/Users/User/anaconda3/envs/py310/Library/bin/cmake.exe
--   System                            : Windows
--   C++ compiler                      : C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.42.34433/bin/Hostx64/x64/cl.exe
--   C++ compiler version              : 19.42.34435.0
--   CXX flags                         : /DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL /EHsc /wd26812
--   Build type                        : Release
--   Compile definitions               : ONNX_ML=1;ONNXIFI_ENABLE_EXT=1;__STDC_FORMAT_MACROS
--   CMAKE_PREFIX_PATH                 : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6
--   CMAKE_INSTALL_PREFIX              : C:/Users/User/Desktop/pytorch_compile/pytorch/torch
--   CMAKE_MODULE_PATH                 : C:/Users/User/Desktop/pytorch_compile/pytorch/cmake/Modules;C:/Users/User/Desktop/pytorch_compile/pytorch/cmake/public/../Modules_CUDA_fix
--
--   ONNX version                      : 1.17.0
--   ONNX NAMESPACE                    : onnx_torch
--   ONNX_USE_LITE_PROTO               : OFF
--   USE_PROTOBUF_SHARED_LIBS          : OFF
--   Protobuf_USE_STATIC_LIBS          : ON
--   ONNX_DISABLE_EXCEPTIONS           : OFF
--   ONNX_DISABLE_STATIC_REGISTRATION  : OFF
--   ONNX_WERROR                       : OFF
--   ONNX_BUILD_TESTS                  : OFF
--   ONNX_BUILD_SHARED_LIBS            :
--   BUILD_SHARED_LIBS                 : OFF
--
--   Protobuf compiler                 :
--   Protobuf includes                 :
--   Protobuf libraries                :
--   BUILD_ONNX_PYTHON                 : OFF
-- Found CUDA with FP16 support, compiling with torch.cuda.HalfTensor
-- Adding -DNDEBUG to compile flags
-- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2
-- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2 - False
-- Compiling with MAGMA support
-- MAGMA INCLUDE DIRECTORIES: C:/magma_dir/include
-- MAGMA LIBRARIES: C:/magma_dir/lib/magma.lib
-- MAGMA V2 check: 0
-- Could not find hardware support for NEON on this machine.
-- No OMAP3 processor on this machine.
-- No OMAP4 processor on this machine.
-- Looking for sbgemm_
-- Looking for sbgemm_ - not found
-- Found a library with LAPACK API (mkl).
disabling ROCM because NOT USE_ROCM is set
-- MIOpen not found. Compiling without MIOpen support
-- Will build oneDNN UKERNEL
-- MKLDNN_CPU_RUNTIME = OMP
CMake Deprecation Warning at third_party/ideep/mkl-dnn/CMakeLists.txt:17 (cmake_minimum_required):
  Compatibility with CMake < 3.5 will be removed from a future version of
  CMake.

  Update the VERSION argument <min> value or use a ...<max> suffix to tell
  CMake that the project does not need compatibility with older versions.


-- DNNL_TARGET_ARCH: X64
-- DNNL_LIBRARY_NAME: dnnl
CMake Warning (dev) at C:/Users/User/anaconda3/envs/py310/Library/share/cmake-3.27/Modules/FindPackageHandleStandardArgs.cmake:438 (message):
  The package name passed to `find_package_handle_standard_args` (OpenMP_C)
  does not match the name of the calling package (OpenMP).  This can lead to
  problems in calling code that expects `find_package` result variables
  (e.g., `_FOUND`) to follow a certain pattern.
Call Stack (most recent call first):
  cmake/Modules/FindOpenMP.cmake:590 (find_package_handle_standard_args)
  third_party/ideep/mkl-dnn/cmake/OpenMP.cmake:55 (find_package)
  third_party/ideep/mkl-dnn/CMakeLists.txt:119 (include)
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found OpenMP_C: -openmp:experimental
CMake Warning (dev) at C:/Users/User/anaconda3/envs/py310/Library/share/cmake-3.27/Modules/FindPackageHandleStandardArgs.cmake:438 (message):
  The package name passed to `find_package_handle_standard_args` (OpenMP_CXX)
  does not match the name of the calling package (OpenMP).  This can lead to
  problems in calling code that expects `find_package` result variables
  (e.g., `_FOUND`) to follow a certain pattern.
Call Stack (most recent call first):
  cmake/Modules/FindOpenMP.cmake:590 (find_package_handle_standard_args)
  third_party/ideep/mkl-dnn/cmake/OpenMP.cmake:55 (find_package)
  third_party/ideep/mkl-dnn/CMakeLists.txt:119 (include)
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found OpenMP_CXX: -openmp:experimental
-- Found Git: C:/Program Files/Git/cmd/git.exe (found version ""2.47.0.windows.1"")
-- Enabled testing coverage: CI
-- Enabled workload: TRAINING
-- Enabled primitives: ALL
-- Enabled primitive CPU ISA: ALL
-- Enabled primitive GPU ISA: ALL
-- Enabled GeMM kernels ISA: ALL
-- Primitive cache is enabled
-- Experimental functionality for ukernels is enabled
-- The ASM_MASM compiler identification is MSVC
-- Found assembler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.42.34433/bin/Hostx64/x64/ml64.exe
-- Graph component is enabled
-- Graph compiler backend is disabled.
-- Found MKL-DNN: TRUE
-- {fmt} version: 11.1.1
-- Build type: Release
-- Using CPU-only version of Kineto
-- Configuring Kineto dependency:
--   KINETO_SOURCE_DIR = C:/Users/User/Desktop/pytorch_compile/pytorch/third_party/kineto/libkineto
--   KINETO_BUILD_TESTS = OFF
--   KINETO_LIBRARY_TYPE = static
CMake Warning (dev) at third_party/kineto/libkineto/CMakeLists.txt:15 (find_package):
  Policy CMP0148 is not set: The FindPythonInterp and FindPythonLibs modules
  are removed.  Run ""cmake --help-policy CMP0148"" for policy details.  Use
  the cmake_policy command to set the policy and suppress this warning.

This warning is for project developers.  Use -Wno-dev to suppress it.

INFO CUDA_SOURCE_DIR =
INFO ROCM_SOURCE_DIR =
INFO CUPTI unavailable or disabled - not building GPU profilers
-- Kineto: FMT_SOURCE_DIR = C:/Users/User/Desktop/pytorch_compile/pytorch/third_party/fmt
-- Kineto: FMT_INCLUDE_DIR = C:/Users/User/Desktop/pytorch_compile/pytorch/third_party/fmt/include
INFO CUPTI_INCLUDE_DIR = /extras/CUPTI/include
INFO ROCTRACER_INCLUDE_DIR = /include/roctracer
INFO DYNOLOG_INCLUDE_DIR = C:/Users/User/Desktop/pytorch_compile/pytorch/third_party/kineto/libkineto/third_party/dynolog/
INFO IPCFABRIC_INCLUDE_DIR = C:/Users/User/Desktop/pytorch_compile/pytorch/third_party/kineto/libkineto/third_party/dynolog//dynolog/src/ipcfabric/
-- Configured Kineto (CPU)
-- Performing Test HAS/WD4624
-- Performing Test HAS/WD4624 - Success
-- Performing Test HAS/WD4068
-- Performing Test HAS/WD4068 - Success
-- Performing Test HAS/WD4067
-- Performing Test HAS/WD4067 - Success
-- Performing Test HAS/WD4267
-- Performing Test HAS/WD4267 - Success
-- Performing Test HAS/WD4661
-- Performing Test HAS/WD4661 - Success
-- Performing Test HAS/WD4717
-- Performing Test HAS/WD4717 - Success
-- Performing Test HAS/WD4244
-- Performing Test HAS/WD4244 - Success
-- Performing Test HAS/WD4804
-- Performing Test HAS/WD4804 - Success
-- Performing Test HAS/WD4273
-- Performing Test HAS/WD4273 - Success
-- Performing Test HAS_WNO_STRINGOP_OVERFLOW
-- Performing Test HAS_WNO_STRINGOP_OVERFLOW - Failed
--
-- Use the C++ compiler to compile (MI_USE_CXX=ON)
--
-- Library base name: mimalloc
-- Version          : 1.8
-- Build type       : release
-- C++ Compiler     : C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.42.34433/bin/Hostx64/x64/cl.exe
-- Compiler flags   : /Zc:__cplusplus
-- Compiler defines :
-- Link libraries   : psapi;shell32;user32;advapi32;bcrypt
-- Build targets    : static
--
-- Performing Test HAS_WDEPRECATED
-- Performing Test HAS_WDEPRECATED - Failed
-- don't use NUMA
-- Looking for backtrace
-- Looking for backtrace - not found
-- Could NOT find Backtrace (missing: Backtrace_LIBRARY Backtrace_INCLUDE_DIR)
-- Autodetected CUDA architecture(s):  8.9 8.9 8.6
-- headers outputs:
-- sources outputs:
-- declarations_yaml outputs:
-- Performing Test COMPILER_SUPPORTS_NO_AVX256_SPLIT
-- Performing Test COMPILER_SUPPORTS_NO_AVX256_SPLIT - Failed
-- Using ATen parallel backend: OMP
-- Found OpenSSL: C:/Users/User/anaconda3/envs/py310/Library/lib/libcrypto.lib (found version ""3.4.0"")
-- Check size of long double
-- Check size of long double - done
-- Performing Test COMPILER_SUPPORTS_FLOAT128
-- Performing Test COMPILER_SUPPORTS_FLOAT128 - Failed
-- Performing Test COMPILER_SUPPORTS_SSE2
-- Performing Test COMPILER_SUPPORTS_SSE2 - Success
-- Performing Test COMPILER_SUPPORTS_SSE4
-- Performing Test COMPILER_SUPPORTS_SSE4 - Success
-- Performing Test COMPILER_SUPPORTS_AVX
-- Performing Test COMPILER_SUPPORTS_AVX - Success
-- Performing Test COMPILER_SUPPORTS_FMA4
-- Performing Test COMPILER_SUPPORTS_FMA4 - Success
-- Performing Test COMPILER_SUPPORTS_AVX2
-- Performing Test COMPILER_SUPPORTS_AVX2 - Success
-- Performing Test COMPILER_SUPPORTS_AVX512F
-- Performing Test COMPILER_SUPPORTS_AVX512F - Success
-- Found OpenMP_C: -openmp:experimental (found version ""2.0"")
-- Found OpenMP_CXX: -openmp:experimental (found version ""2.0"")
-- Found OpenMP: TRUE (found version ""2.0"")
-- Performing Test COMPILER_SUPPORTS_OPENMP
-- Performing Test COMPILER_SUPPORTS_OPENMP - Success
-- Performing Test COMPILER_SUPPORTS_OMP_SIMD
-- Performing Test COMPILER_SUPPORTS_OMP_SIMD - Failed
-- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES
-- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES - Failed
-- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH
-- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH - Failed
-- Performing Test COMPILER_SUPPORTS_SYS_GETRANDOM
-- Performing Test COMPILER_SUPPORTS_SYS_GETRANDOM - Failed
-- Configuring build for SLEEF-v3.7.0
   Target system: Windows-10.0.26100
   Target processor: AMD64
   Host system: Windows-10.0.26100
   Host processor: AMD64
   Detected C compiler: MSVC @ C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.42.34433/bin/Hostx64/x64/cl.exe
   CMake: 3.27.4
   Make program: C:/Users/User/anaconda3/envs/py310/Library/bin/ninja.exe
-- Using option `/D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE  ` to compile libsleef
-- Building shared libs : OFF
-- Building static test bins: OFF
-- MPFR : LIB_MPFR-NOTFOUND
-- GMP : LIBGMP-NOTFOUND
-- RT :
-- FFTW3 : LIBFFTW3-NOTFOUND
-- OPENSSL : 3.4.0
-- SDE : SDE_COMMAND-NOTFOUND
-- COMPILER_SUPPORTS_OPENMP : FALSE
AT_INSTALL_INCLUDE_DIR include/ATen/core
core header install: C:/Users/User/Desktop/pytorch_compile/pytorch/build/aten/src/ATen/core/TensorBody.h
core header install: C:/Users/User/Desktop/pytorch_compile/pytorch/build/aten/src/ATen/core/aten_interned_strings.h
core header install: C:/Users/User/Desktop/pytorch_compile/pytorch/build/aten/src/ATen/core/enum_tag.h
-- Autodetected CUDA architecture(s):  8.9 8.9 8.6
CMake Warning at CMakeLists.txt:1275 (message):
  Generated cmake files are only fully tested if one builds with system glog,
  gflags, and protobuf.  Other settings may generate files that are not well
  tested.


--
-- ******** Summary ********
-- General:
--   CMake version         : 3.27.4
--   CMake command         : C:/Users/User/anaconda3/envs/py310/Library/bin/cmake.exe
--   System                : Windows
--   C++ compiler          : C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.42.34433/bin/Hostx64/x64/cl.exe
--   C++ compiler id       : MSVC
--   C++ compiler version  : 19.42.34435.0
--   Using ccache if found : OFF
--   CXX flags             : /DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273
--   Shared LD flags       : /machine:x64 /ignore:4049 /ignore:4217 /ignore:4099
--   Static LD flags       : /machine:x64 /ignore:4049 /ignore:4217 /ignore:4099
--   Module LD flags       : /machine:x64 /ignore:4049 /ignore:4217 /ignore:4099
--   Build type            : Release
--   Compile definitions   : ONNX_ML=1;ONNXIFI_ENABLE_EXT=1;ONNX_NAMESPACE=onnx_torch;_CRT_SECURE_NO_DEPRECATE=1;IDEEP_USE_MKL;USE_EXTERNAL_MZCRC;MINIZ_DISABLE_ZIP_READER_CRC32_CHECKS;FLASHATTENTION_DISABLE_ALIBI;WIN32_LEAN_AND_MEAN;_UCRT_LEGACY_INFINITY;NOMINMAX;USE_MIMALLOC
--   CMAKE_PREFIX_PATH     : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6
--   CMAKE_INSTALL_PREFIX  : C:/Users/User/Desktop/pytorch_compile/pytorch/torch
--   USE_GOLD_LINKER       : OFF
--
--   TORCH_VERSION         : 2.7.0
--   BUILD_STATIC_RUNTIME_BENCHMARK: OFF
--   BUILD_BINARY          : OFF
--   BUILD_CUSTOM_PROTOBUF : ON
--     Link local protobuf : ON
--   BUILD_PYTHON          : ON
--     Python version      : 3.10.16
--     Python executable   : C:/Users/User/anaconda3/envs/py310/python.exe
--     Python library      : C:/Users/User/anaconda3/envs/py310/libs/python310.lib
--     Python includes     : C:/Users/User/anaconda3/envs/py310/include
--     Python site-package : C:\Users\User\anaconda3\envs\py310\Lib\site-packages
--   BUILD_SHARED_LIBS     : ON
--   CAFFE2_USE_MSVC_STATIC_RUNTIME     : OFF
--   BUILD_TEST            : OFF
--   BUILD_JNI             : OFF
--   BUILD_MOBILE_AUTOGRAD : OFF
--   BUILD_LITE_INTERPRETER: OFF
--   INTERN_BUILD_MOBILE   :
--   TRACING_BASED         : OFF
--   USE_BLAS              : 1
--     BLAS                : mkl
--     BLAS_HAS_SBGEMM     :
--   USE_LAPACK            : 1
--     LAPACK              : mkl
--   USE_ASAN              : OFF
--   USE_TSAN              : OFF
--   USE_CPP_CODE_COVERAGE : OFF
--   USE_CUDA              : ON
--     Split CUDA          :
--     CUDA static link    : OFF
--     USE_CUDNN           : ON
--     USE_CUSPARSELT      : ON
--     USE_CUDSS           : ON
--     USE_CUFILE          : OFF
--     CUDA version        : 12.6
--     USE_FLASH_ATTENTION : OFF
--     USE_MEM_EFF_ATTENTION : ON
--     cuDNN version       : 9.6.0
--     cuSPARSELt version  : 0.6.3
--     CUDA root directory : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6
--     CUDA library        : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/lib/x64/cuda.lib
--     cudart library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/lib/x64/cudart.lib
--     cublas library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/lib/x64/cublas.lib
--     cufft library       : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/lib/x64/cufft.lib
--     curand library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/lib/x64/curand.lib
--     cusparse library    : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/lib/x64/cusparse.lib
--     cuDNN library       : C:/Program Files/NVIDIA/CUDNN/v9.6/lib/12.6/x64/cudnn.lib
--     cuSPARSELt library  : C:/Program Files/NVIDIA cuSPARSELt/v0.6/lib/cusparseLt.lib
--     cuDSS library       : C:/Program Files/NVIDIA cuDSS/v0.4/lib/12/cudss.lib
--     nvrtc               : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/lib/x64/nvrtc.lib
--     CUDA include path   : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/include
--     NVCC executable     : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/bin/nvcc.exe
--     CUDA compiler       : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/bin/nvcc.exe
--     CUDA flags          :  -DLIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS -Xcompiler  /Zc:__cplusplus -Xcompiler /w -w -Xcompiler /FS -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch --use-local-env -gencode arch=compute_89,code=sm_89 -gencode arch=compute_86,code=sm_86 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --Werror cross-execution-space-call --no-host-device-move-forward --expt-relaxed-constexpr --expt-extended-lambda  -Xcompiler=/wd4819,/wd4503,/wd4190,/wd4244,/wd4251,/wd4275,/wd4522 -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__
--     CUDA host compiler  :
--     CUDA --device-c     : OFF
--     USE_TENSORRT        :
--   USE_XPU               : OFF
--   USE_ROCM              : OFF
--   BUILD_NVFUSER         :
--   USE_EIGEN_FOR_BLAS    :
--   USE_FBGEMM            : ON
--     USE_FAKELOWP          : OFF
--   USE_KINETO            : ON
--   USE_GFLAGS            : OFF
--   USE_GLOG              : OFF
--   USE_LITE_PROTO        : OFF
--   USE_PYTORCH_METAL     : OFF
--   USE_PYTORCH_METAL_EXPORT     : OFF
--   USE_MPS               : OFF
--   CAN_COMPILE_METAL     :
--   USE_MKL               : ON
--     USE_STATIC_MKL      : OFF
--   USE_MKLDNN            : ON
--   USE_MKLDNN_ACL        : OFF
--   USE_MKLDNN_CBLAS      : OFF
--   USE_UCC               : OFF
--   USE_ITT               : ON
--   USE_NCCL              : OFF
--   USE_NNPACK            : OFF
--   USE_NUMPY             : ON
--   USE_OBSERVERS         : ON
--   USE_OPENCL            : OFF
--   USE_OPENMP            : ON
--   USE_MIMALLOC          : ON
--     USE_MIMALLOC_ON_MKL   : OFF
--   USE_VULKAN            : OFF
--   USE_PROF              : OFF
--   USE_PYTORCH_QNNPACK   : OFF
--   USE_XNNPACK           : ON
--   USE_DISTRIBUTED       : ON
--     USE_MPI               : OFF
--     USE_GLOO              : ON
--     USE_GLOO_WITH_OPENSSL : OFF
--     USE_TENSORPIPE        : OFF
--   Public Dependencies  : caffe2::mkl
--   Private Dependencies : Threads::Threads;pthreadpool;cpuinfo;XNNPACK;microkernels-prod;fbgemm;ittnotify;fp16;caffe2::openmp;gloo;fmt::fmt-header-only;kineto
--   Public CUDA Deps.    :
--   Private CUDA Deps.   : caffe2::curand;caffe2::cufft;caffe2::cublas;torch::cudnn;torch::cusparselt;gloo_cuda;fmt::fmt-header-only;C:/Program Files (x86)/Intel/oneAPI/mkl/2025.0/lib/mkl_lapack95_lp64.lib;C:/Program Files (x86)/Intel/oneAPI/mkl/2025.0/lib/mkl_intel_lp64_dll.lib;C:/Program Files (x86)/Intel/oneAPI/mkl/2025.0/lib/mkl_intel_thread_dll.lib;C:/Program Files (x86)/Intel/oneAPI/mkl/2025.0/lib/mkl_core_dll.lib;C:/Users/User/anaconda3/envs/py310/Library/lib/libiomp5md.lib;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/lib/x64/cudart_static.lib;CUDA::cusparse;CUDA::cufft;CUDA::cusolver;torch::magma;ATEN_CUDA_FILES_GEN_LIB
--   USE_COREML_DELEGATE     : OFF
--   BUILD_LAZY_TS_BACKEND   : ON
--   USE_ROCM_KERNEL_ASSERT : OFF
-- Performing Test HAS_WMISSING_PROTOTYPES
-- Performing Test HAS_WMISSING_PROTOTYPES - Failed
-- Performing Test HAS_WERROR_MISSING_PROTOTYPES
-- Performing Test HAS_WERROR_MISSING_PROTOTYPES - Failed
-- Configuring done (71.9s)
-- Generating done (12.1s)
CMake Warning:
  Manually-specified variables were not used by the project:

    USE_STATIC_DISPATCH


-- Build files have been written to: C:/Users/User/Desktop/pytorch_compile/pytorch/build
```

### Versions

Not applicable (can't import torch)","closed","Refatoração","{}",0,"2025-01-12 04:16:30","2025-01-12 05:58:59","2025-01-12 05:58:59","Panchovix",NULL,NULL,"https://github.com/pytorch/pytorch/issues/144636",0
"38","2782208242","2782208242","144632","[MPSInductor] Properly generate index expressions","Now test_slice_scatter4_mps passes

Before this change test_torchinductor.py reported 422 failed and 337 passed, after this change 412 failed 347 passed.

Fixes https://github.com/pytorch/pytorch/issues/144630


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,""topic: not user facing"",ciflow/mps,""module: inductor"",ciflow/inductor}",3,"2025-01-12 00:31:50","2025-01-12 06:11:10","2025-01-12 06:10:08","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144632",0
"39","2646929286","2646929286","140224","[mobile] Fix Conv2d string padding handling in XNNPACK optimization","Prevents rewriting Conv2d operations that use string-based padding modes ('same') to prepacked ops. This fixes internal assertion failures in alias analysis when handling string padding values.

Changes:
- Added padding mode filter to Conv2d rewrite patterns and applied it to regular/transpose operations
- Added test coverage for Conv2d with 'same' padding mode

Fixes #65490



cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel","closed","Refatoração","{""oncall: jit"",triaged,""open source"",""release notes: jit""}",1,"2024-11-10 07:23:19","2024-11-16 07:11:32","2024-11-16 07:11:32","AkiSakurai",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140224",6
"40","2646883650","2646883650","140221","[mobile] Fix Conv1d optimization with 'same' padding mode","Adds a filter to prevent rewriting Conv1d operations that use string-based padding modes ('same') to Conv2d. This resolves a runtime error where string padding values were incorrectly handled as lists during mobile optimization.

Added test coverage for Conv1d with 'same' padding to verify the fix.

Fixes #104606



cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel","closed","Refatoração","{""oncall: jit"",triaged,""open source"",ciflow/trunk,""release notes: jit""}",2,"2024-11-10 06:19:05","2024-11-16 07:19:56","2024-11-16 07:10:36","AkiSakurai",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140221",6
"41","2646266465","2646266465","140212","Remove legacy code from `python/builder`","Remove magma legacy code

cc @seemethere @malfet @pytorch/pytorch-dev-infra","closed","Refatoração","{""module: ci"",triaged}",1,"2024-11-09 16:12:33","2024-11-11 23:31:10","2024-11-11 23:31:10","afrittoli",NULL,NULL,"https://github.com/pytorch/pytorch/issues/140212",2
"42","2646094177","2646094177","140210","[torchgen] Improve schema parsing with regex for numeric ranges","Replaces the hardcoded string replacement for numeric ranges with a more robust regex pattern that handles any combination of positive and negative numbers in default value ranges.
Fixes #135470","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""release notes: export""}",7,"2024-11-09 13:57:02","2024-11-14 23:29:32","2024-11-14 23:28:30","AkiSakurai",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140210",5
"261","2653830243","2653830243","140489","Add support for parsing torch.Generator in JIT","Fixes #140420


cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel","closed","Refatoração","{""oncall: jit"",triaged,""open source"",Merged,ciflow/trunk,""release notes: jit""}",3,"2024-11-13 01:26:59","2024-11-13 23:32:15","2024-11-13 23:07:01","antoniojkim",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140489",0
"44","2634005985","2634005985","139669","[AOTAutograd] Handle edge cases for donated buffer & enable in oss","This PR enables donated buffer in OSS and handles two edge cases:

1. While donated buffer relies on storage to check alias, sparse tensor subclasses does not provide access to storage. So we skip sparse tensor subclasses for donated buffer.
2. Handles missing ""val"" from n.meta. This is observed from `inductor/test_fused_attention.py::SDPAPatternRewriterCpuTests::test_sdpa_rewriter_11_cpu`, 
`functorch/test_aotdispatch.py::TestAOTAutograd::test_input_mutation_simple_with_none_and_nontensor`, and 
`inductor/test_compiled_autograd.py::TestCompiledAutograd::test_trace_run_with_rng_state`.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor,ciflow/rocm}",4,"2024-11-04 22:40:02","2024-11-05 21:47:27","2024-11-05 18:38:23","BoyuanFeng","BoyuanFeng",NULL,"https://github.com/pytorch/pytorch/pull/139669",1
"45","2633979553","2633979553","139665","FlexAttention Benchmark","1. Add alibi, sliding window, tahn softcap, prefixLM, and document_mask from attn_gym to benchmark. 

2. Add comparison to different SDPA backends & FAv2, FAv3, FAKV.

Dependent on https://github.com/pytorch/pytorch/pull/139639

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor""}",7,"2024-11-04 22:26:02","2024-12-23 22:33:20","2024-12-20 17:52:26","joydddd",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139665",46
"46","2604094841","2604094841","138544","[AOTI][refactor] Clean up test_aot_inductor skip list","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138544

Summary: Remove skips for already fixed tests. Change remaining skip to xfail so that the failure list can be more proactively maintained.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @chauhang @aakhundov

Differential Revision: [D64761257](https://our.internmc.facebook.com/intern/diff/D64761257)","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor""}",7,"2024-10-22 02:58:34","2024-11-22 02:09:53","2024-10-22 21:32:52","desertfire",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138544",0
"47","2604075971","2604075971","138541","[AOTI][reland] Fix test_index_put_with_none_index_cpu_with_stack_allocation","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #138544
* __->__ #138541

Summary: The problem happened after splitting CppWrapperCpu and CppWrapperCpuArrayRef, because CppWrapperCpuArrayRef.generate_index_put_fallback missed a statement.

Running test_aot_inductor.py as a whole didn't reveal the problem, but running test_index_put_with_none_index_cpu_with_stack_allocation individually did. Digging deeper, the root cause is init_backend_registration has incorrectly cached CPU CppWrapperCodegen class, which means CppWrapperCpuArrayRef was never picked when running test_aot_inductor.py as a whole. To fix the problem, all the ArrayRef tests are split into a separate file. Also a code checking is added to regex match AOTInductorModelRunMinimalArrayrefInterface so this kind of false passing signal won't be unnoticed.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @chauhang @aakhundov

Differential Revision: [D64734106](https://our.internmc.facebook.com/intern/diff/D64734106)","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor,""release notes: inductor""}",5,"2024-10-22 02:43:45","2024-11-23 02:06:25","2024-10-22 14:17:30","desertfire",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138541",0
"48","2604057893","2604057893","138540","[AOTI] add C shim for QConvPointWise","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #139054
* __->__ #138540
* #138806
* #138691

This PR adds C shim for `QConvPointWisePT2E` and `QConvPointWiseBinaryPT2E` similar to https://github.com/pytorch/pytorch/pull/138439. Besides that, we aligned the implementation of `qconv_pointwise` with `qlinear_pointwise` in the following aspects:
1. The parameter order of `qconv_pointwise` and `qlinear_pointwise` are quite different, we aligned the schema of `qconv_pointwise` to have similar parameter order as `qlinear_pointwise` to make it more consistent.
2. We always converted `x_scale` and `x_zero_point` to Tensors, just like in the lowering of `qlinear_pointwise`. This avoids the need to create two separate C APIs (one for `double x_scale` and `int64_t x_zero_point`, and another for `Tensor` versions). Instead, we only need one API for `Tensor`-based `x_scale` and `x_zero_point`. If we later add dynamic quantization for qconv (which will use `Tensor` for `x_scale` and `x_zero_point`), we can reuse the code from this PR and don't need to change the C shim layer API.




cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""module: cpu"",""open source"",Merged,ciflow/trunk,""release notes: quantization"",""module: inductor"",ciflow/inductor}",3,"2024-10-22 02:27:08","2024-11-30 02:08:06","2024-10-31 02:03:04","chunyuan-w",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138540",9
"49","2796232991","2796232991","145109","Skip test responsible for causing flakiness","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145110
* __->__ #145109

Investigation is a separate issue. For now I want to get the CI back up
and running on the other tests. The problem seems to be that
IncludeDispatchKeyGuard doesn't actually reset the state, which seems
very, very wrong.","closed","Refatoração","{Merged,""topic: not user facing""}",1,"2025-01-17 20:21:27","2025-01-18 00:58:28","2025-01-18 00:58:27","zou3519",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145109",1
"58","2653384295","2653384295","140454","Add safe.directory to Almalinux docker image","Something that was accidentally dropped by: https://github.com/pytorch/pytorch/pull/140157
Needs to be re-added. I believe its part of our Docker images. Please see: https://github.com/pytorch/pytorch/blob/main/.ci/docker/manywheel/Dockerfile#L21","closed","Refatoração","{Merged,""topic: not user facing""}",3,"2024-11-12 21:30:36","2024-11-13 03:10:13","2024-11-12 23:28:15","atalman",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140454",0
"59","2653354733","2653354733","140452","Inductor accuracy failures for Weight Norm","### 🐛 Describe the bug

When compiling a weight-normed linear layer, some shapes are causing accuracy failures.
The backends `eager` and `aot_eager` seem to work, but `inductor` causes issues.
For example, the code below passes with `in_features=1024`, but fails with `in_features=1025`.
This leads to severe regressions when training models that use Weight Norm.

This fails on multiple Pytorch versions for Python 3.12 (tested: 2.4.1, 2.5, 2.6 current nightly), but works on 2.3 for Python 3.8.

```
import sys
from functools import partial
from math import inf
import torch
from torch import tensor, device
import torch.fx as fx
import torch._dynamo
from torch._dynamo.testing import rand_strided
from torch._dynamo.debug_utils import run_fwd_maybe_bwd

import torch._dynamo.config
import torch._inductor.config
import torch._functorch.config
import torch.fx.experimental._config
torch._dynamo.config.optimize_ddp = False

from typing import Dict, Optional
import torch
from torch.nn import *


from torch.nn import *
class Repro(torch.nn.Module):
    def __init__(self, in_features):
        super().__init__()
        self.weight_normed_linear = torch.nn.utils.parametrizations.weight_norm(torch.nn.Linear(in_features=in_features, out_features=2)).cuda()
        self.linear = torch.nn.Linear(in_features=2, out_features=1).cuda()

    def forward(self, x_0):
        x_1 = self.weight_normed_linear(x_0)
        x_2 = self.linear(x_1)
        return (x_2,)

def load_args(in_features, reader):  
    buf0 = reader.storage('fbae9e314f27f66ab2f21026f411a176d6711e51', 9043968, device=device(type='cuda', index=0), dtype_hint=torch.float16)
    reader.tensor(buf0, (2, in_features), dtype=torch.float16, requires_grad=True) 

if __name__ == '__main__':

    for in_features in [1024, 1025]:
      torch.compiler.reset()

      mod = Repro(in_features)
      load_args_partial = partial(load_args, in_features)
      load_args_partial.version = 0

      from torch._dynamo.repro.after_dynamo import run_repro
      run_repro(mod,load_args_partial, accuracy=True, command='run',
              save_dir='/stuff/felixb/kernel_workspace/checkpoints', autocast=True, backend='inductor')
```


### Versions

This passes on 2.3.1 (with Python 3.8), but starts breaking in 2.4 (with Python 3.12) and doesn't seem to be fixed on the current nightly.

cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @aakhundov @BoyuanFeng","closed","Refatoração","{""high priority"",triaged,""oncall: pt2"",""module: inductor""}",5,"2024-11-12 21:22:31","2025-01-14 19:27:40","2025-01-14 19:27:39","bonpyt","shunting314",NULL,"https://github.com/pytorch/pytorch/issues/140452",63
"278","2680295340","2680295340","141254","Back out ""[export] serialize sympy.Exprs as ASTs instead of strings""","Clone of https://github.com/pytorch/pytorch/pull/141253","closed","Refatoração","{Merged,ciflow/trunk,ciflow/inductor,""release notes: export""}",4,"2024-11-21 17:32:54","2024-12-22 02:11:13","2024-11-21 22:10:00","pianpwk",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141254",0
"61","2653315684","2653315684","140450","[DeviceMesh] Add DeviceMesh.manual_seed API","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #140450



follows RFC #140301
Also, following a suggestion from @yifuwang, starts off by implementing the API just for DeviceMesh.  This is easier to validate (ensure that dims are 'orthogonal') and we encourage adoption of DeviceMesh anyway for new use cases and especially multi-dimensional parallelism use cases.  We can always follow up with an API in torch.distributed namespace that works for ProcessGroup if we want.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @d4l3k @c-p-i-o @tianyu-l @XilunWu","closed","Refatoração","{""oncall: distributed"",""topic: not user facing"",""module: dtensor""}",3,"2024-11-12 20:58:14","2024-12-28 02:03:07","2024-11-27 23:38:23","wconstab",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140450",15
"62","2788948547","2788948547","144839","Apply Ruff fixes and pyupgrade to torch/fx","Fixes #ISSUE_NUMBER


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{triaged,""open source"",""release notes: fx"",fx,ciflow/inductor,suppress-bc-linter}",1,"2025-01-15 06:28:10","2025-01-21 02:15:06","2025-01-21 02:14:55","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144839",6
"50","2796194618","2796194618","145101","PEP585 update - benchmarks tools torchgen","This is one of a series of PRs to update us to PEP585 (changing Dict -> dict, List -> list, etc).  Most of the PRs were completely automated with RUFF as follows:

Since RUFF UP006 is considered an ""unsafe"" fix first we need to enable unsafe fixes:

```
--- a/tools/linter/adapters/ruff_linter.py
+++ b/tools/linter/adapters/ruff_linter.py
@@ -313,6 +313,7 @@
                     ""ruff"",
                     ""check"",
                     ""--fix-only"",
+                    ""--unsafe-fixes"",
                     ""--exit-zero"",
                     *([f""--config={config}""] if config else []),
                     ""--stdin-filename"",
```

Then we need to tell RUFF to allow UP006 (as a final PR once all of these have landed this will be made permanent):

```
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@


 [tool.ruff]
-target-version = ""py38""
+target-version = ""py39""
 line-length = 88
 src = [""caffe2"", ""torch"", ""torchgen"", ""functorch"", ""test""]

@@ -87,7 +87,6 @@
     ""SIM116"", # Disable Use a dictionary instead of consecutive `if` statements
     ""SIM117"",
     ""SIM118"",
-    ""UP006"", # keep-runtime-typing
     ""UP007"", # keep-runtime-typing
 ]
 select = [
```

Finally running `lintrunner -a --take RUFF` will fix up the deprecated uses.



Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145101



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{Merged,ciflow/trunk,""release notes: releng"",""topic: not user facing"",""module: dynamo"",ciflow/inductor}",3,"2025-01-17 19:53:05","2025-01-18 05:06:13","2025-01-18 05:05:10","aorenste",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145101",1
"51","2796160432","2796160432","145099","Make MultiProcContinuousTest timeout configurable","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145125
* #144834
* __->__ #145099
* #145011
* #145010

Allows test classes using MPCT to set their own timeout as a class
property, which is good enough since the processgroup is shared across
test instances and the timeout is set at processgroup init.

Also sets a default timeout of 2 minutes, which is probably (?) long
enough for reasonable tests, but can be changed if it causes flakyness.
It's preferable to have as short default timeout as possible, since when
debugging tests getting a timeout quickly helps.","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",5,"2025-01-17 19:28:45","2025-01-18 04:38:20","2025-01-18 04:37:15","wconstab",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145099",1
"52","2796136375","2796136375","145097","[ca] Use aot_eager on flex attention test","FIXES https://github.com/pytorch/pytorch/issues/144912

The flex attention lowering incompatibilities are covered by https://github.com/pytorch/pytorch/blob/main/test/inductor/test_flex_attention.py. For the CA + flex integration, we don't actually need to test the lowering, only the frontend graph capture.

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145097



cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""module: rocm"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor,ciflow/rocm}",4,"2025-01-17 19:11:52","2025-01-18 02:48:19","2025-01-18 02:47:16","xmfan",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145097",1
"53","2595229803","2595229803","138228","[DO NOT REIVEW] test CI infrastructure","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138228","closed","Refatoração","{""open source"",ciflow/trunk,""module: inductor"",ciflow/inductor}",2,"2024-10-17 16:38:52","2024-11-21 02:07:51","2024-10-21 05:31:12","etaf",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138228",4
"54","2595036895","2595036895","138223","unravel_index check if all elements in `indices` are within the acceptable range","Fixes #136736 


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @mcarilli @ptrblck @leslie-fang-intel","closed","Refatoração","{""module: cpu"",triaged,""open source"",""module: amp (automated mixed precision)"",""release notes: quantization"",""release notes: releng""}",6,"2024-10-17 15:12:56","2024-12-13 15:57:10","2024-12-13 15:57:10","Rahul-Ram-03",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138223",57
"55","2661649150","2661649150","140799","[ARM] Expand linux aarch64 unit test list","Expand the list of unit tests for test_linux_aarch64

These have been verified externally as passing on neoverse n1 and v1 based machines.

@malfet 

cc @malfet @snadampal @milpuz01","closed","Refatoração","{triaged,""open source"",""module: arm"",Merged,ciflow/trunk,""topic: not user facing""}",16,"2024-11-15 11:05:55","2024-11-27 21:21:46","2024-11-27 18:43:59","robert-hardwick",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140799",12
"56","2661528692","2661528692","140798","No Python3.8 build for Pytorch 2.5.x","### 🐛 Describe the bug

`pip3.8 install torch==2.5.0 --index-url https://download.pytorch.org/whl/cu118` returns failure.

### Versions

I am using Ubuntu 20.04 whose python3 environment is 3.8
Hope to add 3.8 support for Pytorch 2.5.x

cc @seemethere @malfet @osalpekar @atalman @svekars @brycebortree @sekyondaMeta @AlannaBurke","closed","Refatoração","{""module: binaries"",""module: docs"",""oncall: releng"",triaged}",7,"2024-11-15 10:23:43","2024-11-27 18:34:52","2024-11-27 00:26:00","ghostplant",NULL,NULL,"https://github.com/pytorch/pytorch/issues/140798",12
"57","2660839942","2660839942","140786","[Inductor][CPP] Enable horizontal Transverse","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #140786
* #140921
* #137975

**Summary**
Current, CPP GEMM Template using vertical transverse strategy to do the cache blocking and loop transverse, which assumes Matrix B in L1 cache and Matrix A in L2 cache. Nevertheless, we found when Matrix A is much larger than Matrix B, horizontal transverse can give better performance. In this PR:

- We implement the horizontal transverse strategy which is default off and can be turn on by inductor config: `config.cpp.cpp_gemm_transverse_strategy = ""HORIZONTAL""`
- We also implement the heuristic to choose between vertical and horizontal transverse when user set this config as: `config.cpp.cpp_gemm_transverse_strategy = ""VERTICAL,HORIZONTAL""` 

**Test Plan**
```
python -u -m pytest -s -v test/inductor/test_cpu_select_algorithm.py -k test_horizontal_transverse
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""open source"",ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",1,"2024-11-15 05:46:05","2025-01-14 05:52:59","2025-01-14 05:52:59","leslie-fang-intel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140786",60
"228","2612517449","2612517449","138846","[c10d][CI] Improve world size setting in some tests","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138846

Following change in #137161 , bumping world size for some test suites.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,Reverted,ciflow/trunk,""topic: not user facing""}",14,"2024-10-24 20:38:31","2024-11-26 02:09:15","2024-10-25 23:02:20","kwen2501",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138846",1
"63","2788814828","2788814828","144832","[Cutlass] Seeing if changing default copies fixes perf","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #144832



cc @ptrblck @msaroufim @eqy","closed","Refatoração","{""module: cuda"",ciflow/trunk,""topic: not user facing"",""module: sdpa""}",4,"2025-01-15 04:33:37","2025-01-15 17:43:31","2025-01-15 17:43:31","drisspg",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144832",0
"64","2786241683","2786241683","144753","[DCP] Fix fsspec fsync bug on .finish()","Fixes #144752 

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0","closed","Refatoração","{""oncall: distributed"",""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: distributed_checkpoint""}",7,"2025-01-14 04:49:21","2025-01-19 03:22:06","2025-01-19 03:21:02","cassanof",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144753",5
"65","2786226212","2786226212","144751","[dynamo] Add `--profile-details` and `--export-perfdoctor` option","Summary:
Add `--profile-details` option to add shapes and other details to the Kineto profile.

Add `--export-perfdoctor` to directly dump trace to perfdoctor for webview.

Test Plan:
```
$ buck2 run mode/opt //caffe2/benchmarks/dynamo:torchbench_internal -- --only mrs_video_watch_over --performance --training --amp --export-profiler-trace --backend=inductor --profile-details --export-perfdoctor
```

https://interncache-all.fbcdn.net/manifold/perfetto-artifacts/tree/ui/index.html#!/?url=https://interncache-all.fbcdn.net/manifold/pyper_traces/tree/traces/test/inductor_mrs_video_watch_over_rank_0_20250113_173817_6535183793.json.gz

Differential Revision: D68134547




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor}",43,"2025-01-14 04:40:44","2025-01-23 19:10:45","2025-01-23 19:09:43","xuzhao9",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144751",9
"66","2786174062","2786174062","144747","add fp8 support to index_cuda","Fixes #133605

**Summary**

This PR adds support for FP8 data types to the `index_cuda` op. 

It uses `AT_DISPATCH_V2` which is a new macro that can handle arbitrary number of dtypes, as opposed to the old implementations which had a separate macro for each possible number of dtype arguments (e.g. `AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND{2,3,4,5...}`).

**Test plan**

Updated test `index_cuda_with_cpu` in `test/test_fake_tensor.py` to have cases for all dtypes handled by `index_cuda`, including fp8 dtypes.","closed","Refatoração","{Merged,ciflow/trunk,""release notes: quantization""}",6,"2025-01-14 03:51:25","2025-01-17 22:54:29","2025-01-17 22:53:26","danielvegamyhre",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144747",3
"67","2786167538","2786167538","144746","expose extra torch_python apis","Fixes #144302
After checking the code of my third-party devices, I think these APIs are also relied on by us, so I exposed them according to the discussion in the issue.","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing""}",6,"2025-01-14 03:47:53","2025-01-16 20:51:37","2025-01-16 20:50:35","garfield1997",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144746",2
"68","2731327488","2731327488","142789","DISABLED test_op_has_batch_rule_polar_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_op_has_batch_rule_polar_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_op_has_batch_rule_polar_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_polar_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 26015232 on device 0. CUDA driver allocated memory was 184811520 and is now 223608832.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_polar_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 22:21:57","2024-12-19 20:12:03","2024-12-19 20:12:03","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142789",9
"229","2612500298","2612500298","138845","test only","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #136732
* __->__ #138845



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",""topic: not user facing""}",1,"2024-10-24 20:28:20","2024-11-24 02:13:40","2024-10-24 20:29:20","yifuwang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138845",0
"69","2731325935","2731325935","142780","DISABLED test_op_has_batch_rule_fft_fft2_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_op_has_batch_rule_fft_fft2_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_op_has_batch_rule_fft_fft2_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_fft_fft2_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 6656 on device 0. CUDA driver allocated memory was 163315712 and is now 173277184.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_fft_fft2_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 22:21:12","2024-12-19 20:11:59","2024-12-19 20:11:59","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142780",9
"204","2662783006","2662783006","140839","Upload sccache stats into benchmark database with build step time","Guinea pig benchmark database","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",test-config/default}",5,"2024-11-15 18:27:20","2024-12-22 02:10:39","2024-11-21 22:38:48","clee2000",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140839",6
"71","2743131846","2743131846","143307","easy: sort dictionary keys for inductor config when publishing","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #143317
* __->__ #143307

This means we should get consistent logging strings for the same
config on different ranks

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Teste de Regressão","{Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor}",14,"2024-12-16 18:47:55","2025-01-09 18:02:29","2025-01-09 18:01:23","c00w",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143307",24
"72","2742985940","2742985940","143301","Fix a misspelling [ONNX]",NULL,"closed","Refatoração","{""module: onnx"",""open source"",Merged,ciflow/trunk,""topic: not user facing""}",7,"2024-12-16 17:31:47","2024-12-16 20:20:46","2024-12-16 20:19:44","xadupre",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143301",0
"73","2659510276","2659510276","140724","[ROCm] remove size restrictions in gemm_and_bias","This aligns hipblaslt behavior with CUDA_VERSION >= 12010.

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",""open source"",Merged,""topic: not user facing"",rocm,""rocm priority"",ciflow/rocm}",4,"2024-11-14 17:18:00","2024-11-15 02:24:33","2024-11-15 02:23:29","jeffdaily",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140724",1
"74","2745745033","2745745033","143406","torch.compile - Simplify error when triton package is  not found","### 🐛 Describe the bug

Initial title:  titon: python 3.13 torch.compile smoke test is failing in CI/CD

I observe following failure running smoke_test for python 3.13, cuda 11.8, cuda 12.4, cuda 12.6: https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/smoke_test/smoke_test.py


Detected during validation testing. Please see: https://github.com/pytorch/test-infra/issues/6077#issue-2745142173

Worklfow: https://github.com/pytorch/pytorch/actions/runs/12378525708/job/34555282656?pr=143397#step:15:1434

Full log:
```
++ python3 ./smoke_test/smoke_test.py --package torchonly
torch: 2.6.0+cu118
ATen/Parallel:
	at::get_num_threads() : 8
	at::get_num_interop_threads() : 16
OpenMP 201511 (a.k.a. OpenMP 4.5)
	omp_get_max_threads() : 8
Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
	mkl_get_max_threads() : 8
Intel(R) MKL-DNN v3.5.3 (Git Hash 66f0cb9eb66affd2da3bf5f8d897376f04aae6af)
std::thread::hardware_concurrency() : 16
Environment variables:
	OMP_NUM_THREADS : [not set]
	MKL_NUM_THREADS : [not set]
ATen parallel backend: OpenMP

Testing smoke_test_conv2d
Testing smoke_test_conv2d with cuda
/pytorch/pytorch/.ci/pytorch/./smoke_test/smoke_test.py:232: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Testing smoke_test_conv2d with cuda for torch.float16
Testing smoke_test_conv2d with cuda for torch.float32
Testing smoke_test_conv2d with cuda for torch.float64
Testing smoke_test_linalg on cpu
Testing smoke_test_linalg on cuda
Testing smoke_test_linalg with cuda for torch.float32
Testing smoke_test_linalg with cuda for torch.float64
Testing smoke_test_compile for cuda and torch.float16
Traceback (most recent call last):
  File ""/pytorch/pytorch/.ci/pytorch/./smoke_test/smoke_test.py"", line 385, in <module>
    main()
    ~~~~^^
  File ""/pytorch/pytorch/.ci/pytorch/./smoke_test/smoke_test.py"", line 379, in main
    smoke_test_cuda(
    ~~~~~~~~~~~~~~~^
        options.package, options.runtime_error_check, options.torch_compile_check
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""/pytorch/pytorch/.ci/pytorch/./smoke_test/smoke_test.py"", line 186, in smoke_test_cuda
    smoke_test_compile(""cuda"" if torch.cuda.is_available() else ""cpu"")
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/pytorch/pytorch/.ci/pytorch/./smoke_test/smoke_test.py"", line 286, in smoke_test_compile
    x_pt2 = torch.compile(foo)(x)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py"", line 573, in _fn
    return fn(*args, **kwargs)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py"", line 1380, in __call__
    return self._torchdynamo_orig_callable(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        frame, cache_entry, self.hooks, frame_state, skip=1
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py"", line 1164, in __call__
    result = self._inner_convert(
        frame, cache_entry, hooks, frame_state, skip=skip + 1
    )
  File ""/opt/conda/envs/conda-env-12[3750](https://github.com/pytorch/test-infra/actions/runs/12375056109/job/34539025723#step:14:3751)56109/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py"", line 547, in __call__
    return _compile(
        frame.f_code,
    ...<14 lines>...
        skip=skip + 1,
    )
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py"", line 986, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py"", line 715, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_utils_internal.py"", line 95, in wrapper_function
    return function(*args, **kwargs)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py"", line 750, in _compile_inner
    out_code = transform_code_object(code, transform)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/bytecode_transformation.py"", line 1361, in transform_code_object
    transformations(instructions, code_options)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py"", line 231, in _fn
    return fn(*args, **kwargs)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py"", line 662, in transform
    tracer.run()
    ~~~~~~~~~~^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py"", line 2868, in run
    super().run()
    ~~~~~~~~~~~^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py"", line 1052, in run
    while self.step():
          ~~~~~~~~~^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py"", line 962, in step
    self.dispatch_table[inst.opcode](self, inst)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py"", line 3048, in RETURN_VALUE
    self._return(inst)
    ~~~~~~~~~~~~^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py"", line 3033, in _return
    self.output.compile_subgraph(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        self,
        ^^^^^
    ...<2 lines>...
        ),
        ^^
    )
    ^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/output_graph.py"", line 1101, in compile_subgraph
    self.compile_and_call_fx_graph(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        tx, list(reversed(stack_values)), root, output_replacements
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/output_graph.py"", line 1382, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/output_graph.py"", line 1432, in call_user_compiler
    return self._call_user_compiler(gm)
           ~~~~~~~~~~~~~~~~~~~~~~~~^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/output_graph.py"", line 1483, in _call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
        e.__traceback__
    ) from None
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/output_graph.py"", line 1462, in _call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/repro/after_dynamo.py"", line 130, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/__init__.py"", line 2314, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/compile_fx.py"", line 1863, in compile_fx
    return aot_autograd(
           ~~~~~~~~~~~~~
    ...<6 lines>...
        cudagraphs=cudagraphs,
        ~~~~~~~~~~~~~~~~~~~~~~
    )(model_, example_inputs_)
    ~^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/backends/common.py"", line 83, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py"", line 1155, in aot_module_simplified
    compiled_fn = dispatch_and_compile()
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py"", line 1131, in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        functional_call,
        ^^^^^^^^^^^^^^^^
    ...<3 lines>...
        shape_env,
        ^^^^^^^^^^
    )
    ^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py"", line 580, in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
        flat_fn, fake_flat_args, aot_config, fake_mode, shape_env
    )
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py"", line 830, in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
                               ~~~~~~~~~~~^
        flat_fn,
        ^^^^^^^^
    ...<2 lines>...
        fw_metadata=fw_metadata,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py"", line 203, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py"", line 489, in __call__
    return self.compiler_fn(gm, example_inputs)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/compile_fx.py"", line 1741, in fw_compiler_base
    return inner_compile(
        gm,
    ...<5 lines>...
        boxed_forward_device_index=forward_device,
    )
Traceback (most recent call last):
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/compile_fx.py"", line 569, in compile_fx_inner
  File ""/home/ec2-user/actions-runner/_work/test-infra/test-infra/test-infra/.github/scripts/run_with_env_secrets.py"", line 102, in <module>
    main()
  File ""/home/ec2-user/actions-runner/_work/test-infra/test-infra/test-infra/.github/scripts/run_with_env_secrets.py"", line 98, in main
    run_cmd_or_die(f""docker exec -t {container_name} /exec"")
  File ""/home/ec2-user/actions-runner/_work/test-infra/test-infra/test-infra/.github/scripts/run_with_env_secrets.py"", line 39, in run_cmd_or_die
    raise RuntimeError(f""Command {cmd} failed with exit code {exit_code}"")
RuntimeError: Command docker exec -t 9918c06ce07754c73f8d05922651f9952bdb247939406ec9e910b63df29bad02 /exec failed with exit code 1
    return wrap_compiler_debug(_compile_fx_inner, compiler_name=""inductor"")(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        gm,
        ^^^
        example_inputs,
        ^^^^^^^^^^^^^^^
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py"", line 102, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/compile_fx.py"", line 685, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
        gm, example_inputs, inputs_to_check, **graph_kwargs
    )
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/compile_fx.py"", line 1129, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/compile_fx.py"", line 1044, in codegen_and_compile
    compiled_fn = graph.compile_to_module().call
                  ~~~~~~~~~~~~~~~~~~~~~~~^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/graph.py"", line 2027, in compile_to_module
    return self._compile_to_module()
           ~~~~~~~~~~~~~~~~~~~~~~~^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/graph.py"", line 2033, in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ~~~~~~~~~~~~^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/graph.py"", line 1964, in codegen
    self.scheduler = Scheduler(self.operations)
                     ~~~~~~~~~^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/scheduler.py"", line 1798, in __init__
    self._init(nodes)
    ~~~~~~~~~~^^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/scheduler.py"", line 1816, in _init
    self.nodes = [self.create_scheduler_node(n) for n in nodes]
                  ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/scheduler.py"", line 1947, in create_scheduler_node
    return SchedulerNode(self, node)
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/scheduler.py"", line 893, in __init__
    self._compute_attrs()
    ~~~~~~~~~~~~~~~~~~~^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/scheduler.py"", line 907, in _compute_attrs
    group_fn = self.scheduler.get_backend(device).group_fn
               ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/scheduler.py"", line 3441, in get_backend
    self.backends[device] = self.create_backend(device)
                            ~~~~~~~~~~~~~~~~~~~^^^^^^^^
  File ""/opt/conda/envs/conda-env-12375056109/lib/python3.13/site-packages/torch/_inductor/scheduler.py"", line 3432, in create_backend
    raise RuntimeError(
        ""Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton""  # noqa: B950
    )
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

++ handle_error
Please note: We are currently migrating Linux Wheel builds to Manywheel 2.28
++ echo 'Please note: We are currently migrating Linux Wheel builds to Manywheel 2.28'
++ echo 'If you see error like: ImportError: /lib64/libc.so.6: version GLIBC_2.28 not found'
++ echo 'Please migrate to: https://github.com/pytorch/test-infra/blob/main/.github/workflows/linux_job_v2.yml'
++ echo 'Issue: https://github.com/pytorch/pytorch/issues/123649'
If you see error like: ImportError: /lib64/libc.so.6: version GLIBC_2.28 not found
Please migrate to: https://github.com/pytorch/test-infra/blob/main/.github/workflows/linux_job_v2.yml
Issue: https://github.com/pytorch/pytorch/issues/123649
Error: Process completed with exit code 1.```

### Versions

2.6.0

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @aakhundov @ezyang @gchanan @zou3519 @msaroufim","closed","Refatoração","{triaged,""oncall: pt2"",""module: inductor""}",4,"2024-12-17 18:42:12","2024-12-24 21:48:34","2024-12-24 21:48:34","atalman","jansel",NULL,"https://github.com/pytorch/pytorch/issues/143406",7
"75","2745662972","2745662972","143401","fix a few int64_t index computations, fix complex128 scan that had to…","…o few threads
per title

cc @eqy","closed","Refatoração","{Merged,ciflow/trunk,""release notes: cuda""}",3,"2024-12-17 18:08:23","2025-01-18 02:05:02","2024-12-18 04:27:29","ngimel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143401",1
"76","2745640480","2745640480","143399","Fix unused variables in test/torch.py","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143399","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-12-17 17:57:01","2025-01-18 02:05:57","2024-12-18 17:57:26","rec",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143399",1
"77","2745569577","2745569577","143396","Fix unused Python variables in test/nn","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143396","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",9,"2024-12-17 17:28:16","2025-01-17 17:07:23","2024-12-18 03:30:57","rec",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143396",1
"78","2745568912","2745568912","143395","[BE] Refactor argument parsing into its own function","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #143513
* #143512
* #143511
* __->__ #143395","closed","Refatoração","{Merged,""topic: not user facing""}",3,"2024-12-17 17:27:56","2025-01-18 02:06:08","2024-12-18 19:42:51","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143395",1
"79","2788190330","2788190330","144796","Fix FakeTensor device creation for MPS","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #144827
* #144826
* __->__ #144796
* #144795

By promoting torch.device(""mps"") to `torch.device(""mps:0"")`, but skipping `is_initialized` check, as MPS does not really support multi-GPU right now

This fixes `GPUTests.test_remove_no_ops_mps`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov @BoyuanFeng","closed","Refatoração","{Merged,""topic: not user facing"",ciflow/mps,""module: inductor"",ciflow/inductor}",1,"2025-01-14 20:09:37","2025-01-15 05:01:28","2025-01-15 05:01:27","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144796",1
"97","2685039440","2685039440","141407","Add triton_op test for user defined triton caching","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #141407

Fix failing internal codecache test

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",3,"2024-11-23 00:41:04","2024-12-24 02:04:50","2024-11-23 07:54:41","oulgen",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141407",0
"80","2788190250","2788190250","144795","[BE] Extend `test_remove_no_ops`","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #144827
* #144826
* #144796
* __->__ #144795

----

- Use `is_dtype_supported` to skip dtype promotions portion of the test on unsupported device
- Extend it to use `torch.float16` so promotions could be checked there
- Implement `CpuInterface.is_bfloat16_supported` that returns true (which looks like the case, even if it's supported via emulation)
cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov @BoyuanFeng","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",ciflow/mps,""module: inductor"",""module: dynamo"",ciflow/inductor}",6,"2025-01-14 20:09:34","2025-01-15 05:01:33","2025-01-15 05:00:29","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144795",1
"82","2603991877","2603991877","138534","Use ubsan and asan","Fixes #ISSUE_NUMBER","closed","Refatoração","{""open source"",Stale,""topic: not user facing""}",2,"2024-10-22 01:21:52","2025-01-20 10:36:43","2025-01-20 10:36:43","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138534",90
"83","2603867562","2603867562","138526","[Cherry-Pick] Use cuda 12.4 pytorch_extra_install_requirements as default","Cherry-Picks https://github.com/pytorch/pytorch/pull/138458
Need to do it manually due to conflict with generated files.","closed","Refatoração","{""topic: not user facing""}",1,"2024-10-21 23:23:16","2024-10-22 01:50:51","2024-10-22 01:11:08","atalman",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138526",1
"88","2689612714","2689612714","141480","[Quant][PT2E][X86] annotate and convert for linear_dynamic_fp16","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #141556
* #141549
* __->__ #141480

Annotate linear node for `linear_dynamic_fp16` with `X86InductorQuantizer`
After `convert_pt2e`, the pattern will be
```
  x
  |
linear <- to_fp32 <- to_fp16 <- w
```

**Test plan**
```
pytest test/quantization/pt2e/test_x86inductor_quantizer.py -k test_linear_dynamic_fp16
```

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @SherlockNoMad @EikanWang @wenzhe-nrv","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""release notes: quantization"",intel,fx}",4,"2024-11-25 07:44:15","2024-12-30 02:07:12","2024-11-29 07:48:42","Xia-Weiwen",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141480",4
"106","2614798301","2614798301","138925","Allow schedules to run with single stage","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138925

Ran into issues (https://github.com/pytorch/pytorch/pull/138863) when adding a Schedule with a single stage, so adding code to support this edge case (mostly for test purposes)

cc @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,ciflow/trunk,""release notes: distributed (pipeline)"",""module: pipelining""}",3,"2024-10-25 18:03:29","2024-11-30 02:07:57","2024-10-30 17:33:27","H-Huang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138925",5
"84","2603856229","2603856229","138524","[aoti] Print output name for sympy.Expr as well","To avoid
```
NotImplementedError: unsupported type of output=s0*s1
```

It seems like this was caused by the use of `_scaled_dot_product_flash_attention`.

Fallback kernek:
```
FallbackKernel(
  python_kernel_name='torch.ops.aten._scaled_dot_product_flash_attention.default',
  name=buf55,
  layout=MultiOutputLayout(device=device(type='cuda', index=0)),
  inputs=[ComputedBuffer(name='buf52', layout=FixedLayout('cuda', torch.bfloat16, size=[1, 6, s0*s1, 64], stride=[384*s0*s1, 64*s0*s1, 64, 1]), data=Pointwise(device=device(type='cuda', index=0), dtype=torch.bfloat16, inner_fn=<function BaseView.make_loader.<locals>.loader at 0x7fcd7f99da20>, ranges=[1, 6, s0*s1, 64])), ComputedBuffer(name='buf53', layout=FixedLayout('cuda', torch.bfloat16, size=[1, 6, s0*s1, 64], stride=[384*s0*s1, 64*s0*s1, 64, 1]), data=Pointwise(device=device(type='cuda', index=0), dtype=torch.bfloat16, inner_fn=<function BaseView.make_loader.<locals>.loader at 0x7fcd7f99d480>, ranges=[1, 6, s0*s1, 64])), ComputedBuffer(name='buf54', layout=FixedLayout('cuda', torch.bfloat16, size=[1, 6, s0*s1, 64], stride=[384*s0*s1, 64*s0*s1, 64, 1]), data=Pointwise(device=device(type='cuda', index=0), dtype=torch.bfloat16, inner_fn=<function BaseView.make_loader.<locals>.loader at 0x7fcd7f99c430>, ranges=[1, 6, s0*s1, 64]))],
  constant_args=(0.125,),
  kwargs={'scale': 0.125},
  output_view=None,
  python_kernel_name=torch.ops.aten._scaled_dot_product_flash_attention.default,
  cpp_kernel_name=at::_ops::_scaled_dot_product_flash_attention::call,
  ordered_kwargs_for_cpp_kernel=['scale'],
  op_overload=aten._scaled_dot_product_flash_attention.default,
  arg_properties=[{'name': 'query', 'type': Tensor, 'default_value': None}, {'name': 'key', 'type': Tensor, 'default_value': None}, {'name': 'value', 'type': Tensor, 'default_value': None}, {'name': 'dropout_p', 'type': float, 'default_value': 0.0}, {'name': 'is_causal', 'type': bool, 'default_value': False}, {'name': 'return_debug_mask', 'type': bool, 'default_value': False}],
  kwarg_properties=None,
  unbacked_bindings=None,
  mutation_outputs=[],
  origin_node=None,
  origins=OrderedSet([_scaled_dot_product_flash_attention])
)
```

codegen with this pr
```
// Topologically Sorted Source Nodes: [scaled_dot_product_attention], Original ATen: [aten._scaled_dot_product_flash_attention]
    double var_147 = 0.125;
    AtenTensorHandle buf56_handle;
    AtenTensorHandle buf57_handle;
    auto buf55_4 = s0*s1;
    auto buf55_5 = s0*s1;
    AtenTensorHandle buf58_handle;
    AtenTensorHandle buf59_handle;
    AtenTensorHandle buf60_handle;
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_cuda__scaled_dot_product_flash_attention(convert_arrayref_tensor_to_tensor(buf52), convert_arrayref_tensor_to_tensor(buf53), convert_arrayref_tensor_to_tensor(buf54), 0.0, 0, 0, &var_147, &buf56_handle, &buf57_handle, nullptr, nullptr, &buf55_4, &buf55_5, &buf58_handle, &buf59_handle, &buf60_handle));
    RAIIAtenTensorHandle buf56(buf56_handle);
    RAIIAtenTensorHandle buf57(buf57_handle);
    RAIIAtenTensorHandle buf58(buf58_handle);
    RAIIAtenTensorHandle buf59(buf59_handle);
    RAIIAtenTensorHandle buf60(buf60_handle);
```

Differential Revision: D64724460

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",12,"2024-10-21 23:14:21","2024-10-29 16:03:51","2024-10-29 16:02:48","henrylhtsang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138524",8
"85","2731258547","2731258547","142576","DISABLED test_op_has_batch_rule_sgn_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_op_has_batch_rule_sgn_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_op_has_batch_rule_sgn_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_sgn_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 15758336 on device 0. CUDA driver allocated memory was 185860096 and is now 206831616.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_sgn_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 21:47:04","2024-12-19 20:21:32","2024-12-19 20:21:32","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142576",9
"86","2731257214","2731257214","142569","DISABLED test_op_has_batch_rule_repeat_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_op_has_batch_rule_repeat_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_op_has_batch_rule_repeat_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_repeat_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 11776 on device 0. CUDA driver allocated memory was 183762944 and is now 185860096.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_repeat_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 21:46:32","2024-12-19 20:21:30","2024-12-19 20:21:30","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142569",9
"87","2731257213","2731257213","142568","DISABLED test_op_has_batch_rule_outer_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_op_has_batch_rule_outer_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_op_has_batch_rule_outer_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_outer_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 1024 on device 0. CUDA driver allocated memory was 182714368 and is now 184811520.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_outer_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 21:46:32","2024-12-19 20:21:29","2024-12-19 20:21:29","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142568",9
"208","2595687301","2595687301","138249","Move test_verifier to training IR","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #138261
* #138251
* __->__ #138249

Differential Revision: [D64560351](https://our.internmc.facebook.com/intern/diff/D64560351)","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",7,"2024-10-17 20:32:27","2024-11-18 02:10:52","2024-10-18 07:37:34","tugsbayasgalan",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138249",1
"89","2689472330","2689472330","141479","[1/N] Add Intel GPU Support to Torch Test Cases","For RFC https://github.com/pytorch/pytorch/issues/142029, make the GPU information checkers in inductor general for all the tests:

- Defined GPU_TYPES and get_gpu_type() to torch/_utils.py so that it can be used by both inductor tests and other tests.
- Move the definition of HAS_CUDA/HAS_XPU/HAS_GPU to torch/testing/_internal/common_utils.py, so that it can be shared by inductor tests and other tests.
- Rename the HAS_GPU in inductor to HAS_TRITON_GPU because it also check the availability of triton.


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""oncall: distributed"",""open source"",""topic: not user facing"",""module: inductor"",""module: dynamo"",ciflow/xpu}",3,"2024-11-25 07:03:36","2024-12-25 14:52:02","2024-12-25 14:52:02","daisyden",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141479",30
"90","2689315469","2689315469","141477","[MPS] Make MetalShaderLibrary usable from C++","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #141478
* __->__ #141477
* #141476
* #141475
* #141474

By guarding Metal framework include and defining all ObjC protocols to dummy `void*`","closed","Refatoração","{Merged,""topic: not user facing"",""release notes: mps"",ciflow/mps}",3,"2024-11-25 06:11:09","2024-12-26 02:05:07","2024-11-25 18:41:01","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141477",0
"91","2728104195","2728104195","142404","[c10d] Update `backend` arg documentation","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #142404

Update doc to reflect change brought by https://github.com/pytorch/pytorch/pull/142216

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,ciflow/trunk,""release notes: distributed (c10d)""}",6,"2024-12-09 19:46:07","2025-01-11 02:08:25","2024-12-09 21:53:47","kwen2501",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142404",0
"92","2657113796","2657113796","140633","Fix misuse of offset param in seek","Fixes #115630.  

The size of BufferAdapter has been calculated wrongly due to misuse of python method seek. Causes miniz reader initialized with wrong size.

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel","closed","Refatoração","{""oncall: jit"",triaged,""open source"",Merged,ciflow/trunk,""release notes: jit""}",6,"2024-11-13 23:30:37","2024-11-15 19:09:00","2024-11-15 19:07:56","cz2h",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140633",2
"94","2685354516","2685354516","141418","Make computeStorageNbytes a bit more lazy, don't directly guard size oblivious","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #141418

Signed-off-by: Edward Z. Yang <ezyang@meta.com>","closed","Refatoração","{}",3,"2024-11-23 04:42:12","2024-12-25 02:04:30","2024-11-24 14:03:41","ezyang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141418",1
"95","2685147177","2685147177","141414","[TEST] Check if duck sizing is dead","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #141414

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

cc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal @voznesenskym @penguinwu @EikanWang @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{""module: mkldnn"",""module: dynamo"",ciflow/inductor,ciflow/linux-aarch64}",2,"2024-11-23 02:56:02","2025-01-11 02:07:09","2024-12-10 19:47:46","ezyang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141414",17
"96","2685115666","2685115666","141412","Bump onnxscript version in CI","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #141413
* __->__ #141412","closed","Refatoração","{""module: onnx"",""open source"",Merged,ciflow/trunk,""release notes: onnx"",""topic: not user facing""}",3,"2024-11-23 02:18:52","2024-12-25 02:04:43","2024-11-24 06:51:50","justinchuby",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141412",1
"98","2619867872","2619867872","139137","TunableOp use dense size calculations as minimum sizes","Fixes #139116.  Also fixes other unreported issues with torch.bmm due to incorrect size calculations.","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",rocm,ciflow/rocm}",6,"2024-10-29 00:56:38","2024-10-31 06:03:03","2024-10-31 06:02:01","jeffdaily",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139137",2
"99","2619844161","2619844161","139135","[profiler] Annotate triton kernels with kernel hash","Summary:
As above, annotates triton kernel hash in the profile attributes.

Added a new unit test in profiler to triton/dynamo events.

Test Plan: Running new unit test in CI

Differential Revision: D64937468




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{fb-exported,Stale,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",8,"2024-10-29 00:30:04","2025-01-31 02:00:24","2025-01-31 02:00:24","briancoutinho",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139135",94
"101","2619783913","2619783913","139128","[Submodule] update submodule onnx==1.17.0","Follow-up PR of: https://github.com/pytorch/pytorch/pull/138719

CC @malfet @ezyang","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",11,"2024-10-28 23:37:24","2024-10-31 02:51:05","2024-10-31 02:50:03","ptrblck",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139128",3
"102","2619766115","2619766115","139127","[export] Modify swap to take in lambda","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139127
* #139126

Differential Revision: [D65100693](https://our.internmc.facebook.com/intern/diff/D65100693)","closed","Refatoração","{Stale,ciflow/trunk,""release notes: export""}",3,"2024-10-28 23:23:01","2025-01-09 23:36:40","2025-01-09 23:36:40","angelayi",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139127",73
"103","2614970448","2614970448","138931","Gaussian nll loss scalar variance support","Fixes #138747

Adds support for `variance` being a Tensor or a float in `gaussian_nll_loss` to avoid a cpu-gpu sync point in the loss function, when the variance is a static tensor like `<scalar>*torch.ones_like(input)`","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""release notes: nn"",""topic: not user facing"",suppress-bc-linter}",20,"2024-10-25 19:43:55","2024-12-04 07:52:31","2024-11-21 18:20:12","michael-diggin",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138931",27
"104","2614882493","2614882493","138929","[export] add mark_unbacked to export dynamic_shapes API","Adding `Dim.UNBACKED` to represent unbacked dims, basically calls `torch._dynamo.decorators.mark_unbacked()` under the hood. Does some plumbing for unbacked bindings & unbacked hints.




cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{fb-exported,Stale,fx,ciflow/inductor,""release notes: export""}",5,"2024-10-25 18:49:16","2025-01-26 04:35:05","2025-01-26 04:35:05","pianpwk",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138929",93
"105","2614841844","2614841844","138927","[export] Fix non-strict retracing with kwargs","Summary:
`torch.fx.Interpreter.run()` only takes args as input. Currently we pass kwargs as well which causes errors during retracing.

Flatten the kwargs and concat them with args will solve the issue.

Several previously failing tests under `_retraceability_non_strict` now passes.

Test Plan:
```
buck2 test @//mode/dev-nosan //caffe2/test:test_export -- -r _retraceability_non_strict
```

Differential Revision: D64980053","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""release notes: export""}",7,"2024-10-25 18:24:52","2024-10-29 04:32:25","2024-10-29 04:31:23","yiming0416",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138927",4
"107","2617045993","2617045993","139025","[Intel GPU] Support RegisterXPU.cpp codegen and compile for the in-tree XPU structured GEMM OPs.","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #135320
* #135318
* #139026
* #136742
* __->__ #139025

[Intel GPU] Support RegisterXPU.cpp codegen and compile for the in-tree XPU structured GEMM ops.

Motivation: There are two parts of aten ops for XPU, one is in-tree ops like GEMM related OPs and the other is out-off-tree ops in torch-xpu-ops. For the in-tree part，since Pytorch uses native_functions.yaml registration and is equipped with convenient codegen capabilities, we want to take advantage of these benefits as well.
At the same time, since AOT Inductor also uses native_functions.yaml to generate c shim wrappers, we also need to enable this mechanism for XPU.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""module: cpu"",""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",suppress-bc-linter,ciflow/xpu}",6,"2024-10-28 01:22:58","2024-12-10 02:12:48","2024-11-09 13:09:30","etaf",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139025",12
"108","2616914121","2616914121","139020","Add missing description for input parameters","Add missing descriptions for parameter `_use_new_zipfile_serialization` and `_disable_byteorder_record`.

Fixes #138017 


cc @svekars @brycebortree @sekyondaMeta","closed","Refatoração","{""module: docs"",triaged,""open source""}",7,"2024-10-27 22:32:48","2024-11-27 22:19:47","2024-11-27 22:19:47","cz2h",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139020",31
"109","2616900018","2616900018","139018","Fix some nits in symbolic_shapes.py","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139018

While I was reading through this file for understanding, I fixed some nits.

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{Merged,ciflow/trunk,""release notes: fx"",""topic: not user facing"",fx,ciflow/inductor}",3,"2024-10-27 21:58:15","2024-11-28 02:13:26","2024-10-28 04:27:14","bobrenjc93",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139018",1
"110","2618698163","2618698163","139069","[Flex Attention] Dynamic shapes fix","Fixes #139064


@Chillee 


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""open source"",""module: inductor""}",6,"2024-10-28 15:02:23","2024-12-11 21:17:28","2024-12-11 21:17:28","alexdremov",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139069",44
"111","2618285339","2618285339","139062","[Bugfix] UnicodeDecodeError: 'utf-8' codec can't decode byte","Fixes #113564

When I used PyTorch's profiler to analyze the performance of vLLM, I encountered the following error. This error is similar to #113564. After analysis and troubleshooting, I changed the temporary file from text mode to binary mode, and it no longer reported an error and ran normally.

```bash
ERROR 10-28 10:25:50 engine.py:160]   File ""/usr/local/lib/python3.12/dist-packages/torch/profiler/profiler.py"", line 722, in stop 
ERROR 10-28 10:25:50 engine.py:160]     self._transit_action(self.current_action, None) 
ERROR 10-28 10:25:50 engine.py:160]   File ""/usr/local/lib/python3.12/dist-packages/torch/profiler/profiler.py"", line 751, in _transit_action 
ERROR 10-28 10:25:50 engine.py:160]     action() 
ERROR 10-28 10:25:50 engine.py:160]   File ""/usr/local/lib/python3.12/dist-packages/torch/profiler/profiler.py"", line 745, in _trace_ready 
ERROR 10-28 10:25:50 engine.py:160]     self.on_trace_ready(self) 
ERROR 10-28 10:25:50 engine.py:160]   File ""/usr/local/lib/python3.12/dist-packages/torch/profiler/profiler.py"", line 444, in handler_fn 
ERROR 10-28 10:25:50 engine.py:160]     prof.export_chrome_trace(os.path.join(dir_name, file_name)) 
ERROR 10-28 10:25:50 engine.py:160]   File ""/usr/local/lib/python3.12/dist-packages/torch/profiler/profiler.py"", line 220, in export_chrome_trace 
ERROR 10-28 10:25:50 engine.py:160]     fout.writelines(fin) 
ERROR 10-28 10:25:50 engine.py:160]   File ""<frozen codecs>"", line 322, in decode 
ERROR 10-28 10:25:50 engine.py:160] UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8e in position 5896: invalid start byte
```","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""release notes: profiler"",""topic: bug fixes""}",14,"2024-10-28 12:27:47","2025-01-23 11:02:44","2024-10-29 17:16:29","Abatom",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139062",1
"113","2747233054","2747233054","143492","[Inductor UT] Mark test case test_linear_and_cel as requires_cuda as","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #142322
* __->__ #143492
* #143491

it's only for cuda now.
Fix #143479

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""open source"",""topic: not user facing"",""module: inductor"",ciflow/inductor,ciflow/xpu}",1,"2024-12-18 09:10:47","2025-01-26 02:04:54","2024-12-27 00:41:34","etaf",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143492",9
"114","2746731051","2746731051","143467","log guard_size_oblivious call sites","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143467

This makes it much easier to know what's going on when we guard on data dependent operations. Currently if we throw on guard on data dependent, we only show the python invocation that caused it (not the underlying leaf c++ call which truly causes it). 

For reviewers: I was torn on whether or not to make this a special thing separate to TORCH_LOGS=""dynamic"" but decided against it since dynamic is already opt in. The benefit is we now get much more visibility when users run with this flag on, but logs do get a bit more spewy. I'm still open to the idea of moving these logs somewhere else and maybe building a UI on top of it to make it easier to manage the information overload.

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{""release notes: fx"",""topic: not user facing"",fx,ciflow/inductor}",6,"2024-12-18 04:23:37","2025-01-19 02:07:19","2024-12-19 15:19:00","bobrenjc93",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143467",1
"115","2725375965","2725375965","142337","[torchgen] Fix an unused variable in api/python.py","Extracted from https://github.com/pytorch/pytorch/pull/136359

Changes behavior, but the original code seems like it was an obvious oops.

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #142337","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-12-08 16:44:25","2025-01-08 02:05:26","2024-12-08 21:48:10","rec",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142337",0
"116","2725345576","2725345576","142335","[aarch64] add CUDA 12.6 sbsa nightly binary","related to #138440 

cc @atalman @Aidyn-A @nWEIdia @ptrblck","closed","Refatoração","{""open source"",Merged,ciflow/binaries,ciflow/trunk,""release notes: releng""}",8,"2024-12-08 15:47:28","2024-12-10 15:13:01","2024-12-10 06:19:31","tinglvv",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142335",2
"117","2725255340","2725255340","142332","Re-enable some C++ warnings","It enables some C++ warnings since the code base is fairly clean. Meanwhile, Wextra-semi is disabled on CUDA generated code since there is no way to fix them without the cooperation of CUDA team.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing"",ciflow/periodic,ciflow/s390}",6,"2024-12-08 13:52:16","2024-12-17 23:13:02","2024-12-12 04:02:15","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142332",4
"118","2736952403","2736952403","143145","add README.md for compile time benchmarks","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143145
* #143143



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor}",3,"2024-12-12 21:53:49","2025-01-14 02:03:29","2024-12-13 05:12:28","bobrenjc93",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143145",1
"119","2736903393","2736903393","143142","[BC BREAKING] Change n_frames and padding computation in stft","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143142

Signed-off-by: Edward Z. Yang <ezyang@meta.com>","closed","Refatoração","{""topic: bc breaking""}",6,"2024-12-12 21:22:01","2025-01-27 15:09:29","2025-01-27 15:09:29","ezyang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143142",46
"120","2736831518","2736831518","143137","[ROCm] Improve performance of reduce sum for 3D shapes","Improve performance of reduce sum for 3D shapes.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",triaged,""open source"",Merged,""release notes: cuda"",ciflow/rocm}",3,"2024-12-12 20:34:39","2024-12-13 19:03:06","2024-12-13 19:02:02","doru1004",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143137",1
"144","2737056769","2737056769","143152","Migrate compiler config to Config","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143152
* #143229","closed","Refatoração","{Merged,Reverted,ciflow/trunk,""topic: not user facing"",ci-no-td}",10,"2024-12-12 23:18:49","2025-01-14 02:05:06","2024-12-14 07:38:28","oulgen",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143152",2
"303","2728706506","2728706506","142447","[Device] Add ""mps"" to `torch._utils._get_device_attr`","Follow up after  https://github.com/pytorch/pytorch/pull/141098","closed","Refatoração","{Merged,ciflow/trunk,""release notes: python_frontend"",""topic: improvements"",""topic: bug fixes""}",6,"2024-12-10 01:31:59","2024-12-12 22:29:39","2024-12-10 20:57:20","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142447",0
"121","2736795994","2736795994","143135","[Inductor] Fix cooperative reduction tests broken in recent refactor","These tests were broken by https://github.com/pytorch/pytorch/pull/142020. This PR updates the fixed configs accordingly.


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""module: cpu"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",6,"2024-12-12 20:14:17","2025-01-14 02:03:23","2024-12-13 02:03:46","blaine-rister",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143135",1
"122","2598629631","2598629631","138383","Allow inplacing buffer when other users are inconsequential","Summary:
I think we can inplace a buffer if all of the users of said buffer are ""inconsequential"", defined as having been removed, being completed, or being part of the ancestors set. In particular, this allows LayerNorm to inplace its input buffer.

Implements:
https://github.com/pytorch/pytorch/issues/132826

Test Plan:
New unit test of matmul followed by LayerNorm, make sure there's an inplaced buffer.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,Reverted,ciflow/trunk,""topic: performance"",""topic: not user facing"",""module: inductor"",ciflow/inductor,""release notes: inductor"",ci-no-td}",35,"2024-10-19 01:11:33","2024-12-06 02:13:26","2024-11-05 03:44:12","exclamaforte",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138383",17
"124","2598588295","2598588295","138380","[export] Add retraceability_non_strict to tests","Summary: We expand the tests to cover retraceability_non_strict. Currently failing tests are skipped.

Test Plan:
```
buck2 test @//mode/dev-nosan //caffe2/test:test_export -- -r _retraceability
```

Differential Revision: D64611532","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""topic: not user facing""}",10,"2024-10-18 23:57:26","2024-10-22 21:06:55","2024-10-22 21:05:53","yiming0416",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138380",4
"125","2598561254","2598561254","138379","[AOTI] Fix check_model_with_multiple_inputs in test_aot_inductor","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138379
* #138544

Summary: Add missing use_minimal_arrayref_interface setting to check_model_with_multiple_inputs.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @chauhang @aakhundov

Differential Revision: [D64635211](https://our.internmc.facebook.com/intern/diff/D64635211)","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor""}",8,"2024-10-18 23:38:48","2024-11-22 02:09:50","2024-10-23 00:54:31","desertfire",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138379",5
"126","2598504962","2598504962","138376","[Flex Attention] Don't compute fill order to compute stride order just to get fill order back","Was a bit confusing to read when working on #138354

""computer-assisted proof""
```
import random

def argsort(seq):
    # preserve original order for equal strides
    getter = seq.__getitem__
    a_r = range(len(seq))
    return list(reversed(sorted(a_r, key=getter, reverse=True)))  # noqa: C413

def stride_order2fill_order(order):
    """"""
    Convert stride order to fill order
    For channel last format,

    stride order = [3, 0, 2, 1] and fill order = [1, 3, 2, 0]
    """"""
    lookup = {pos: idx for idx, pos in enumerate(order)}
    fill_order = [lookup[i] for i in range(len(order))]
    return fill_order

def get_stride_order(seq):
    """"""
    Convert strides to stride order
    """"""
    sorted_idx: List[int] = argsort(seq)
    out = [0 for _ in range(len(seq))]
    a = sorted_idx.copy()
    for i, elem in enumerate(sorted_idx):
        out[elem] = i
    fillorder = stride_order2fill_order(out)
    assert fillorder == sorted_idx
    return out

for _ in range(1000):
    a = [0, 1, 2, 3]
    random.shuffle(a)
    get_stride_order(a)
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov @Chillee @drisspg @yanboliang @BoyuanFeng","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor,""module: flex attention""}",16,"2024-10-18 22:43:28","2024-10-22 18:41:44","2024-10-22 18:40:41","eqy",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138376",4
"127","2598488043","2598488043","138375","build: set environment variables for debug information and undefined …","…behavior sanitizer

Fixes #ISSUE_NUMBER","closed","Refatoração","{triaged,""open source"",Stale}",4,"2024-10-18 22:26:21","2025-01-21 15:34:55","2025-01-21 15:34:55","Pranika-Kalimuthu",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138375",95
"154","2746453932","2746453932","143454","[foreach_map] Add foreach_map Adam impl to compiled optimizer tests","Adds a foreach_map backed Adam to compiled optimizer tests

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""module: inductor"",ciflow/inductor,""release notes: inductor""}",3,"2024-12-18 00:53:11","2025-01-19 02:07:45","2024-12-19 03:16:50","mlazos",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143454",1
"128","2726645054","2726645054","142363","[aarch64] Fix libcusparselt format for CUDA sbsa docker","Corrects https://github.com/pytorch/pytorch/pull/141433/

Error whe building arm wheel https://github.com/pytorch/pytorch/actions/runs/12226514901/job/34101913511
`/opt/rh/gcc-toolset-11/root/usr/bin/ld: /usr/local/cuda/lib64/libcusparseLt.so: error adding symbols: file in wrong format`

cc @ptrblck @msaroufim @eqy @malfet @snadampal @milpuz01 @atalman @Skylion007 @nWEIdia","closed","Refatoração","{""module: cuda"",""open source"",""module: arm"",Merged,ciflow/trunk,""topic: not user facing"",ciflow/linux-aarch64}",15,"2024-12-09 10:37:00","2024-12-19 08:08:42","2024-12-09 15:29:38","tinglvv",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142363",0
"129","2726487443","2726487443","142361","DISABLED test_init_process_group (__main__.DeviceMeshTest)","Platforms: rocm

This test was disabled because it is failing on main branch ([recent examples](https://torch-ci.com/failure?failureCaptures=%5B%22distributed%2Ftest_device_mesh.py%3A%3ADeviceMeshTest%3A%3Atest_init_process_group%22%5D)).

Here is an example failure distributed/test_device_mesh.py::DeviceMeshTest::test_init_process_group [GH job link](https://github.com/pytorch/pytorch/actions/runs/12199208850/job/34034032697) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/661d1f03725fb0d412bad5c3c0001084a9a07b25)

Maybe coming from https://github.com/pytorch/pytorch/pull/142038 or https://github.com/pytorch/pytorch/pull/142216

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""oncall: distributed"",""module: rocm"",skipped}",1,"2024-12-09 09:31:41","2024-12-10 01:22:24","2024-12-10 01:22:24","huydhn",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142361",1
"145","2650495787","2650495787","140320","Respect ROCR_VISIBLE_DEVICES on AMD GPU device discovery","Fixes #140318","closed","Refatoração","{triaged,""open source"",Merged,Reverted,ciflow/trunk,""topic: not user facing"",ci-no-td}",29,"2024-11-11 21:58:36","2024-12-09 16:06:05","2024-12-06 20:09:59","tbennun",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140320",25
"130","2726476910","2726476910","142360","[Inductor] Use sleef implementation for CPP backend asinh codegen","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #142360

**Summary**
Fix https://github.com/pytorch/pytorch/issues/142345. Previously, we use `asinh(x) = log(x + sqrt(1 + x**2))` to calculate the result of `asinh`, the issue happens when input with `-10000.1`, which makes `x + sqrt(1 + x**2)` close to 0 and log(0) is invalid. We use the `sleef` implementation in this PR to fix this issue.

**Test Plan**
```
python -u -m pytest -s -v test/inductor/test_cpu_repro.py -k test_asinh_with_corner_inputs
```

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""module: cpu"",""open source"",Merged,Reverted,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor,ci-no-td}",14,"2024-12-09 09:26:54","2025-01-14 02:04:22","2024-12-14 00:27:57","leslie-fang-intel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142360",5
"131","2726443244","2726443244","142359","Add missing bc CI dependency","The [bc](https://www.geeksforgeeks.org/bc-command-linux-examples) command that I use to calculate the MAX_JOBS in https://github.com/pytorch/pytorch/pull/142164 isn't part of the Docker image https://github.com/pytorch/pytorch/actions/runs/12230618287/job/34113698986#step:14:321.  I missed this error when landing https://github.com/pytorch/pytorch/pull/142164.","closed","Refatoração","{Merged,""topic: not user facing"",ciflow/inductor}",3,"2024-12-09 09:12:29","2024-12-19 21:55:38","2024-12-09 16:07:42","huydhn",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142359",0
"133","2696488551","2696488551","141631","[FR] Include mismatch rank into mismatch_collectives and update log message","Summary: We want to return the mismatch ranks info in the `mismatch_collectives` field. Also update the logging message when no error is found and it's not partial analysis.

Test Plan: CI

Differential Revision: D66522602","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""topic: not user facing""}",25,"2024-11-26 23:47:18","2024-11-27 18:58:25","2024-11-27 18:57:24","fduwjj",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141631",1
"134","2696414673","2696414673","141626","[pytorch/profiler] Honor escape quotes arg in a profiler metadata log formatter (#141527)","Summary:

We were ignoring the with_escaped_quotes param in format_list inline function iin utils.cpp in the case where we had to truncate a list of more than kTruncatelength items.

In that case we would truncate a list into a string but always return it with an escaped quotes wrapping it. this will cause issues if this string is meant to be added to other lists which will also go through formatting. Leading to cases like `""[""[a, b, c, ...]""]""`.

now the above will be well formatted as `""[[a, b, c, ...]]""` as the escape quote requests will be honored.

Differential Revision: D66521676","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""topic: not user facing""}",9,"2024-11-26 23:25:24","2024-12-03 20:44:02","2024-12-03 20:42:59","sanrise","sanrise",NULL,"https://github.com/pytorch/pytorch/pull/141626",7
"135","2675345688","2675345688","141120","Symbolic execution fails when kernel has a fallthrough registration","### 🐛 Describe the bug

Is symbolic execution expected to fail for kernels that have fallthrough registrations? 

Reproducer with a fallthrough for the mul operator:

```python
import torch
import torch.fx.experimental
import torch.fx.experimental.proxy_tensor
from torch._subclasses.fake_tensor import FakeTensorMode
from torch.fx.experimental.symbolic_shapes import ShapeEnv

a = torch.tensor([1.0, 2.0, 3.0])
shape_env = ShapeEnv()
fake_mode = FakeTensorMode(shape_env=shape_env)
fake_a = fake_mode.from_tensor(a)


def run_mul(non_fallthrough_keys: torch.DispatchKeySet, *, has_fallthrough: bool) -> torch.Tensor:
    with (
        fake_mode,
        torch._C._EnablePythonDispatcher(),
        torch._C._IncludeDispatchKeyGuard(torch.DispatchKey.Meta),
        torch._C._ExcludeDispatchKeyGuard(
            torch._C.DispatchKeySet(torch.DispatchKey.Python).add(
                torch.DispatchKey.PythonTLSSnapshot
            )
        ),
    ):
        fake_mode.in_kernel_invocation = True
        expected_key_set = torch._ops.key_extractor(tensors=[fake_a], key_mask=non_fallthrough_keys)

        assert expected_key_set.highestPriorityTypeId() == torch.DispatchKey.PythonDispatcher
        has_meta = expected_key_set.has(torch.DispatchKey.Meta)
        if has_fallthrough:
            assert not has_meta
        else:
            assert has_meta

        # Call mul operator
        return 5 * fake_a


if __name__ == ""__main__"":
    lib = torch.library.Library(""aten"", ""IMPL"", ""CPU"")
    non_fallthrough_keys = torch._C._dispatch_keyset_full()

    # No error
    run_mul(non_fallthrough_keys, has_fallthrough=False)

    # makeFallthrough on CPU will remove all dense keys (likewise for any other dense key):
    #   CPU, CUDA, HIP, XLA, MPS, IPU, XPU, HPU, VE, Lazy, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, Meta,
    lib.impl(torch.ops.aten.mul.Tensor, torch.library.fallthrough_kernel)
    non_fallthrough_keys = non_fallthrough_keys.remove(torch.DispatchKey.CPU)
    # Does not contain CPU or Meta
    assert not non_fallthrough_keys.has(torch.DispatchKey.CPU) and not non_fallthrough_keys.has(
        torch.DispatchKey.Meta
    )

    # RuntimeError: TensorIterator does not support symbolic shapes; please implement this operator in torch/_refs using the elementwise or reduction helpers (look at backtrace to find out what operator this is)
    run_mul(non_fallthrough_keys, has_fallthrough=True)
```

Error:
```shell
/usr/local/lib/python3.10/dist-packages/torch/library.py:346: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::mul.Tensor(Tensor self, Tensor other) -> Tensor
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: CPU
  previous kernel: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079
       new kernel: registered at /dev/null:3553 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:155.)
  self.m.impl(
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-3-3bcae3f373b9> in <cell line: 38>()
     53 
     54     # RuntimeError: TensorIterator does not support symbolic shapes; please implement this operator in torch/_refs using the elementwise or reduction helpers (look at backtrace to find out what operator this is)
---> 55     run_mul(non_fallthrough_keys, has_fallthrough=True)

<ipython-input-3-3bcae3f373b9> in run_mul(non_fallthrough_keys, has_fallthrough)
     33 
     34         # Call mul operator
---> 35         return 5 * fake_a
     36 
     37 

RuntimeError: TensorIterator does not support symbolic shapes; please implement this operator in torch/_refs using the elementwise or reduction helpers (look at backtrace to find out what operator this is)
```

### Versions

```shell
Collecting environment information...
PyTorch version: 2.6.0.dev20241120+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 14.0.0-1ubuntu1.1
CMake version: version 3.30.5
Libc version: glibc-2.35

Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-6.1.85+-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.2.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 535.104.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        46 bits physical, 48 bits virtual
Byte Order:                           Little Endian
CPU(s):                               2
On-line CPU(s) list:                  0,1
Vendor ID:                            GenuineIntel
Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz
CPU family:                           6
Model:                                79
Thread(s) per core:                   2
Core(s) per socket:                   1
Socket(s):                            1
Stepping:                             0
BogoMIPS:                             4399.99
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
Hypervisor vendor:                    KVM
Virtualization type:                  full
L1d cache:                            32 KiB (1 instance)
L1i cache:                            32 KiB (1 instance)
L2 cache:                             256 KiB (1 instance)
L3 cache:                             55 MiB (1 instance)
NUMA node(s):                         1
NUMA node0 CPU(s):                    0,1
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Mitigation; PTE Inversion
Vulnerability Mds:                    Vulnerable; SMT Host state unknown
Vulnerability Meltdown:               Vulnerable
Vulnerability Mmio stale data:        Vulnerable
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Vulnerable
Vulnerability Spec rstack overflow:   Not affected
Vulnerability Spec store bypass:      Vulnerable
Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Vulnerable

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] nvidia-cublas-cu12==12.4.5.8
[pip3] nvidia-cuda-cupti-cu12==12.4.127
[pip3] nvidia-cuda-nvrtc-cu12==12.4.127
[pip3] nvidia-cuda-runtime-cu12==12.4.127
[pip3] nvidia-cudnn-cu12==9.1.0.70
[pip3] nvidia-cufft-cu12==11.2.1.3
[pip3] nvidia-curand-cu12==10.3.5.147
[pip3] nvidia-cusolver-cu12==11.6.1.9
[pip3] nvidia-cusparse-cu12==12.3.1.170
[pip3] nvidia-cusparselt-cu12==0.6.2
[pip3] nvidia-nccl-cu12==2.21.5
[pip3] nvidia-nvjitlink-cu12==12.4.127
[pip3] nvidia-nvtx-cu12==12.4.127
[pip3] nvtx==0.2.10
[pip3] optree==0.13.1
[pip3] pynvjitlink-cu12==0.4.0
[pip3] pytorch-triton==3.1.0+cf34004b8a
[pip3] torch==2.6.0.dev20241120+cu124
[pip3] torchaudio==2.5.1+cu121
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.20.1+cu121
[conda] Could not collect
```

cc @chauhang @penguinwu @ezyang @bobrenjc93 @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{triaged,""oncall: pt2"",""module: dynamic shapes""}",5,"2024-11-20 10:31:56","2024-12-09 20:32:46","2024-12-09 20:32:46","kundaMwiza",NULL,NULL,"https://github.com/pytorch/pytorch/issues/141120",19
"136","2674510685","2674510685","141110","add inductor-cu126 tag","related to https://github.com/pytorch/pytorch/issues/138440
for testing https://github.com/pytorch/pytorch/pull/140793

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal @mcarilli @ptrblck @leslie-fang-intel @voznesenskym @penguinwu @EikanWang @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @xmfan @atalman","closed","Refatoração","{""oncall: distributed"",""module: cpu"",""module: mkldnn"",""open source"",""module: amp (automated mixed precision)"",ciflow/trunk,""release notes: quantization"",""topic: not user facing"",ciflow/mps,""module: inductor"",""module: dynamo"",ciflow/inductor,""module: distributed_checkpoint"",""module: compiled autograd"",ciflow/linux-aarch64}",5,"2024-11-20 05:45:30","2024-11-21 20:06:14","2024-11-21 20:03:56","tinglvv",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141110",1
"140","2635486830","2635486830","139753","Add SVE implementation for 8 bit quantized embedding bag on aarch64","cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01","closed","Refatoração","{""module: cpu"",""open source"",""module: arm"",Stale,""release notes: quantization""}",3,"2024-11-05 13:35:41","2025-01-23 22:06:17","2025-01-23 22:06:17","annop-w",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139753",79
"141","2635138284","2635138284","139749","Tests Generelization for multiple accelerator devices","Motivation: Generalize unit tests so that can be executed for cuda and non cuda devices.
Chnages: There are general changes in common_dtesnor module for device type generalization so that tests can be executed on non cuda devices too.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing""}",50,"2024-11-05 11:04:35","2025-01-14 08:53:52","2025-01-14 08:52:49","rahulsingh-intel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139749",70
"142","2737108667","2737108667","143156","[BE] Stop using deprecated APIs in mkldnn_pattern_matcher","This should fix
```
/var/lib/jenkins/workspace/test/inductor/test_mkldnn_pattern_matcher.py:157: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,""topic: not user facing"",""module: inductor"",ciflow/inductor}",4,"2024-12-13 00:09:18","2025-01-14 02:04:15","2024-12-13 06:37:22","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143156",0
"143","2737060150","2737060150","143153","Prefer normal stride selection logic when tensor is contiguous","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143153

Fixes: https://github.com/pytorch/pytorch/issues/142024","closed","Refatoração","{""release notes: fx"",fx,""module: dynamo"",ciflow/inductor}",3,"2024-12-12 23:22:18","2025-01-18 02:02:39","2024-12-17 03:53:03","bobrenjc93",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143153",5
"330","2753656321","2753656321","143693","Fix issue with setAttribute and int8_t vs int32_t variables","Test Plan: Sandcastle","closed","Refatoração","{fb-exported,Merged,Reverted,ciflow/trunk,""release notes: cpp"",""topic: improvements"",ci-no-td}",78,"2024-12-21 00:14:53","2024-12-23 22:33:22","2024-12-21 05:31:59","r-barnes",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143693",0
"146","2650484257","2650484257","140319","ROCm: Enable 4 gpu tests for distributed config","Change the label to make sure the jobs land on a
node which has >= 4 GPUs.


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""oncall: distributed"",""module: rocm"",""open source"",Merged,""topic: not user facing"",ciflow/periodic,keep-going,ciflow/rocm}",8,"2024-11-11 21:51:39","2025-01-02 17:23:16","2025-01-02 17:22:14","jagadish-amd",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140319",52
"147","2650471459","2650471459","140318","Visible devices are not respected on AMD systems","### 🐛 Describe the bug

When running distributed (or multi-process) jobs on AMD GPU systems with GPU isolation (i.e., the `*_VISIBLE_DEVICES` environment variable), PyTorch still tries to set the seed of all the devices it finds. This causes the following exception to be raised:

```
File ""/path/to/torch/random.py"", line 46, in manual_seed                                                                                               
  torch.cuda.manual_seed_all(seed)                                                                                                                                                                      
File ""/path/to/torch/cuda/random.py"", line 131, in manual_seed_all                                                                                    
  _lazy_call(cb, seed_all=True)                                                                                                                                                                        
File ""/path/to/torch/cuda/__init__.py"", line 249, in _lazy_call                                                                                       
  callable()                                                                                                                                                                                            
File ""/path/to/torch/cuda/random.py"", line 128, in cb                                                                                                  
  default_generator = torch.cuda.default_generators[i]                                                                                                                                                 
                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^                                                                                                                                                 
IndexError: tuple index out of range
```


The underlying issue is that the visible devices are not properly detected due to reading the wrong environment variable name: `HIP_VISIBLE_DEVICES` should be renamed to `ROCR_VISIBLE_DEVICES` (see https://rocm.docs.amd.com/en/latest/conceptual/gpu-isolation.html#rocr-visible-devices ). On many batch schedulers and clusters, the ROCr environment variable is the one that is set.

This was likely introduced in #119182 (cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd)

I will push a pull request to fix this later today.

### Versions

PyTorch version: 2.5.1+rocm6.2","closed","Refatoração","{""module: rocm"",triaged}",12,"2024-11-11 21:43:09","2024-12-25 02:37:13","2024-12-25 02:37:13","tbennun","jataylo",NULL,"https://github.com/pytorch/pytorch/issues/140318",44
"148","2733368211","2733368211","142871","add private config to temporarily preserve old FSDP guard behavior","Summary: https://github.com/pytorch/pytorch/pull/138819 wobbled dynamo guards in a way that caused some performance regression, so this PR temporarily adds a config to get the old behavior back while we investigate.

Test Plan: CI

Differential Revision: D67096751




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Teste de Regressão","{fb-exported,Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor}",11,"2024-12-11 16:00:46","2024-12-13 22:07:53","2024-12-13 22:06:50","bdhirsh",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142871",2
"149","2733304820","2733304820","142869","Upgrade expecttest to 0.3.0","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #142869

Signed-off-by: Edward Z. Yang <ezyang@meta.com>","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",6,"2024-12-11 15:34:11","2025-01-11 02:11:30","2024-12-11 19:04:19","ezyang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142869",0
"150","2732284975","2732284975","142858","Apply clang-format for ATen/core/boxing cpp files","Code change via add path config in `.lintrunner.toml` file and running

```bash
 $ lintrunner -a --take CLANGFORMAT --all-files
```","closed","Refatoração","{""open source"",""topic: not user facing""}",2,"2024-12-11 08:56:33","2025-01-13 09:09:53","2024-12-12 01:27:24","zeshengzong",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142858",1
"151","2748386329","2748386329","143508","Upgrade submodule ideep for bf16f32 matmul changes","This change will enable this PR #140159  to pick proper kernels in bf16 mode for SDPA layer.

cc: @yanbing-j 


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal @malfet @milpuz01","closed","Refatoração","{""module: cpu"",triaged,""module: mkldnn"",""open source"",""module: arm"",Merged,ciflow/trunk,""topic: not user facing"",ciflow/linux-aarch64}",3,"2024-12-18 17:31:50","2024-12-19 06:50:21","2024-12-19 06:49:19","aditew01",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143508",1
"152","2748379403","2748379403","143507","[ROCm] Fix unit test: matmul_offline_mgpu_tunableop","Fixes #141652 

This PR contains:

- Fix for `matmul_offline_mgpu_tunableop`
- Modifications to _checking_tuning_assertions to enable TunableOp if it is disabled. Also moved it into the concurrent futures initializer.


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang","closed","Refatoração","{""module: rocm"",""open source"",Merged,ciflow/trunk,""release notes: linalg_frontend"",ciflow/periodic}",12,"2024-12-18 17:27:49","2024-12-19 20:31:58","2024-12-19 19:48:23","naromero77amd",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143507",1
"153","2746588868","2746588868","143461","Fix torch._refs.tensor error with empty list","Fixes #143216

**Test Result**

**Before**

```python
>>> import torch
>>> torch._refs.tensor([]) 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/zong/code/pytorch/torch/_refs/__init__.py"", line 6614, in tensor
    new_tensor = _internal_new_from_data(
                 ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zong/code/pytorch/torch/_refs/__init__.py"", line 6596, in _internal_new_from_data
    tensor = _recursive_build(inferred_scalar_type, data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zong/code/pytorch/torch/_refs/__init__.py"", line 6545, in _recursive_build
    return torch.stack([_recursive_build(scalarType, item) for item in seq])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects a non-empty TensorList

```

**After**

```python
>>> torch._refs.tensor([]) 
tensor([])
>>> torch._refs.tensor([], device='cuda') 
tensor([], device='cuda:0')
```

```bash
$ pytest test/test_tensor_creation_ops.py -k test_refs_tensor
```

![image](https://github.com/user-attachments/assets/5be4c17a-bea6-4b7b-bec1-b4fcb417a8cd)

```bash
$ lintrunner
```
![image](https://github.com/user-attachments/assets/e8f88f41-78ac-4337-b53f-2e524de2bec0)

cc @ezyang @albanD","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing""}",10,"2024-12-18 02:31:38","2025-01-08 01:30:06","2025-01-08 01:29:04","zeshengzong",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143461",21
"159","2627021622","2627021622","139396","[orm] fix live_memory computation in lpmf algorithm","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139396



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",4,"2024-10-31 14:36:16","2024-12-01 02:20:20","2024-10-31 23:45:33","xuanzhang816",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139396",0
"160","2626797879","2626797879","139390","[MPS] Add support for bf16 autocast","This PR adds support for bf16 autocast. Most of the code and ideas are copied from #99272.

Most of the heavy lifting was done by AI.

Fixes #139386


cc @mcarilli @ptrblck @leslie-fang-intel @jgong5","closed","Refatoração","{triaged,""open source"",""module: amp (automated mixed precision)"",Merged,ciflow/trunk,""release notes: mps""}",4,"2024-10-31 13:00:25","2024-12-16 15:20:32","2024-11-20 19:52:31","hvaara",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139390",20
"161","2663593784","2663593784","140863","Add prepare_obs_or_fq_callback to quantizer","Test Plan: CI.

Differential Revision: D65982003","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""release notes: quantization""}",8,"2024-11-16 01:40:19","2024-11-19 01:14:43","2024-11-19 01:13:41","sxu",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140863",3
"162","2663559715","2663559715","140861","set CUB_VERSION to 200001 for USE_ROCM","Summary:
currently, CUB_VERSION is 0 for USE_ROCM
CUB_VERSION is used for determine whether to use advanced cub APIs for some implementation.

Test Plan:
`buck2 build --flagfile fbsource//arvr/mode/win/vs2022/cpp20/cuda12_5/dev --flagfile fbsource//arvr/mode/cuda/rtx30 fbsource//arvr/libraries/eye/apollo_visualizer:unit_test_apollo_hu_module_capability`

`buck2 build --flagfile fbcode//mode/amd-gpu fbcode//aiplatform/modelstore/checkpointing/pyper:tensor_save_load_utils`

Differential Revision: D63054638




cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",fb-exported,Merged,ciflow/trunk,""release notes: cuda"",ciflow/rocm}",9,"2024-11-16 00:45:32","2024-12-10 02:29:53","2024-12-10 02:28:50","wangzhenict",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140861",24
"163","2663538375","2663538375","140859","[Test clean monitor] DO NOT REVIEW",NULL,"closed","Refatoração","{""module: rocm"",""topic: not user facing""}",1,"2024-11-16 00:17:40","2024-11-26 17:33:59","2024-11-26 17:33:59","yangw-dev",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140859",10
"164","2663500722","2663500722","140857","[Utilization Monitor] input to disable utilization monitor","# Overview
Currently monitor.py produces error only result, this pr introduct disable-monitor option to all *-test.yml. We also like to explore how the monitor code affect benchmark results.

# next steps
- fix the monitor.py
- enable non-benchmark tests with monitor
- investigate benchmark test behavior with monitor background job




cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",Merged,ciflow/trunk,""topic: not user facing""}",4,"2024-11-15 23:48:35","2024-11-18 23:27:08","2024-11-18 23:26:06","yangw-dev",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140857",3
"165","2663482592","2663482592","140856","[ROCm] port CK rowwise F8 from fbgemm","This ports (copies) FBGEMM's implementation from @jwfromm.

https://github.com/pytorch/FBGEMM/tree/main/fbgemm_gpu/experimental/gen_ai/src/quantize/ck_extensions/fp8_rowwise

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @yanbing-j @vkuzo @albanD @kadeng @penguinwu","closed","Refatoração","{""module: rocm"",triaged,""open source"",Merged,Reverted,ciflow/trunk,""release notes: nn"",skip-pr-sanity-checks,rocm,""rocm priority"",""module: float8"",ciflow/rocm,ci-no-td}",24,"2024-11-15 23:35:17","2025-01-02 16:21:54","2025-01-02 16:21:54","jeffdaily",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140856",48
"166","2648895831","2648895831","140259","[ROCm] Correct numerical issues in layer norm backwards kernel","It was raised that the backwards layer norm on AMD was slightly off the accuracy of the equivalent NVIDIA implementation.

On AMD we call into a helper kernel `cuLoadWriteStridedInputs` which processes strided input and accumulates the partial gradients into shared memory.

In this kernel (https://github.com/pytorch/pytorch/pull/87635) we truncated `mean` and `rstd` from T_ACC type to T which causes numerical issues in the warp buffers created in this kernel. This PR will use the correct accumulator type for mean and rstd.

Note: Only AMD call into this call stack for backwards layer norm, so this was not an issue for NV.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",""open source"",Merged,ciflow/trunk,""release notes: cuda"",""rocm priority"",ciflow/rocm}",4,"2024-11-11 10:33:01","2024-11-11 20:45:24","2024-11-11 20:44:20","jataylo",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140259",0
"167","2630301924","2630301924","139546","debug","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139546
* #139227","closed","Refatoração","{""topic: not user facing""}",1,"2024-11-02 07:58:07","2024-12-03 02:12:32","2024-11-02 09:12:29","yifuwang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139546",0
"168","2630282092","2630282092","139545","[c10d] Remove dead Dynamo marker","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139545

Per discussion with @anijain2305, `dynamo_unsupported_distributed_c10d_ops` is not referenced anywhere.
Removing this dead code.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,ciflow/trunk,""release notes: distributed (c10d)""}",3,"2024-11-02 07:25:52","2024-12-03 02:12:09","2024-11-03 00:40:29","kwen2501",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139545",1
"169","2630241677","2630241677","139544","[flex attention][torch.compile] LoweringException: TypeError: cannot determine truth value of Relational","### 🐛 Describe the bug

Here is a code example I tried with:
```
torch._dynamo.config.cache_size_limit = 1000
flex_attention_compile= torch.compile(flex_attention, dynamic=True)
data_type = torch.float16

def scorer_test(score, b, h, q_idx, kv_idx):
    mask = (q_idx >= kv_idx)
    return torch.where(mask, score, -float(""inf""))

device = 'cuda'
B, H, S, D = 288, 16, 20, 64
q = torch.randn(B, H, S, D, device=""cuda"", dtype=data_type, requires_grad=True)
k = torch.randn(B, H, S, D, device=""cuda"", dtype=data_type, requires_grad=True)
v = torch.randn(B, H, S, D, device=""cuda"", dtype=data_type, requires_grad=True)
flex_out = flex_attention_compile(q, k, v, score_mod=scorer_test, block_mask=None)
```

Running the above code, I get an error ""backend='inductor' raised: LoweringException: TypeError: cannot determine truth value of Relational"". But when S is set to 200, the error disappeared. Also if dynamic option is set to False, there is no error even S is set to 20.

Also, if the value of B is changed, 200 may not work either.

I wonder what happens behind this. What should be set to avoid this, if dynamic = True is required in my code?

Thanks in advance!



### Error logs

backend='inductor' raised:
LoweringException: TypeError: cannot determine truth value of Relational
  target: flex_attention
  args[0]: TensorBox(StorageBox(
    InputBuffer(name='primals_3', layout=FixedLayout('cuda', torch.float16, size=[s0, 16, s2, 64], stride=[1024*s2, 64*s2, 64, 1]))
  ))
  args[1]: TensorBox(StorageBox(
    InputBuffer(name='primals_4', layout=FixedLayout('cuda', torch.float16, size=[s0, 16, s2, 64], stride=[1024*s2, 64*s2, 64, 1]))
  ))
  args[2]: TensorBox(StorageBox(
    InputBuffer(name='primals_5', layout=FixedLayout('cuda', torch.float16, size=[s0, 16, s2, 64], stride=[1024*s2, 64*s2, 64, 1]))
  ))
  args[3]: Subgraph(name='sdpa_score0', graph_module=<lambda>(), graph=None)
  args[4]: (TensorBox(StorageBox(
    ComputedBuffer(name='buf0', layout=FixedLayout('cuda', torch.int32, size=[1, 1, 1], stride=[1, 1, 1]), data=Pointwise(
      'cuda',
      torch.int32,
      def inner_fn(index):
          _, _, _ = index
          tmp0 = ops.constant(1, torch.int32)
          return tmp0
      ,
      ranges=[1, 1, 1],
      origin_node=full,
      origins=OrderedSet([full])
    ))
  )), TensorBox(StorageBox(
    ComputedBuffer(name='buf1', layout=FixedLayout('cuda', torch.int32, size=[1, 1, 1, 1], stride=[1, 1, 1, 1]), data=Pointwise(
      'cuda',
      torch.int32,
      def inner_fn(index):
          _, _, _, _ = index
          tmp0 = ops.constant(0, torch.int32)
          return tmp0
      ,
      ranges=[1, 1, 1, 1],
      origin_node=full_default,
      origins=OrderedSet([full_default])
    ))
  )), None, None, TensorBox(StorageBox(
    ComputedBuffer(name='buf4', layout=FixedLayout('cuda', torch.int32, size=[1, 1, 1], stride=[1, 1, 1]), data=Pointwise(
      'cuda',
      torch.int32,
      def inner_fn(index):
          _, _, _ = index
          tmp0 = ops.load(buf2, 0)
          tmp1 = ops.to_dtype(tmp0, torch.int64, src_dtype=torch.int32)
          tmp2 = ops.to_dtype(tmp1, torch.int32, src_dtype=torch.int64)
          return tmp2
      ,
      ranges=[1, 1, 1],
      origin_node=convert_element_type,
      origins=OrderedSet([convert_element_type, sum_1])
    ))
  )), TensorBox(StorageBox(
    ComputedBuffer(name='buf5', layout=FixedLayout('cuda', torch.int32, size=[1, 1, 1, 1], stride=[1, 1, 1, 1]), data=Pointwise(
      'cuda',
      torch.int32,
      def inner_fn(index):
          _, _, _, _ = index
          tmp0 = ops.index_expr(0, dtype=torch.int16)
          tmp1 = ops.to_dtype(tmp0, torch.int64, src_dtype=torch.int16)
          tmp2 = ops.to_dtype(tmp1, torch.int32, src_dtype=torch.int64)
          return tmp2
      ,
      ranges=[1, 1, 1, 1],
      origin_node=convert_element_type_1,
      origins=OrderedSet([sort, convert_element_type_1])
    ))
  )), None, None, 1073741824, 1073741824, Subgraph(name='sdpa_mask0', graph_module=<lambda>(), graph=None))
  args[5]: 0.125
  args[6]: {'ROWS_GUARANTEED_SAFE': False, 'PRESCALE_QK': False, 'OUTPUT_LOGSUMEXP': True}
  args[7]: ()
  args[8]: ()

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
TypeError: cannot determine truth value of Relational

During handling of the above exception, another exception occurred:

torch._inductor.exc.LoweringException: TypeError: cannot determine truth value of Relational
  target: flex_attention
  args[0]: TensorBox(StorageBox(
    InputBuffer(name='primals_3', layout=FixedLayout('cuda', torch.float16, size=[s0, 16, s2, 64], stride=[1024*s2, 64*s2, 64, 1]))
  ))
  args[1]: TensorBox(StorageBox(
    InputBuffer(name='primals_4', layout=FixedLayout('cuda', torch.float16, size=[s0, 16, s2, 64], stride=[1024*s2, 64*s2, 64, 1]))
  ))
  args[2]: TensorBox(StorageBox(
    InputBuffer(name='primals_5', layout=FixedLayout('cuda', torch.float16, size=[s0, 16, s2, 64], stride=[1024*s2, 64*s2, 64, 1]))
  ))
  args[3]: Subgraph(name='sdpa_score0', graph_module=<lambda>(), graph=None)
  args[4]: (TensorBox(StorageBox(
    ComputedBuffer(name='buf0', layout=FixedLayout('cuda', torch.int32, size=[1, 1, 1], stride=[1, 1, 1]), data=Pointwise(
      'cuda',
      torch.int32,
      def inner_fn(index):
          _, _, _ = index
          tmp0 = ops.constant(1, torch.int32)
          return tmp0
      ,
      ranges=[1, 1, 1],
      origin_node=full,
      origins=OrderedSet([full])
    ))
  )), TensorBox(StorageBox(
    ComputedBuffer(name='buf1', layout=FixedLayout('cuda', torch.int32, size=[1, 1, 1, 1], stride=[1, 1, 1, 1]), data=Pointwise(
      'cuda',
      torch.int32,
      def inner_fn(index):
          _, _, _, _ = index
          tmp0 = ops.constant(0, torch.int32)
          return tmp0
      ,
      ranges=[1, 1, 1, 1],
      origin_node=full_default,
      origins=OrderedSet([full_default])
    ))
  )), None, None, TensorBox(StorageBox(
    ComputedBuffer(name='buf4', layout=FixedLayout('cuda', torch.int32, size=[1, 1, 1], stride=[1, 1, 1]), data=Pointwise(
      'cuda',
      torch.int32,
      def inner_fn(index):
          _, _, _ = index
          tmp0 = ops.load(buf2, 0)
          tmp1 = ops.to_dtype(tmp0, torch.int64, src_dtype=torch.int32)
          tmp2 = ops.to_dtype(tmp1, torch.int32, src_dtype=torch.int64)
          return tmp2
      ,
      ranges=[1, 1, 1],
      origin_node=convert_element_type,
      origins=OrderedSet([convert_element_type, sum_1])
    ))
  )), TensorBox(StorageBox(
    ComputedBuffer(name='buf5', layout=FixedLayout('cuda', torch.int32, size=[1, 1, 1, 1], stride=[1, 1, 1, 1]), data=Pointwise(
      'cuda',
      torch.int32,
      def inner_fn(index):
          _, _, _, _ = index
          tmp0 = ops.index_expr(0, dtype=torch.int16)
          tmp1 = ops.to_dtype(tmp0, torch.int64, src_dtype=torch.int16)
          tmp2 = ops.to_dtype(tmp1, torch.int32, src_dtype=torch.int64)
          return tmp2
      ,
      ranges=[1, 1, 1, 1],
      origin_node=convert_element_type_1,
      origins=OrderedSet([sort, convert_element_type_1])
    ))
  )), None, None, 1073741824, 1073741824, Subgraph(name='sdpa_mask0', graph_module=<lambda>(), graph=None))
  args[5]: 0.125
  args[6]: {'ROWS_GUARANTEED_SAFE': False, 'PRESCALE_QK': False, 'OUTPUT_LOGSUMEXP': True}
  args[7]: ()
  args[8]: ()




### Minified repro

_No response_

### Versions

PyTorch version: 2.5.1+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.116-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB

Nvidia driver version: 535.104.12
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      46 bits physical, 57 bits virtual
Byte Order:                         Little Endian
CPU(s):                             64
On-line CPU(s) list:                0-63
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz
CPU family:                         6
Model:                              106
Thread(s) per core:                 1
Core(s) per socket:                 32
Socket(s):                          2
Stepping:                           6
CPU max MHz:                        3400.0000
CPU min MHz:                        800.0000
BogoMIPS:                           5200.00
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bt
s rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_
lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx51
2f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat
pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                     VT-x
L1d cache:                          3 MiB (64 instances)
L1i cache:                          2 MiB (64 instances)
L2 cache:                           80 MiB (64 instances)
L3 cache:                           96 MiB (2 instances)
NUMA node(s):                       2
NUMA node0 CPU(s):                  0-31
NUMA node1 CPU(s):                  32-63
Vulnerability Gather data sampling: Mitigation; Microcode
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.26.3
[pip3] nvidia-cublas-cu12==12.1.3.1
[pip3] nvidia-cuda-cupti-cu12==12.1.105
[pip3] nvidia-cuda-nvrtc-cu12==12.1.105
[pip3] nvidia-cuda-runtime-cu12==12.1.105
[pip3] nvidia-cudnn-cu12==9.1.0.70
[pip3] nvidia-cufft-cu12==11.0.2.54
[pip3] nvidia-curand-cu12==10.3.2.106
[pip3] nvidia-cusolver-cu12==11.4.5.107
[pip3] nvidia-cusparse-cu12==12.1.0.106
[pip3] nvidia-nccl-cu12==2.21.5
[pip3] nvidia-nvjitlink-cu12==12.1.105
[pip3] nvidia-nvtx-cu12==12.1.105
[pip3] torch==2.5.1+cu121
[pip3] torchaudio==2.5.1+cu121
[pip3] torchvision==0.20.1+cu121
[pip3] triton==3.1.0
[conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl                       2023.1.0         h213fc3f_46344    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl-service               2.4.0           py310h5eee18b_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl_fft                   1.3.10          py310h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl_random                1.2.7           py310h1128e8f_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] numpy                     1.26.3                   pypi_0    pypi
[conda] numpy-base                1.26.4          py310hb5e798b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi
[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi
[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi
[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi
[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi
[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi
[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi
[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi
[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi
[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi
[conda] nvidia-nvjitlink-cu12     12.1.105                 pypi_0    pypi
[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi
[conda] torch                     2.5.1+cu121              pypi_0    pypi
[conda] torchaudio                2.5.1+cu121              pypi_0    pypi
[conda] torchvision               0.20.1+cu121             pypi_0    pypi
[conda] triton                    3.1.0                    pypi_0    pypi


cc @ezyang @chauhang @penguinwu @bobrenjc93 @zou3519 @ydwu4 @bdhirsh @yf225 @Chillee @drisspg @yanboliang @BoyuanFeng","closed","Refatoração","{triaged,""oncall: pt2"",""module: dynamic shapes"",""module: higher order operators"",""module: pt2-dispatcher"",""module: flex attention""}",2,"2024-11-02 05:51:47","2024-11-05 18:43:28","2024-11-05 18:43:27","JeffSHF",NULL,NULL,"https://github.com/pytorch/pytorch/issues/139544",3
"170","2630180458","2630180458","139543","[CI] Skip test_cuda_tracker_equivalence for ROCm","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139543

Test fails on ROCm, skipping it for this platform.
Resolves https://github.com/pytorch/pytorch/issues/139515

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""oncall: distributed"",""module: rocm"",Merged,""topic: not user facing"",ciflow/rocm}",3,"2024-11-02 03:55:27","2024-12-03 02:12:06","2024-11-02 15:39:10","kwen2501",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139543",0
"293","2713643105","2713643105","141923","[64-bit][CUDA] Upsample2D 64-bit indexing fix attempt 2","#141831
Block/thread math requires a cast...

cc @ptrblck @msaroufim","closed","Refatoração","{""module: cuda"",triaged,""open source"",Merged,ciflow/trunk,""topic: bug fixes"",""topic: not user facing""}",31,"2024-12-02 23:56:15","2025-01-13 09:30:42","2025-01-04 02:30:40","eqy",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141923",33
"221","2605996596","2605996596","138607","[export] fix test_unbacked_bindings_for_divisible_u_symint","Fixes #ISSUE_NUMBER","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",ciflow/inductor,""release notes: export""}",5,"2024-10-22 16:55:55","2024-11-23 02:06:34","2024-10-23 21:10:07","pianpwk",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138607",1
"171","2630074116","2630074116","139537","[Inductor] Skip coordinate_descent_tuning for mm/bmm decomposition on CPU","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139537

**Summary**
Fix issue: https://github.com/pytorch/pytorch/issues/138823, `coordinate_descent_tuning` doesn't benefit on CPU and prefer lowering `mm`/`bmm` into ATEN kernels or CPP GEMM Template.

**Test Plan**
```
python -u -m pytest -s -v test/inductor/test_cpu_select_algorithm.py -k test_cpp_coordinate_descent_tuning
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",3,"2024-11-02 00:36:22","2024-12-05 02:14:33","2024-11-03 10:10:31","leslie-fang-intel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139537",1
"172","2677578630","2677578630","141186","[Docs] Make links to source link to source","Rewrite [SOURCE] links in the API docs to point to the source file in github repo.


cc @brycebortree @sekyondaMeta @AlannaBurke","closed","Refatoração","{""module: docs"",Merged,""release notes: build"",""topic: docs""}",6,"2024-11-20 23:53:32","2024-12-23 02:07:46","2024-11-22 00:50:22","svekars",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141186",2
"173","2677551862","2677551862","141182","[Doc] Remove mention of Intel Macs","As we are no longer supporting those.
At mention that MPS support needs Ventura+.","closed","Refatoração","{Merged,""topic: not user facing""}",3,"2024-11-20 23:35:08","2024-12-12 22:30:30","2024-11-21 01:05:14","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141182",1
"174","2769042166","2769042166","144202","[ca] dedup node names when AOT bwd graph is reused multiple times","This error started popping up in HUD CA benchmarks:
```python
 File ""/data/users/xmfan/core/b/pytorch/torch/_dynamo/compiled_autograd.py"", line 371, in dce
    self.fx_tracer.graph.eliminate_dead_code(is_impure)
  File ""/data/users/xmfan/core/b/pytorch/torch/fx/graph.py"", line 1862, in eliminate_dead_code
    self.lint()
  File ""/data/users/xmfan/core/b/pytorch/torch/fx/graph.py"", line 1753, in lint
    raise RuntimeError(f""Node redefined name {node.name}!"")
RuntimeError: Node redefined name aot0_expand!
```

We added CA initial capture's renaming (https://github.com/pytorch/pytorch/pull/133148) to help debug issues with AOT backward, but it errors out when we have multiple instances of the same AOT backward. This likely only showed up now because of increased hierarchical graph reuse. I fix it by adding a postfix counter to the node name 

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #144202



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",""module: dynamo"",ciflow/inductor,""module: compiled autograd""}",3,"2025-01-04 23:44:05","2025-01-07 20:24:16","2025-01-07 20:23:12","xmfan",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144202",3
"175","2768972651","2768972651","144199","[mps/BE] Fix linter warning/advice.","Two spaces before an inline comment according to E261.



cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen","closed","Refatoração","{Merged,""topic: not user facing"",""module: mps"",ciflow/mps}",3,"2025-01-04 19:23:45","2025-01-04 20:16:45","2025-01-04 20:15:43","dcci",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144199",0
"176","2768953718","2768953718","144198","[mps/BE] Enable a test that now passes.","After the implementation of floordiv in https://github.com/pytorch/pytorch/commit/464b50dbd7e0692970a2e34aac5b2eeb088741c6 landed, this now passes.

cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: mps"",ciflow/mps,""module: inductor"",ciflow/inductor}",9,"2025-01-04 18:23:33","2025-01-06 03:15:26","2025-01-06 03:14:24","dcci",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144198",2
"177","2768951494","2768951494","144195","[mps/inductor] Add support for floor().","cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: mps"",""module: inductor"",ciflow/inductor}",7,"2025-01-04 18:16:35","2025-01-06 02:08:21","2025-01-06 02:07:19","dcci",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144195",2
"178","2768937877","2768937877","144194","[EZ][BE] Cleanup `test_mps_basic`","- Sort imported tests alphabetically
- Run `add` tests with `check_lowp=False` as it is tested explicitly by parametrization
- Do not hardcode device, but rather use `self.device` property

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",ciflow/mps,""module: inductor"",ciflow/inductor}",6,"2025-01-04 17:36:06","2025-01-04 21:37:45","2025-01-04 21:36:42","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144194",0
"179","2768845758","2768845758","144191","c10::optional -> std::optional in a few places","Test Plan: Sandcastle

Reviewed By: malfet

Differential Revision: D67816636","closed","Refatoração","{fb-exported,""release notes: cpp""}",45,"2025-01-04 13:30:38","2025-01-04 14:00:45","2025-01-04 14:00:45","jeanschmidt",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144191",0
"180","2772058095","2772058095","144317","Move Windows arm64 scripts from pytorch/builder","This PR moves the Windows Arm64 scripts from the builder repository to the main repository. The corresponding PR to pytorch/builder that removes them is here : https://github.com/pytorch/builder/pull/2058","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""release notes: build""}",6,"2025-01-07 07:05:30","2025-01-27 12:57:49","2025-01-23 19:29:32","iremyux",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144317",16
"181","2772054569","2772054569","144316","[CD] Enable profiling for XPU Windows nightly wheels","PR https://github.com/pytorch/pytorch/pull/144034 added profiling support for torch XPU Windows binary, enable it in PyTorch XPU Windows CD
Works for https://github.com/pytorch/pytorch/issues/114850

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @gujinghui @EikanWang @fengyuan14 @guangyey","closed","Refatoração","{""module: windows"",""open source"",Merged,ciflow/trunk,""release notes: releng"",""topic: not user facing"",ciflow/binaries_wheel,""module: xpu""}",7,"2025-01-07 07:03:04","2025-01-14 19:02:33","2025-01-14 19:01:30","chuanqi129",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144316",7
"287","2703769061","2703769061","141794","Fix NOLINTNEXTLINE","Fixes #ISSUE_NUMBER


cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel","closed","Refatoração","{""oncall: jit"",""open source"",Merged,NNC,Reverted,ciflow/trunk,""release notes: jit"",""release notes: quantization"",ci-no-td}",14,"2024-11-29 03:30:57","2024-12-12 00:45:18","2024-12-02 19:22:02","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141794",3
"327","2745568243","2745568243","143393","[CD] Run smoke tests on MacOS wheel","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #143395
* #143394
* __->__ #143393



cc @seemethere","closed","Refatoração","{Merged,""release notes: releng"",""topic: improvements"",ciflow/binaries_wheel}",3,"2024-12-17 17:27:44","2025-01-18 02:04:31","2024-12-17 22:47:10","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143393",0
"182","2771995311","2771995311","144313","[inductor] [bug fix] align `avg_pool` with eager when handling `uint`","Fixes #144310

~~We just need to add a check in lowering~~

updated: we add the error checking in `meta registration`


### UT
```
 pytest -s -v test/inductor/test_torchinductor.py -k test_avg_pool_errors_with_uint
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor""}",9,"2025-01-07 06:21:39","2025-01-17 02:44:45","2025-01-16 23:37:55","shaoyuyoung",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144313",9
"188","2639551643","2639551643","139947","Install torchtune and ao when testing ExecuTorch llama3","A follow-up on https://github.com/pytorch/pytorch/pull/139700, it's good that ET already pins ao, which makes this step much easier on PT CI.","closed","Refatoração","{""release notes: releng"",ciflow/inductor,test-config/executorch,no-runner-experiments}",6,"2024-11-07 00:27:29","2024-11-09 03:09:18","2024-11-09 03:09:18","huydhn",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139947",2
"189","2639513194","2639513194","139940","[BE] Replace `skipIfMPS` with `expectedFailureMPS`","Functionally two decorators are very similar, but one should rely on expectedFailure as much as possible to get signal when something is fixed.
- Move `product_version` variable from `test_mps` to common_utils, but call it `MACOS_VERSION`
- Introduce `skipIfMPSOnMacOS13`  to decorate the hard crashes that happens only on MacOS13 (which at this point will not get any fixes and will be deprecated soon)
- Add `device_type='mps'` to all `skipIfMPS` per https://github.com/pytorch/pytorch/issues/140560","closed","Refatoração","{Merged,""topic: not user facing"",ciflow/mps}",8,"2024-11-06 23:52:33","2024-12-19 02:09:40","2024-11-15 03:48:40","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139940",9
"191","2702269389","2702269389","141772","[AOTI] Temporary files generated by the AOTI package loader are never cleaned up","### 🐛 Describe the bug

The Temporary files generated in `AOTIModelPackageLoader::AOTIModelPackageLoader(
    const std::string& model_package_path,
    const std::string& model_name = ""model"")` are never cleaned and never be reused.

https://github.com/pytorch/pytorch/blob/0f261e8f77c3b374fd8fec3c389f2cefbdabd280/torch/csrc/inductor/aoti_package/model_package_loader.cpp#L279C2-L292C44


When running a benchmark suite, for example:
```
python benchmarks/dynamo/huggingface.py --inference --bfloat16 --accuracy -d cuda  --export-aot-inductor
```
The generated temporary files eg. `/tmp/UUw6E0`,  `/ tmp/HsJ6PV` ... are never cleaned.

![image](https://github.com/user-attachments/assets/c3287c63-7430-454f-b52f-fd093054c7ba)


### Versions


PyTorch version: 2.6.0a0+git6a096a0
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.5 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.31.0
Libc version: glibc-2.35

Python version: 3.9.20 | packaged by conda-forge | (main, Sep 30 2024, 17:49:10)  [GCC 13.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-125-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.4.99
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA A100-PCIE-40GB
Nvidia driver version: 550.120
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True


cc @chauhang @penguinwu @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @desertfire @chenyang78","closed","Refatoração","{triaged,""oncall: pt2"",""oncall: export"",""module: aotinductor""}",2,"2024-11-28 14:03:07","2024-12-06 11:46:49","2024-12-06 11:46:49","etaf",NULL,NULL,"https://github.com/pytorch/pytorch/issues/141772",8
"192","2702006365","2702006365","141769","GroupNorm Doc Typo: Changed 'standard-deviation' to 'variance'","Fixes #141315

It should say ""variance"" because the code calculates variance directly, not its square root(standard deviation)","closed","Refatoração","{""open source"",""topic: not user facing""}",3,"2024-11-28 12:28:41","2024-11-28 19:16:27","2024-11-28 19:16:27","ghost",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141769",0
"193","2702005164","2702005164","141768","Enabled force_shape_pad for test_pad_mm and test_slice_mm_bandwidth_computation","Some tests fail for ROCm build on navi arch because of this check: https://github.com/pytorch/pytorch/blob/f83361b2745e0f3232a211f5d2e5a464e02f6c1f/torch/_inductor/fx_passes/pad_mm.py#L211

There is no need to determine if mm is compute bound for most of the padding tests since they don't specifically test compute bound behavior. We don't have enough empirical data to fine tune this check for AMD gpus yet. I propose to force the shape padding for the tests that we had trouble with to avoid this unnecessary logic path.

Please correct me if I didn't add other tests that can potentially fail with this issue or if I added a test that is dependent on logic below the `force_shape_pad` check here: https://github.com/pytorch/pytorch/blob/f83361b2745e0f3232a211f5d2e5a464e02f6c1f/torch/_inductor/fx_passes/pad_mm.py#L444

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",rocm}",21,"2024-11-28 12:28:07","2024-12-24 11:04:45","2024-12-24 11:03:42","iupaikov-amd",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141768",26
"194","2701910170","2701910170","141767","Check /var/lib/jenkins/workspace exists before setting permissions","Currently, if you run these CI scripts in a non-jenkins environment, they fail due to the folder not existing. This ensures the CI scripts can be re-used in different runners.","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",4,"2024-11-28 11:43:38","2024-12-03 20:57:26","2024-12-03 20:56:23","Mousius",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141767",5
"205","2593131461","2593131461","138124","[DO NOT MERGE] Test docker images","Test CUDNN dockers

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @rec","closed","Refatoração","{""open source"",Stale,ciflow/binaries,ciflow/trunk,""release notes: releng"",""module: dynamo"",ciflow/inductor}",2,"2024-10-16 21:50:11","2025-01-16 19:33:55","2025-01-16 19:33:55","Skylion007",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138124",92
"288","2703699399","2703699399","141792","Fix performance-unnecessary-copy-initialization","Fixes #ISSUE_NUMBER


cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @mingfeima @XiaobingSuper @ashokei @jingxu10","closed","Refatoração","{""oncall: jit"",""module: cpu"",""open source"",better-engineering,Merged,NNC,ciflow/trunk,""release notes: jit""}",9,"2024-11-29 03:03:21","2024-12-12 01:51:55","2024-11-29 22:10:09","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141792",0
"206","2593092081","2593092081","138121","Naive impls for NJT matmul","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138121

Our matmul support is abysmal - let's at least get this working and do it performantly later.

Bonus: implements `bmm` as well.

jagged <-> padded dense conversions are utilized when possible, and an unbind-based fallback otherwise (the former works with torch.compile and the latter doesn't). Some testing is missing because we don't have factory function support yet :(","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-10-16 21:27:11","2024-11-16 02:08:40","2024-10-17 01:31:48","jbschlosser",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138121",1
"195","2701227390","2701227390","141759","[Inductor] Constrain the shape of other tensor for Conv/Linear + broadcast add fusion.","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #141759


Fix https://github.com/pytorch/pytorch/issues/141671.

Summary:
The performance regression of these two timm_models is caused by Conv/Linear + broadcast add fusion run into oneDNN ref path. This PR constrains the shape of other tensor for Conv/Linear + broadcast add fusion to fix this issue.



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Teste de Regressão","{""module: cpu"",""open source"",Merged,Reverted,ciflow/trunk,""module: inductor"",ciflow/inductor,""release notes: inductor"",ci-no-td}",22,"2024-11-28 08:03:32","2025-01-19 02:07:22","2024-12-20 00:36:00","jiayisunx",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141759",22
"196","2701075711","2701075711","141757","Computing mIoU during validation","deleted...","closed","Refatoração","{}",1,"2024-11-28 07:07:31","2024-12-04 06:37:57","2024-12-03 23:20:01","jawi289o",NULL,NULL,"https://github.com/pytorch/pytorch/issues/141757",5
"197","2808298623","2808298623","145576","[Inductor][CPP] fix torch logit decomposition","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145576

**Summary**

Fix issue https://github.com/pytorch/pytorch/issues/145379, current decomposition using `self = torch.clamp(self, lo, hi)` which gives wrong result when `lo` is larger than `hi` comparing to eager implementation: https://github.com/pytorch/pytorch/blob/cd68d549111a8c5d0e056bbb2922e6b37bf88841/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp#L165
Align their behavior in this PR.

**Test Plan**
```
python -u -m pytest -s -v test/inductor/test_cpu_repro.py -k test_torch_logit
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",3,"2025-01-24 01:38:25","2025-01-27 19:39:01","2025-01-27 19:37:53","leslie-fang-intel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145576",3
"198","2808245753","2808245753","145569","[aotinductor] update unbacked symint runtime assertion msg","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145569



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",6,"2025-01-24 00:48:11","2025-01-28 21:44:06","2025-01-28 21:43:00","ColinPeppler",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145569",4
"199","2808233453","2808233453","145566","Advance docker release latest verison to cuda 12.4","Fixed latest tag in ghcr.io to be cuda 12.4 docker image. Todo, Need to add it to : https://github.com/pytorch/builder/blob/main/CUDA_UPGRADE_GUIDE.MD 

Will need to check if we can automate this by introducing cuda_stable variable or something like this.","closed","Refatoração","{Merged,""topic: not user facing""}",3,"2025-01-24 00:35:51","2025-01-24 15:28:30","2025-01-24 15:27:27","atalman",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145566",0
"200","2808233357","2808233357","145565","Refactor fuzzer and add support for Dynamo","## Summary:
Dynamo now works with config fuzzer.

For BE week, we also found and fixed 5 different bugs (in inductor):
- https://github.com/pytorch/pytorch/pull/145426
- https://github.com/pytorch/pytorch/pull/145523
- https://github.com/pytorch/pytorch/pull/145527
- https://github.com/pytorch/pytorch/pull/145532
- https://github.com/pytorch/pytorch/pull/145538

## Test Plan:
New Dynamo Unit tests



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: improvements"",""module: inductor"",ciflow/inductor,""release notes: inductor""}",16,"2025-01-24 00:35:44","2025-01-28 00:45:33","2025-01-28 00:45:32","exclamaforte",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145565",4
"201","2808233008","2808233008","145564","[dynamo] Dynamo doesn't prune dead input cell object","### 🐛 Describe the bug

As title. This is a minimal repro for something @yifuwang ran into.

```python
import torch

@torch.compile(fullgraph=True, backend=""eager"")
def f(x):
    x = x.cos()
    def inner():
        return x.sin()
    return inner()

f(torch.ones(10))
```

Running the above with `TORCH_LOGS=""graph_code""` gives
```
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code] TRACED GRAPH
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]  ===== __compiled_fn_1 =====
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]  /home/ryanguo99/repos/pytorch-39/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]     def forward(self, L_x_: ""f32[10][1]cpu""):
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         l_x_ = L_x_
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]          # File: /home/ryanguo99/scratch/test-dict-cond.py:5 in f, code: x = x.cos()
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         x: ""f32[10][1]cpu"" = l_x_.cos();  l_x_ = None
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]          # File: /home/ryanguo99/scratch/test-dict-cond.py:7 in inner, code: return x.sin()
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         sin: ""f32[10][1]cpu"" = x.sin()
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         return (sin, x)
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]
V0123 16:23:43.105702 1322915 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]
```

Note that `x` doesn't really need to be returned; it's returned because Dynamo conservatively thinks `x` could still be used somewhere else.  This is a very special case that only shows up under the interaction of root-frame input, cell objects, and mutation to the cell objects.

### Error logs

_No response_

### Versions

Python 3.12.5, main 5fd881a5b67

cc @chauhang @penguinwu","closed","Refatoração","{triaged,""oncall: pt2"",dynamo-triage-jan2025}",0,"2025-01-24 00:35:20","2025-01-28 18:28:16","2025-01-28 18:28:16","StrongerXi","StrongerXi",NULL,"https://github.com/pytorch/pytorch/issues/145564",4
"202","2663273125","2663273125","140851","[ROCm][CI] upgrade CI and manywheel docker images to ROCm 6.2.4","Fixes issue of long docker build times in PRs which trigger the docker build in regular PyTorch build jobs eg. https://github.com/pytorch/pytorch/actions/runs/11751388838/job/32828886198. These docker builds take a long time for ROCm6.2 because:
1. They are run on less capable machines (`c5.2xlarge`) instead of the beefier ones on which [docker-build workflows](https://github.com/pytorch/pytorch/blob/924c1fe3f304aa599b823fb549c35b7809f61086/.github/workflows/docker-builds.yml#L50) run (`c5.12xlarge`)
2. ROCm6.2 docker builds enabled building of MIOpen from source, which runs into [timeout of 90mins](https://github.com/pytorch/test-infra/blob/9abd4d95bb0b86d78d1929abcd6046d07e8a5864/.github/actions/calculate-docker-image/action.yml#L171): https://github.com/pytorch/pytorch/actions/runs/11751388838/job/32828886198#step:7:160

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",""open source"",Merged,Reverted,ciflow/trunk,""topic: not user facing"",ciflow/periodic,ciflow/rocm,ci-no-td,ciflow/inductor-rocm}",21,"2024-11-15 21:54:42","2024-11-25 19:20:09","2024-11-23 03:36:30","jithunnair-amd",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140851",8
"203","2662788269","2662788269","140840","Refactor UnflattenedModule's adapt flat args","Test Plan: unblocks model launch

Differential Revision: D66014709","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""release notes: export""}",6,"2024-11-15 18:31:17","2024-11-16 05:10:41","2024-11-16 05:09:40","angelayi",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140840",1
"207","2593052137","2593052137","138114","remove `maybe_enable_thunkify` on nested_tensor.py","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138114
* #125941
* #133337","closed","Refatoração","{""open source"",""topic: not user facing""}",2,"2024-10-16 21:01:12","2024-11-23 02:06:16","2024-10-23 17:09:10","guilhermeleobas",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138114",7
"209","2595550413","2595550413","138245","[Distributed][CI] Add SM guard for compiled tests involving BF16","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #135273
* #137161
* #138174
* __->__ #138245



cc @XilunWu @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,ciflow/trunk,""topic: not user facing"",keep-going}",12,"2024-10-17 19:23:09","2024-11-18 02:11:50","2024-10-18 21:39:42","kwen2501",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138245",1
"210","2595528138","2595528138","138244","[ROCm] pytorch-nightly container is not updated every night","### 🐛 Describe the bug

Hi @powderluv @hliuca ,

Unfortunately the `rocm/pytorch-nightly` container image is not updated every night. As of right now, **the last update from 6 days ago**. Thus users need to add pytorch nightly from pypi inside an older container which may not have the latest dependencies.

<img width=""619"" alt=""image"" src=""https://github.com/user-attachments/assets/c0fc32b9-7c3e-436d-ab2e-6a49b7e9d91e"">


# Current WorkAround

```bash
# use old image as base
FROM rocm/pytorch:rocm6.2_ubuntu22.04_py3.10_pytorch_release_2.3.0 

RUN pip3 uninstall -y torch

RUN pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/rocm6.2
```


### Versions

https://hub.docker.com/r/rocm/pytorch-nightly
<img width=""818"" alt=""image"" src=""https://github.com/user-attachments/assets/f1d66a20-b545-4dcd-8ca9-9b6c08402257"">


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",triaged}",1,"2024-10-17 19:09:57","2024-12-18 22:41:39","2024-12-18 22:41:38","OrenLeung",NULL,NULL,"https://github.com/pytorch/pytorch/issues/138244",62
"211","2595526732","2595526732","138243","Use fresh cache directory in test_cudacodecache","This test frequently times out flakily, for example, https://github.com/pytorch/pytorch/actions/runs/11377972115/job/31654107609#step:22:2376.  I still couldn't reproduce this behavior locally running this multiple times and in parallel.  ~~So, I suspect that the error only shows up when other tests are run in paralel.~~

~~I attempt to run this serially in this PR, once land, I can monitor trunk to see if this helps.~~

Running serially still ends up with a timing out https://github.com/pytorch/pytorch/actions/runs/11391445912/job/31697603438, another try with fresh cache.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",test-config/default,""module: inductor""}",3,"2024-10-17 19:09:05","2024-10-18 05:46:44","2024-10-18 05:45:42","huydhn",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138243",1
"212","2595522778","2595522778","138242","Changes for Draw Things","Note that this is a branch off of the 2.4 release branch

- Change PyTorch's custom dylib loading to support the new structure
- Make GPU support check generic to either iOS or macOS
- Fix mysterious crash in MPSDevice.mm by removing logic that is unnecessary if we assume that the default device is suitable","closed","Refatoração","{""release notes: mps""}",2,"2024-10-17 19:06:38","2024-10-17 19:07:21","2024-10-17 19:07:21","michaeleisel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138242",0
"213","2598478359","2598478359","138371","Remove presere ops","Summary:
CI 
#buildall

Test Plan: CI

Reviewed By: StellarrZ

Differential Revision: D64151426","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""release notes: releng"",ciflow/inductor,suppress-bc-linter}",16,"2024-10-18 22:14:05","2024-11-25 02:12:02","2024-10-25 19:13:58","tugsbayasgalan",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138371",7
"214","2598429040","2598429040","138367","[RELEASE-ONLY CHANGE] Don't try to load cufile","cufile is not available in AlmaLinux/AmazonLinux default configuration. cufile is not actually used currently by default - https://github.com/pytorch/pytorch/pull/133489","closed","Refatoração","{}",3,"2024-10-18 21:39:32","2024-11-22 02:08:45","2024-10-22 17:46:17","kit1980",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138367",4
"215","2598409873","2598409873","138366","[pt2] [testing] Skip inductor_freezing - test_cpp_wrapper_cuda internally","Summary: It's been failing CI since probably forever; skip for now

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",12,"2024-10-18 21:25:15","2024-11-24 02:13:45","2024-10-24 14:40:16","masnesral",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138366",6
"216","2598388891","2598388891","138365","Investigate performance regression in CI benchmarking, caused by #136534","This issue is to track and document progress towards resolving the CPU performance regression in the `functorch_maml_omniglot` benchmark when enabling cpp_wrapper ABI-compatible mode. See https://github.com/pytorch/pytorch/pull/136534 for the changed benchmark threshold value.

To date, I have been unable to reproduce this performance regression outside of the pipeline environment, although investigation is ongoing.

cc: @desertfire 

cc @msaroufim @ezyang @chauhang @penguinwu","closed","Teste de Regressão","{""module: performance"",triaged,""oncall: pt2""}",4,"2024-10-18 21:05:08","2024-10-22 14:29:00","2024-10-22 14:28:31","benjaminglass1","benjaminglass1",NULL,"https://github.com/pytorch/pytorch/issues/138365",4
"217","2603488667","2603488667","138499","Eliminate c10 string_utils","Test Plan: Sandcastle","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""release notes: cpp"",""topic: improvements"",""topic: not user facing""}",6,"2024-10-21 19:37:33","2024-10-23 13:41:41","2024-10-23 13:40:21","r-barnes",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138499",2
"218","2603463836","2603463836","138497","[Traceable FSDP2][CI] Skip more tests on rocm","Some of the test checks doesn't work well with rocm.

Fixes https://github.com/pytorch/pytorch/issues/138409.

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138497



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""oncall: distributed"",""module: rocm"",Merged,ciflow/trunk,""topic: not user facing"",ciflow/rocm}",3,"2024-10-21 19:24:14","2024-11-21 02:09:09","2024-10-21 23:11:04","yf225",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138497",0
"219","2606045678","2606045678","138613","[v2.5.1] Release Tracker","This issue is for tracking cherry-picks to the release branch. Following is [release branch](https://github.com/pytorch/pytorch/tree/release/2.5) for the 2.5.1 release.

We're producing the 2.5.1 release on an accelerated timeline, so we're only considering a small number of critical regression fixes and documentation updates.

Only issues that have ‘cherry-picks’ in this tracker will be considered for the release.","closed","Teste de Regressão","{""oncall: releng"",triaged,""release tracker""}",13,"2024-10-22 17:21:30","2024-10-30 22:52:04","2024-10-30 22:52:04","kit1980",NULL,NULL,"https://github.com/pytorch/pytorch/issues/138613",8
"220","2606032794","2606032794","138610","[Cancel] Add clamp for bool tensor","Fixes #135048


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @manuelcandales @SherlockNoMad @angelayi","closed","Refatoração","{""module: cpu"",""module: error checking"",""open source"",actionable,""module: core aten""}",11,"2024-10-22 17:14:17","2024-10-24 03:12:58","2024-10-22 17:23:28","setbit123",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138610",0
"222","2605956830","2605956830","138605","Use the unicode variant of the Windows API (#47422)","Use the unicode variant of the Windows API in c10/util/Backtrace.cpp
- #47422

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex","closed","Refatoração","{""module: windows"",triaged,""open source"",Merged,""release notes: cpp"",""topic: bug fixes""}",6,"2024-10-22 16:39:02","2024-10-26 17:42:43","2024-10-26 17:41:41","taras-janea",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138605",4
"223","2609538981","2609538981","138723","[PT2][Optimus] Normalize Clamp to use kwargs","Summary: The current clamp normalization does not include torch.clamp where its min and max are not normalized to kwargs, thus the batch fusion of clamp can hit min and max are both empty problem.

Test Plan:
```
buck2 run mode/opt servicelab/ai_ml/auto_tune:local_model_pt2 -- --flow_id 654509735 --test_mode split
```

GPU type: NVIDIA PG509-210
=============Print full analysis for offsite_cvr_oba_optout_dedicated_model================
| Metric             | Value            |
|:-------------------|:-----------------|
| GPU type           | A100             |
| Batch size         | 10               |
| Latency            | 227.13 ms        |
| Model size         | 2322763344 bytes |
| Flops/example      | 1136.52 G        |
| TFLOPS             | 50.04            |
| MFU                | 16.04%           |
| Activation/example | 2722.49 MB       |
I1023 112249.043 local_model_with_pt2.py:25] benchmark results [('batch_size', 10), ('latency_ms', 22712), ('model_size_bytes', 2322763344), ('flops_per_example', 113652), ('tflops_g', 5003), ('mfu', 1603), ('activation_per_example_mb', 272249)

Differential Revision: D64848369




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""module: inductor"",ciflow/inductor,""release notes: dynamo"",""release notes: inductor""}",6,"2024-10-23 18:42:10","2024-10-25 21:06:44","2024-10-25 21:05:41","mengluy0125",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138723",2
"224","2609515782","2609515782","138722","[ROCm] cudagraph explicit sync only after capture_begin()","hipGraphExecDestroy doesn't immediately free memory since rocm6.2.
They wait for next sync point in order to free the memory, this is to ensure that all hipGraphLaunch are finished before we release any memory. 
We need to ensure all async opreations finish before deleting the object.

capture_dev_ variable is used to save the device number when capture_begin() method is called
But CUDAGraph can be created and destroyed without calling capture_begin() method. `capture_dev_ = UNDEFINED_DEVICE;` allows to detect such a case and skip sync

Tests impacted:
test_cuda.py::TestCuda::test_graph_make_graphed_callables_*
distributed/test_c10d_nccl.py::ProcessGroupNCCLTest::test_allreduce_in_cudagraph

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing"",ciflow/periodic,rocm,ciflow/rocm}",12,"2024-10-23 18:33:37","2024-11-20 19:38:28","2024-11-20 19:37:25","dnikolaev-amd",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138722",28
"225","2609412241","2609412241","138720","[Pipelining] Clean up hooks in zero bubble","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138720
* #138735
* #138504
* #138119



cc @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,ciflow/trunk,""release notes: distributed (pipeline)"",""module: pipelining""}",6,"2024-10-23 17:59:36","2024-11-25 02:11:47","2024-10-25 12:06:58","H-Huang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138720",2
"226","2609374230","2609374230","138717","Refactor core algorithm for automatic dynamic shapes","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #139001
* #138740
* __->__ #138717

While working on automatic dynamic PGO (https://github.com/pytorch/pytorch/pull/138052) one abstract property I was looking for out of profile information is that it formed a semilattice: I could join together two profiles and get a merged profile that is consistent with the profiles that I saw in both cases. While working on this data structure that supported joins, I realized that the base automatic dynamic algorithm could be implemented in this way, therefore this refactor.

The basic recipe is that we now support a join operation on FrameStateSizeEntry. Intuitively, if you join two sizes that are equal, you get back that size (join(2, 2) == 2), but if you join two different sizes you get a special singleton auto_dynamic indicating that the size of the tensor is dynamic (join(2, 3) == auto_dynamic). So now, the automatic dynamic algorithm is: (1) compute the FrameStateSizeEntry that corresponds to the concrete values we've seen, and (2) join it into the ambient FrameStateSizeEntry. As a bonus, compiler collectives can buy into the same abstraction (we're simply distributing FrameStateSizeEntry from each node to every other node). For convenience, I also added the necessary `auto_unset` extra state which is the identity element (which makes our semilattice bounded from both top and bottom). Here, join(2, auto_unset) == 2.

While doing this, there was a complication: the infer stride algorithm wasn't technically a semilattice. Here, I did what I suggested in the original code review https://github.com/pytorch/pytorch/pull/130232 which is stop using a heuristic, and instead replicate the stride inference algorithm in automatic dynamic. This means that when I join strides together, I don't join their concrete values, instead, if a stride can be inferred as the contiguous stride for a particular inner dimension, then you represent it as InferStride(dim). There's an example in code which I recommend looking at.

Some other extra things that are happening in this PR:

* I tried to deduplicate the size/stride automatic dynamic logic as much as possible. So hopefully less code to review here.
* I had to reimplement all the logging. For the most part I tried to track the logging as closely to the original as possible, but I think we could be emitting less Chrome events here
* The `marked_dynamic` handling is still preserved as is, but I kind of don't like it and we should figure out how to put it somewhere else

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @rec","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor}",1,"2024-10-23 17:45:51","2024-11-28 02:12:26","2024-10-27 03:08:44","ezyang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138717",4
"227","2612552774","2612552774","138847","Using shape dependent conditionals with export resulted in weird KeyError","### 🐛 Describe the bug

Hi, I know this isn't supposed to work, but just want to report the error. The right way is to use torch.cond, which works pretty well.

repro:
```python
class M(torch.nn.Module):
    def forward(self, x):
        if x.size(0) % 2 == 0:
            return x.clone() * 2
        else:
            return x.clone() * 0

input1 = (torch.rand(size=(4,), device=""cuda""),)
model = M().cuda()

_ = model(*input1)

dynamic_shapes = {
    ""x"": {0: torch.export.Dim.DYNAMIC},
}
ep = torch.export.export(model, input1, dynamic_shapes=dynamic_shapes, strict=False)
```

errors:
```
torch/fx/experimental/symbolic_shapes.py"", line 2441, in solve
    tmp_name = f""_{self._dcp.source_name_to_debug_name[self._dcp.symbol_to_source[s][0].name()]}""
KeyError: ""L['args'][0][0].size()[0]""
```

### Versions

trunk

cc @ezyang @chauhang @penguinwu @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4","closed","Refatoração","{triaged,""oncall: pt2"",""oncall: export""}",4,"2024-10-24 21:01:24","2024-10-31 16:58:20","2024-10-31 16:58:20","henrylhtsang","pianpwk",NULL,"https://github.com/pytorch/pytorch/issues/138847",7
"230","2612352055","2612352055","138841","Elide calls to is_nested in Dynamo-traced graphs","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #138098
* #136792
* __->__ #138841

Before this PR, calling `is_nested` in-graph would result in graph code like the following:
```python
  class GraphModule(torch.nn.Module):
      def forward(self, L_nt_: ""f64[3, s1, 5]"", s1: ""Sym(s1)""):
          l_nt_ = L_nt_

          # Note this useless line!
          getattr_1 = l_nt_.is_nested;  getattr_1 = None

          add: ""f64[3, s1, 5]"" = l_nt_ + 2;  l_nt_ = None
          return (add,)
```

This PR follows what is done for `is_sparse` / `is_quantized`: store it onto `TensorVariable` and have `getattr` calls to `is_nested` return the stored value as a constant. This removes the useless line above from the graph. Note that guarding is handled through tensor type check guards, so no need to guard on `is_nested` status.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @rec","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor}",6,"2024-10-24 19:00:51","2024-11-26 02:09:37","2024-10-26 15:03:35","jbschlosser",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138841",2
"231","2615483202","2615483202","138970","[inductor] Only apply score_fusion_memory_threshold to horizontal fusions","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #138893
* #138533
* #137756
* __->__ #138970

PR #136782 made `x.sum()+1` become two kernels, which hurts compile
times as @ezyang noticed and breaks a lot of the tests in this stack.  This reworks that heuristic to not apply as often.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,""module: inductor"",ciflow/inductor,""release notes: inductor""}",1,"2024-10-26 03:11:52","2024-11-28 02:13:58","2024-10-27 16:31:41","jansel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138970",1
"331","2753647263","2753647263","143691","Apply TorchFix TOR203 fixes","Codemodded via `torchfix . --select=TOR203 --fix`.
This is a step to unblock https://github.com/pytorch/pytorch/pull/141076","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",7,"2024-12-21 00:00:46","2025-01-23 02:04:29","2024-12-23 18:21:06","kit1980",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143691",2
"232","2615464879","2615464879","138966","Use -Weverything","Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @EikanWang @voznesenskym @penguinwu @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @rec @xmfan @yf225","closed","Refatoração","{""module: cpu"",""open source"",NNC,Stale,""topic: not user facing"",""module: dynamo"",ciflow/inductor,""module: compiled autograd""}",3,"2024-10-26 02:31:24","2025-01-25 08:35:19","2025-01-25 08:35:19","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138966",91
"233","2615461478","2615461478","138965","Expliclty avoid recording when should_record_events is false in record_shapeenv_event","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #138965
* #138804

Looking at the function record_shapeenv_event its hard to tell that it does not always run
but we do disable it by setting top level is_recording to True self.should_record_events is false
this makes it more explicit to avoid confusion and overloading is_recording.

alternativley we can rename is_recording to do_no_record.
cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{Merged,ciflow/trunk,""release notes: fx"",fx,ciflow/inductor}",3,"2024-10-26 02:20:44","2024-11-28 02:13:22","2024-10-28 18:12:10","laithsakka",NULL,NULL,"https://github.com/pytorch/pytorch/pull/138965",2
"234","2615379492","2615379492","138960","Write a script to convert PT2 compiler benchmark results to the standardize format for OSS benchmark","As spelled out in https://fburl.com/gdoc/hv2utdvg, PT Dev Infra team is building a benchmark database to power various benchmark use cases.  One of the most important use case is inductor benchmark where the results are written to a set of custom CSV files.  However, as the very first use case, the format of these CSV files are fixed in the number of metrics (columns), which makes extending them not an easy task.  They also don't provide lots of important metadata about how the benchmark is setup.

So, the task here is to write a script to convert PT2 compiler benchmark results in CSV format to the standardize JSON format as defined in https://fburl.com/gdoc/hv2utdvg.  This should be done after the benchmark finishes to ensure backward compatibility with any systems still depend on those CSV files.

Here are some links for bootcampers:

* Workflow https://github.com/pytorch/pytorch/blob/main/.github/workflows/inductor-perf-test-nightly.yml
* Where the perf job runs https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/test.sh#L444, and you can see the CSV outputs there
    * Actual CSV examples can be downloaded from test-report of nightly perf run, i.e. https://hud.pytorch.org/pytorch/pytorch/commit/86d4b7d60b264cae5a04a1b20719bcd7a5752a4c
* Dynamo bench code is at https://github.com/pytorch/pytorch/tree/main/benchmarks/dynamo
* The standard benchmark db format is defined in https://github.com/pytorch/test-infra/pull/5839
 

cc @seemethere @malfet @pytorch/pytorch-dev-infra","closed","Refatoração","{""module: ci"",triaged}",2,"2024-10-26 00:04:51","2025-01-08 18:34:52","2025-01-08 18:34:51","huydhn",NULL,NULL,"https://github.com/pytorch/pytorch/issues/138960",74
"235","2619230051","2619230051","139096","Add a temporary Survey about the search","- Add a link to the new search survey 
- Add .css classes needed for the search banner

cc @brycebortree @sekyondaMeta","closed","Refatoração","{""module: docs"",Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-10-28 18:40:46","2024-11-28 02:15:18","2024-10-28 23:43:27","svekars",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139096",0
"236","2619124935","2619124935","139091","[DO NOT LAND][EXPERIMENT][dynamo] Remove all instances in `dynamo_expected_failures` that are from `test/dynamo`","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139091

This patch investigates #139080.","closed","Refatoração","{Stale,""topic: not user facing""}",5,"2024-10-28 17:55:19","2025-01-26 23:34:03","2025-01-26 23:34:03","StrongerXi",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139091",90
"238","2622473022","2622473022","139230","[inductor] Enable AMD cooperative reduction tests","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139230

Fixes #139099


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""module: rocm"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/rocm}",6,"2024-10-29 22:50:41","2024-12-01 02:20:38","2024-11-01 00:55:17","jansel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139230",3
"258","2647633450","2647633450","140233","Enables static quantization for aarch64","This pull request enables static quantization for aarch64, supporting both convolution and matmul in signed 8-bit (`s8s8s8`) and unsigned 8-bit (`u8s8u8`) formats.


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @SherlockNoMad @EikanWang @wenzhe-nrv","closed","Refatoração","{""module: cpu"",""open source"",""release notes: quantization"",fx}",3,"2024-11-10 20:43:49","2024-11-20 12:08:48","2024-11-20 12:08:48","renato-arantes",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140233",10
"239","2622458761","2622458761","139229","Fix regex in `test_static_inputs_address_mutation_log` for Python 3.12","Otherwise Python 3.12's `re` seems to be unhappy with `re.error: global flags not at the start of the expression at position 113`

cc @mcarilli @ezyang @eellison @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""open source"",Merged,""module: cuda graphs"",ciflow/trunk,""topic: not user facing"",""module: inductor""}",3,"2024-10-29 22:39:52","2024-11-30 02:08:39","2024-10-30 02:36:33","eqy",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139229",1
"240","2622376755","2622376755","139223","[export] Update min_val and max_val to Optional[int] in serialization.","Summary: According to export team's discussion, we are upgrading min_val and max_val to optional fields which shouldn't break BC and allows the schema to express infinity.

Test Plan: buck test mode/opt caffe2/test:test_export -- -r test_serialize_infinite_sym_int

Differential Revision: D65167805","closed","Refatoração","{fb-exported,Merged,Reverted,ciflow/trunk,ciflow/inductor,""release notes: export""}",9,"2024-10-29 21:38:35","2024-10-31 07:46:31","2024-10-30 21:14:19","zhxchen17",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139223",1
"241","2622341717","2622341717","139220","Hook up bf16_gemv_trans to x86 bf16 GEMM","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139220
* #139208
* #139081
* #139558
* #139090
* #139084

This is the big milestone for bf16 and should enable us to close https://github.com/pytorch/torchchat/issues/1253 .

Testing: ran python torchchat.py generate llama3.2-1b --dtype bf16 --device cpu on x86 machine with AVX512-bf16. observed similar tokens/sec with and without MKL path hand-disabled. Also observed speedup from ~2.1 tok/sec to 7.4 tok/sec on x86 machine with only AVX2.

Differential Revision: [D65170967](https://our.internmc.facebook.com/intern/diff/D65170967/)","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""release notes: performance_as_product"",""topic: performance""}",17,"2024-10-29 21:14:37","2024-12-09 02:14:09","2024-11-08 23:24:42","swolchok",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139220",10
"332","2753626149","2753626149","143690","[rpc] Fix unit test after c10::nullopt removal",NULL,"closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""release notes: cpp"",""topic: improvements"",""topic: not user facing""}",4,"2024-12-20 23:25:17","2024-12-23 22:33:35","2024-12-20 23:36:09","yf225",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143690",0
"243","2625760821","2625760821","139366","[Inductor] Fix the Index Put lowering with same input of self and values","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139366

**Summary**
Fix the issue: https://github.com/pytorch/pytorch/issues/138908, the root-cause is in https://github.com/pytorch/pytorch/issues/138908#issuecomment-2449192447

**Test Plan**
```
python -u -m pytest -s -v test/inductor/test_cpu_repro.py -k test_index_put
python -u -m pytest -s -v test/inductor/test_cpu_repro.py -k test_index_add
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",14,"2024-10-31 01:46:57","2025-01-16 02:02:32","2024-12-16 17:07:17","leslie-fang-intel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139366",46
"244","2629397799","2629397799","139489","Avoid overflow in float32-to-int32 test","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #139489

Summary:

Triton has added some integer overflow detection when kernels are compiled with
`debug=True`, and this test results in integer overflow (2.0 is 0x40000000,
times 2 is 0x80000000 which overflows a signed int32).

Assertion `int32 overflow detected for operation mul` failed

Fixes #139479

Test Plan:
```
python inductor/test_torchinductor.py -k test_float32_to_int32_cuda
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor""}",3,"2024-11-01 16:26:54","2024-12-02 02:12:48","2024-11-01 20:22:22","bertmaher",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139489",0
"245","2629354740","2629354740","139485","[AOTI] Forward fix #139458","Summary: A new test added in https://github.com/pytorch/pytorch/pull/139458 only fails in certain CI instance. Skip for now as the failing test has a low priority.

@diff-train-skip-merge (to silent fb bot so that I can land this myself)


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor""}",37,"2024-11-01 16:09:23","2024-12-02 02:13:49","2024-11-01 17:14:42","desertfire",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139485",0
"246","2629331032","2629331032","139482","Fix graph breaks related to specialized float inputs","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #138922
* #138918
* #138666
* #138915
* #139454
* #139457
* #139486
* #139484
* __->__ #139482
* #139451

Fixes issue with timm models where

example_value = 0.09999
proxy.node.target = <built-in function sub>

would fall through to

```
        unimplemented(
            ""torch.* op returned non-Tensor ""
            + f""{typestr(example_value)} {proxy.node.op} {proxy.node.target}"",
            case_name=""unsupported_operator"",
        )
```

and graph break

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor}",3,"2024-11-01 15:57:00","2024-12-03 02:12:44","2024-11-02 21:58:48","bobrenjc93",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139482",1
"257","2647819611","2647819611","140236","Use Wextra-semi","Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{""oncall: distributed"",""oncall: jit"",""module: cpu"",""open source"",Merged,ciflow/trunk,""release notes: distributed (rpc)"",""module: dynamo""}",3,"2024-11-11 00:15:38","2024-12-02 04:35:08","2024-11-13 02:15:18","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140236",2
"248","2632664282","2632664282","139621","[ROCm] [Upstream Triton] Flex attention `Assertion idx < size()' failed.`","### 🐛 Describe the bug

Encountered while testing for preview release/2.6 builds as part of https://github.com/pytorch/pytorch/issues/139175

```python test_flex_attention.py -k ""test_load_from_bias_seq_only_float16```

**Traceback**
> test_load_from_bias_seq_only_float16 (__main__.TestFlexAttention) ... python: /root/.triton/llvm/llvm-b5cc222d-ubuntu-x64/include/llvm/ADT/SmallVector.h:291: T& llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2> >::operator[](llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2> >::size_type) [with T = unsigned int; <template-parameter-1-2> = void; llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2> >::reference = unsigned int&; llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2> >::size_type = long unsigned int]: Assertion `idx < size()' failed.

Reproducer:
https://gist.github.com/jataylo/dfd4179d26e8fd5019a81189949e6f3b

### Versions

https://github.com/pytorch/pytorch/pull/139206

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd @bertmaher @int3 @davidberard98 @nmacchioni @chenyang78 @embg @peterbell10 @aakhundov","closed","Refatoração","{""module: rocm"",triaged,""upstream triton""}",4,"2024-11-04 12:16:44","2024-11-08 18:16:19","2024-11-08 18:16:18","jataylo","SamGinzburg",NULL,"https://github.com/pytorch/pytorch/issues/139621",4
"249","2632247779","2632247779","139613","Optimize peak memory for flash _scaled_dot_product_attention_math (#139612)","Fixes #139612  

@drisspg @albanD","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing""}",9,"2024-11-04 09:11:22","2024-11-07 02:26:44","2024-11-07 02:25:42","1274085042",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139613",3
"264","2656764514","2656764514","140606","[aoti] C++ compiler redefinition error on XlmRoberta","Internal link: https://docs.google.com/document/d/1VwApEwh37eURfCQVMWR6p4QgRsBIAqpHUsvAsFvzH_g/edit?usp=sharing

Example error:
```
/tmp/torchinductor_williamwen/czzxfezqogcw2lqab2nhb2hhnkqxnoswdukzwpcofexqaphgfely/cu3d4wlyj2eb5zbgojmii2oxd4titxlttt2cv74wgatgmodmhsr2.cpp: In member function ‘void torch::aot_inductor::AOTInductorModel::run_impl(AtenTensorOpaque**, AtenTensorOpaque**, torch::aot_inductor::DeviceStreamType, AOTIProxyExecutorHandle)’:
...
/tmp/torchinductor_williamwen/czzxfezqogcw2lqab2nhb2hhnkqxnoswdukzwpcofexqaphgfely/c5zdkk6ugqujjkoaxc3zxvvvr42nsb3it64piuu3e4oanp4s7pzm.cpp:18808:10: error: redeclaration of ‘bool buf6_scalar’
18808 |     bool buf6_scalar;
      |          ^~~~~~~~~~~
/tmp/torchinductor_williamwen/czzxfezqogcw2lqab2nhb2hhnkqxnoswdukzwpcofexqaphgfely/c5zdkk6ugqujjkoaxc3zxvvvr42nsb3it64piuu3e4oanp4s7pzm.cpp:4890:10: note: ‘bool buf6_scalar’ previously declared here
 4890 |     bool buf6_scalar;
      |          ^~~~~~~~~~~
...
Traceback (most recent call last):
  File ""/data/users/williamwen/fairseq-env/lib/python3.10/site-packages/torch/_inductor/codecache.py"", line 1714, in run_command_and_check
    subprocess.check_call(cmd)
  File ""/data/users/williamwen/fairseq-env/lib/python3.10/subprocess.py"", line 369, in check_call
    raise CalledProcessError(retcode, cmd)
```

More details including repro are in the internal link.

cc @ezyang @chauhang @penguinwu @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @desertfire @chenyang78","closed","Refatoração","{""oncall: pt2"",""oncall: export"",""module: aotinductor"",empathy-day}",6,"2024-11-13 20:52:59","2025-01-10 22:03:38","2024-12-04 01:56:12","williamwen42","yushangdi",NULL,"https://github.com/pytorch/pytorch/issues/140606",21
"250","2632226933","2632226933","139612","Optimize peak memory for flash _scaled_dot_product_attention_math","### 🚀 The feature, motivation and pitch

  

Currently, `_scaled_dot_product_attention_math` uses `at::_safe_softmax(attn, -1)`, which increases memory usage compared to the previous `at::softmax(attn, -1)` implementation. We found that in some large model applications, the peak memory exceeds the memory capacity due to `_safe_softmax`, resulting in OOM.
The peak memory usage of `_scaled_dot_product_attention_math` is as follows:

**PT2.1**
![image](https://github.com/user-attachments/assets/02d99554-26ee-4979-87c5-a6abedf302de)

pt=2.1.0a0+git7bcf7da: used=2360.94MB  

**PT2.6**  
![image](https://github.com/user-attachments/assets/428fa869-49f4-4884-ad71-6bd37eec669f)  
pt=2.6.0a0+gitf4ee5a2: used=3898.94MB

Test Scripts：  
```python
import torch
from torch.nn.attention import sdpa_kernel, SDPBackend
from torch.profiler import profile, ProfilerActivity

device = ""cuda""

torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)
size = (1,32, 4096, 128)
q = torch.randn(size, device=device, dtype=torch.half, requires_grad=True)
k = torch.randn(size, device=device, dtype=torch.half, requires_grad=True)
v = torch.randn(size, device=device, dtype=torch.half, requires_grad=True)
with sdpa_kernel(SDPBackend.MATH):
    torch.cuda.memory._record_memory_history()
    res=torch.nn.functional.scaled_dot_product_attention(q, k, v)
    torch.cuda.memory._dump_snapshot(""./sdpa_math_fwd_where_2.6.pickle"")
    free, total = map(lambda x: x/2**20, torch.cuda.mem_get_info())
    used=total-free
    print(f'pt={torch.__version__}: {used=:0.2f}MB, {free=:0.2f}MB, {total=:0.2f}MB')
```

@drisspg @albanD 

### 

### Alternatives

I want to implement `_safe_softmax` in the following way, `at::where_out` reduces peak memory usage by reusing  memory
```cpp
Tensor _safe_softmax(
    const Tensor& self,
    int64_t dim,
    std::optional<ScalarType> dtype) {
  auto out = at::softmax(self, dim, dtype);
  const auto neg_inf = at::scalar_tensor(-std::numeric_limits<float>::infinity(), at::TensorOptions().dtype(out.dtype()).device(out.device()));
  const auto masked = self.eq(neg_inf);
  const auto masked_rows = all(masked, dim, true);
  const auto zero = at::scalar_tensor(0.0, at::TensorOptions().dtype(out.dtype()).device(out.device()));
  // reuse storage for out
 return at::where_out(out, masked_rows, zero, out);
}
```

The peak memory usage of `_scaled_dot_product_attention_math` is as follows:
![image](https://github.com/user-attachments/assets/ec0c74c6-06c4-4722-9b1b-cbc824d5f05e)

pt=2.6.0a0+gitf4ee5a2: used=2874.94MB

### Additional context

_No response_

cc @ptrblck @msaroufim @drisspg @mikaylagawarecki","closed","Teste de Regressão","{""module: cuda"",""module: memory usage"",triaged}",1,"2024-11-04 09:00:55","2024-11-07 02:25:42","2024-11-07 02:25:41","1274085042",NULL,NULL,"https://github.com/pytorch/pytorch/issues/139612",3
"252","2637160988","2637160988","139857","[PT2][Optimus] fix the default alpha and beta values","Summary:
We noticed that the default coefficient values for beta and alpha should be int 1, instead of float 1.0, which will cause error when the inputs for the add are int types.

More contex:

https://fb.workplace.com/groups/1075192433118967/permalink/1539142760057263/

Test Plan:
# local reproduce
```
buck2 run mode/opt scripts/shuaiyang:test -- --optimus --flow_id 660724017 2>&1 | tee ~/local_run_shuai_660724017.txt
```

trace link: https://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree/traces/mengluy/2024-11-05-21-18-17/trace.json.gz&bucket=gpu_traces

# E2E

before fix:
f660724017

after fix:

Differential Revision: D65521638




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""module: inductor"",ciflow/inductor,""release notes: dynamo"",""release notes: inductor""}",5,"2024-11-06 06:27:08","2024-11-07 19:13:28","2024-11-07 19:12:26","mengluy0125",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139857",1
"254","2637077201","2637077201","139853","[FR] Enable best effort parital analysis and verbose mode for trace printing","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #140082
* __->__ #139853

Based on user feedback, we want to enable two things for FR analysis script:
1. Print out more information when verbose is specified.
2. Perform best effort based analysis when not all ranks have FR trace dumped.

Differential Revision: [D65516081](https://our.internmc.facebook.com/intern/diff/D65516081/)

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",fb-exported,Merged,ciflow/trunk,""topic: not user facing"",suppress-api-compatibility-check,suppress-bc-linter}",19,"2024-11-06 05:29:13","2024-12-12 02:12:04","2024-11-11 14:38:34","fduwjj",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139853",5
"255","2640446610","2640446610","139993","DISABLED test_comprehensive_bernoulli_cuda_float16 (__main__.TestInductorOpInfoCUDA)","Platforms: inductor, linux



This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_comprehensive_bernoulli_cuda_float16&suite=TestInductorOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/32639235068).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_comprehensive_bernoulli_cuda_float16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py"", line 1152, in test_wrapper
    return test(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py"", line 1434, in only_fn
    return fn(self, *args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2199, in wrapper
    fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py"", line 1229, in dep_fn
    return fn(slf, *args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py"", line 1229, in dep_fn
    return fn(slf, *args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py"", line 1229, in dep_fn
    return fn(slf, *args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 1592, in wrapper
    fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 1528, in wrapper
    fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/unittest/mock.py"", line 1379, in patched
    return func(*newargs, **newkeywargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py"", line 955, in inner
    raise e
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py"", line 947, in inner
    fn(self, device, dtype, op)
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py"", line 1193, in test_comprehensive
    raise e
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py"", line 1153, in test_comprehensive
    self.check_model_gpu(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor.py"", line 613, in check_model_gpu
    check_model(
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor.py"", line 564, in check_model
    actual_grad = compute_grads(example_inputs, kwargs, actual, grads)
  File ""/var/lib/jenkins/workspace/test/inductor/test_torchinductor.py"", line 351, in compute_grads
    return torch.autograd.grad(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 496, in grad
    result = _engine_run_backward(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/graph.py"", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/function.py"", line 307, in apply
    return user_fn(self, *args)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 1705, in backward
    return impl_fn()
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 1695, in impl_fn
    out = CompiledFunction._backward_impl(ctx, all_args)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 2006, in _backward_impl
    CompiledFunction.compiled_bw = aot_config.bw_compiler(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/backends/common.py"", line 51, in _wrapped_bw_compiler
    return disable(disable(bw_compiler)(*args, **kwargs))
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 721, in _fn
    return fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py"", line 95, in wrapper_function
    return function(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py"", line 1650, in bw_compiler
    return inner_compile(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py"", line 587, in compile_fx_inner
    return wrap_compiler_debug(_compile_fx_inner, compiler_name=""inductor"")(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py"", line 102, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py"", line 744, in _compile_fx_inner
    compiled_graph = FxGraphCache.load(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py"", line 1514, in load
    compiled_graph = compile_fx_fn(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py"", line 651, in codegen_and_compile
    compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py"", line 1015, in fx_codegen_and_compile
    compiled_graph = CompiledFxGraph(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py"", line 1657, in __init__
    with open(graph.cache_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpx5wz4xuy/ef/cef4sucerrtrcjuxis44qzy6gdctjwgbf2da3iprr7tggy6plq36.py'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3057, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3057, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py"", line 460, in instantiated_test
    result = test(self, **param_kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 1592, in wrapper
    fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py"", line 1164, in test_wrapper
    raise e_tracked from e
Exception: Caused by sample input at index 0: SampleInput(input=Tensor[size=(3,), device=""cuda:0"", dtype=torch.float16], args=(), kwargs={}, broadcasts_input=False, name='')

To execute this test, run the following from the base repo dir:
    PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=0 python test/inductor/test_torchinductor_opinfo.py TestInductorOpInfoCUDA.test_comprehensive_bernoulli_cuda_float16

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `inductor/test_torchinductor_opinfo.py`

cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @clee2000 @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @aakhundov","closed","Refatoração","{""high priority"",triaged,""module: flaky-tests"",skipped,""oncall: pt2"",""module: inductor""}",10,"2024-11-07 09:42:17","2024-12-03 21:41:51","2024-12-03 21:41:51","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/139993",26
"256","2640347847","2640347847","139988","[aarch64] build cuda 12.6 manywheel dockers","Add Builds sbsa 12.6 manywheel dockers to workflow
Related to #138440 
cc @atalman","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing""}",14,"2024-11-07 09:05:34","2024-12-07 15:39:47","2024-12-07 15:38:44","tinglvv",NULL,NULL,"https://github.com/pytorch/pytorch/pull/139988",30
"259","2647502820","2647502820","140231","Add kwen2501 to CODEOWNERS of c10d backend APIs","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #140231","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-11-10 18:37:06","2024-12-15 02:15:54","2024-11-14 23:58:54","kwen2501",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140231",4
"262","2653812989","2653812989","140487","Add a dashboard for torchao","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #140487

Summary:
trying to add a similar dashboard as https://hud.pytorch.org/benchmark/llms?repoName=pytorch%2Fpytorch

Test Plan:
Run locally:

install torchao
```
pip install --no-use-pep517 --user ""git+https://github.com/pytorch/ao.git@f96e5ec9d30f778c2fb49d7ff9536b8016e1d408""
```
run test:
```
cd benchmarks/torchao
python benchmark.py
```



Check after landing
https://hud.pytorch.org/benchmark/llms?repoName=pytorch%2Fpytorch

Reviewers:

Subscribers:

Tasks:

Tags:","closed","Refatoração","{""release notes: releng"",ciflow/inductor}",2,"2024-11-13 01:16:15","2024-12-21 02:04:32","2024-11-19 22:09:58","jerryzh168",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140487",6
"263","2656813952","2656813952","140611","Don't pass credentials explicitly to sccache","sccache-0.2.14 can query it thru IMDSv1 and sccache-0.8.2 can do it thru v2 (or may be just use trust relationships between host and bucket","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-11-13 21:17:57","2024-12-12 22:31:19","2024-11-14 04:44:57","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140611",1
"265","2656685700","2656685700","140601","lerp_ doesn't correctly type promote","### 🐛 Describe the bug
Found when trying to compile the single tensor Adam/W with a tensor beta1 in https://github.com/pytorch/pytorch/pull/134171
```
import torch

x = torch.ones(2, 2, dtype=torch.float64)
w = torch.ones(2, 2, dtype=torch.float64)
s = torch.tensor(2.2) 

z = x.lerp_(w, s)
```

The scalar `s` should be able to be type promoted here.

### Versions

f204438a79761b028a7ba9ca3cd1126b9d208298

cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar @nairbv @mruberry @jianyuh @nikitaved @pearu @walterddr @xwang233 @Lezcano","closed","Refatoração","{""module: optimizer"",triaged,""module: type promotion"",""module: linear algebra"",actionable}",1,"2024-11-13 20:30:38","2025-01-24 01:18:23","2025-01-24 01:18:22","mlazos",NULL,NULL,"https://github.com/pytorch/pytorch/issues/140601",72
"266","2659873749","2659873749","140741","catch tensor.numel() == 0 in nan detector","Context: we are trying to pass an empty tensor through the system now (sometimes;... its an edge case); and it seems to cause all_reduce to seg fault, which is unexpected to me

Deep Shah and Pavan identified the issue, I'm just pushing for a fix :)

Test Plan: idk what i'm doing here, someone help

Reviewed By: shuqiangzhang

Differential Revision: D65956095




cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",fb-exported,Merged,""release notes: distributed (c10d)""}",12,"2024-11-14 19:38:56","2024-11-15 05:04:27","2024-11-15 05:03:23","HarounH",NULL,NULL,"https://github.com/pytorch/pytorch/pull/140741",1
"272","2670219427","2670219427","140986","Improves wording and grammar on the documentation for nn/module.py","### 📚 The doc issue

I have a local branch which improves the wording and grammar of part of the docs for nn/module.py.  Mostly they are esthetic changes, i.e. most readers probably find the docs comprehensible without them.  However I picked up on this as a good second issue to work on, and I think the improvements still have some value.

BTW I forgot to first report the issue prior to starting changes on my local branch which is why I am proceeding the other way around.  I will upload a PR for this shortly.

### Suggest a potential alternative/fix

N/A

cc @svekars @brycebortree @sekyondaMeta @AlannaBurke @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki","closed","Refatoração","{""module: docs"",""module: nn"",triaged}",1,"2024-11-18 23:03:05","2024-11-21 23:40:45","2024-11-21 23:40:45","fmgblackwolf",NULL,NULL,"https://github.com/pytorch/pytorch/issues/140986",3
"273","2675357158","2675357158","141121","Failure in generating a kernel with 3 tile groups","### 🐛 Describe the bug

Trying to generate a pointwise add kernel with 3 tiling groups fails.

Reproducer:

```python
import torch
from torch._inductor.utils import run_and_get_triton_code
from torch._inductor import config

import functools

config.triton.max_tiles = 3
config.triton.prefer_nd_tiling = True

full_size, view_size, num_block_pointers, num_tiles = (
    (5, 5, 5, 5, 5),
    (3, 3, 5, 3, 5),
    1,
    2,
)

GPU_TYPE = ""cuda""


def get_input() -> torch.Tensor:
    device = torch.device(GPU_TYPE)
    full = torch.randn(full_size).to(device)
    return torch.as_strided(full, view_size, full.stride())


a, b = get_input(), get_input()

opt_fn = torch.compile(functools.partial(torch.add))
code = run_and_get_triton_code(opt_fn, a, b)

```

This error occurs because a LoopBody iteration prefix is z, which matches with the prefix of a range tree for the z dimension. 

Interestingly a z prefix type is banned here: [pytorch/torch/utils/_sympy/symbol.py at c9c8370feb80290dd47f30395a51902265ac0142 · pytorch/pytorch](https://github.com/pytorch/pytorch/blob/c9c8370feb80290dd47f30395a51902265ac0142/torch/utils/_sympy/symbol.py#L62) . And there is no corresponding ZBLOCK here: [pytorch/torch/utils/_sympy/symbol.py at c9c8370feb80290dd47f30395a51902265ac0142 · pytorch/pytorch](https://github.com/pytorch/pytorch/blob/c9c8370feb80290dd47f30395a51902265ac0142/torch/utils/_sympy/symbol.py#L49)


### Error logs

```shell
 def codegen_sync(self):

/usr/local/lib/python3.10/dist-packages/torch/_inductor/codegen/simd.py in codegen_node(self, node)
   1199         schedule_log.debug(""Schedule:\n %s"", node_schedule)
   1200 
-> 1201         return self.codegen_node_schedule(
   1202             SIMDKernelFeatures(node_schedule, numel, rnumel)
   1203         )

/usr/local/lib/python3.10/dist-packages/torch/_inductor/codegen/simd.py in codegen_node_schedule(self, kernel_features)
   1239         )
   1240         for kernel in kernels:
-> 1241             self.codegen_node_schedule_with_kernel(node_schedule, kernel)
   1242         MultiKernel.merge_workspaces_inplace(kernels)
   1243         for kernel in kernels:

/usr/local/lib/python3.10/dist-packages/torch/_inductor/codegen/simd.py in codegen_node_schedule_with_kernel(self, node_schedule, kernel)
   1318                     all_indexing.update(
   1319                         dict.fromkeys(
-> 1320                             node._body.indexing_from_args(index_vars).values()
   1321                         )
   1322                     )

/usr/local/lib/python3.10/dist-packages/torch/_inductor/loop_body.py in indexing_from_args(self, indices)
    391         index = [*itertools.chain.from_iterable(indices)]
    392         assert len(index) == len(self.var_ranges), (index, self.var_ranges)
--> 393         assert all(
    394             v not in self.var_ranges for v in index
    395         ), f""{self.var_ranges=}, {indices=}""

BackendCompilerFailed: backend='inductor' raised:
AssertionError: self.var_ranges={z0: 3, z1: 15, z2: 15}, indices=[[z0, y1, x2], []]

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information
```

### Versions

```shell
Collecting environment information...
PyTorch version: 2.6.0.dev20241120+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 14.0.0-1ubuntu1.1
CMake version: version 3.30.5
Libc version: glibc-2.35

Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-6.1.85+-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.2.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 535.104.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        46 bits physical, 48 bits virtual
Byte Order:                           Little Endian
CPU(s):                               2
On-line CPU(s) list:                  0,1
Vendor ID:                            GenuineIntel
Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz
CPU family:                           6
Model:                                79
Thread(s) per core:                   2
Core(s) per socket:                   1
Socket(s):                            1
Stepping:                             0
BogoMIPS:                             4399.99
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
Hypervisor vendor:                    KVM
Virtualization type:                  full
L1d cache:                            32 KiB (1 instance)
L1i cache:                            32 KiB (1 instance)
L2 cache:                             256 KiB (1 instance)
L3 cache:                             55 MiB (1 instance)
NUMA node(s):                         1
NUMA node0 CPU(s):                    0,1
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Mitigation; PTE Inversion
Vulnerability Mds:                    Vulnerable; SMT Host state unknown
Vulnerability Meltdown:               Vulnerable
Vulnerability Mmio stale data:        Vulnerable
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Vulnerable
Vulnerability Spec rstack overflow:   Not affected
Vulnerability Spec store bypass:      Vulnerable
Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Vulnerable

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] nvidia-cublas-cu12==12.4.5.8
[pip3] nvidia-cuda-cupti-cu12==12.4.127
[pip3] nvidia-cuda-nvrtc-cu12==12.4.127
[pip3] nvidia-cuda-runtime-cu12==12.4.127
[pip3] nvidia-cudnn-cu12==9.1.0.70
[pip3] nvidia-cufft-cu12==11.2.1.3
[pip3] nvidia-curand-cu12==10.3.5.147
[pip3] nvidia-cusolver-cu12==11.6.1.9
[pip3] nvidia-cusparse-cu12==12.3.1.170
[pip3] nvidia-cusparselt-cu12==0.6.2
[pip3] nvidia-nccl-cu12==2.21.5
[pip3] nvidia-nvjitlink-cu12==12.4.127
[pip3] nvidia-nvtx-cu12==12.4.127
[pip3] nvtx==0.2.10
[pip3] optree==0.13.1
[pip3] pynvjitlink-cu12==0.4.0
[pip3] pytorch-triton==3.1.0+cf34004b8a
[pip3] torch==2.6.0.dev20241120+cu124
[pip3] torchaudio==2.5.1+cu121
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.20.1+cu121
[conda] Could not collect
```

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @aakhundov @ezyang @blaine-rister","closed","Refatoração","{triaged,""oncall: pt2"",""module: inductor""}",2,"2024-11-20 10:36:38","2024-11-28 01:34:30","2024-11-28 01:34:30","kundaMwiza","jansel",NULL,"https://github.com/pytorch/pytorch/issues/141121",8
"276","2680559445","2680559445","141263","[ONNX] Remove test_save_with_without_initializer test","The test is flaky and obsolete. So remove.

Fixes https://github.com/pytorch/pytorch/issues/125020","closed","Refatoração","{""module: onnx"",""open source"",Merged,ciflow/trunk,""topic: not user facing""}",3,"2024-11-21 18:59:13","2024-12-22 02:11:13","2024-11-21 22:06:18","justinchuby",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141263",0
"277","2680381519","2680381519","141255","[POC] Make nonzero on meta not fail with flop count","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #141255

Signed-off-by: Edward Z. Yang <ezyang@meta.com>","closed","Refatoração","{}",2,"2024-11-21 17:52:30","2024-12-25 02:04:26","2024-11-24 14:02:58","ezyang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141255",3
"279","2684904477","2684904477","141398","E2E composability testing","Add 3D(pp+tp+fsdp) test `test_3d_with_tp_dp_pp` at test_pp_compodability
Currently provide @parametrize on 
""ScheduleClass"" for pp in [ScheduleGPipe, Schedule1F1B, ScheduleInterleaved1F1B, ScheduleLoopedBFS, ScheduleInterleavedZeroBubble]
""MixedPrecisionParam"" for fsdp in [torch.bfloat16, torch.float32]

Future work:
1. add fp8
2. add cp(context parallelism) to enable 4D test

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""oncall: distributed"",Merged,Reverted,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor,ci-no-td}",19,"2024-11-22 23:22:54","2024-12-12 04:20:34","2024-12-12 04:19:31","mori360",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141398",20
"280","2684900647","2684900647","141397","[ROCM] Enable *_load_dwordx4 ISA for BFloat16 and Half.","Remove input_vec_size constexpr and move it to template parameter. This enables generation of vectorized loads in ROCm AMDGPU backend.


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",triaged,""open source"",Merged,ciflow/trunk,""topic: not user facing"",rocm,ciflow/rocm,ciflow/inductor-rocm}",14,"2024-11-22 23:18:41","2024-12-12 03:28:55","2024-12-12 03:27:51","carlobertolli",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141397",20
"281","2684869042","2684869042","141394","[ROCm] add gfx1101 to wheels","Also simplify the gfx arch logic since we assume a minimum ROCm 6.0 version today.

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",""open source"",""topic: not user facing"",ciflow/rocm}",3,"2024-11-22 22:59:18","2024-11-27 17:08:36","2024-11-27 17:08:35","jeffdaily",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141394",5
"282","2684763484","2684763484","141391","Revert ""[triton] Update pin for PyTorch 2.6/Triton 3.2 (#139206)""","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #141391

This reverts commit c93e57efac091f246b599b4fcdc189ed94753b43.

Reverted https://github.com/pytorch/pytorch/pull/139206 on behalf of https://github.com/atalman due to Will revert and reland skipping xpu builds ([comment](https://github.com/pytorch/pytorch/pull/139206#issuecomment-2494437857))

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov","closed","Teste de Regressão","{ciflow/trunk,""release notes: releng"",""module: inductor"",ciflow/inductor}",2,"2024-11-22 22:26:10","2024-12-23 02:08:19","2024-11-22 22:40:34","ColinPeppler",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141391",0
"283","2692415974","2692415974","141523","[ROCm] Remove gfx906 from CI docker build","cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",""open source"",Merged,""topic: not user facing"",ciflow/rocm}",3,"2024-11-25 22:15:14","2024-12-27 02:06:40","2024-11-25 22:23:31","jithunnair-amd",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141523",0
"284","2692377122","2692377122","141522","handle sympy.oo in bitwise_and/or value_ranges","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #141522
* #138777

An internal test is failing due to not handling `sympy.oo` properly in bitwise_and/or value_ranges: [T208684142](https://www.internalfb.com/intern/tasks/?t=208684142). I don't know how to repro this - seems like this requires inductor to trigger as well.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @penguinwu @bobrenjc93","closed","Refatoração","{""module: cpu"",Merged,ciflow/trunk,""release notes: fx"",""topic: bug fixes"",""module: dynamic shapes"",ciflow/inductor}",7,"2024-11-25 21:52:06","2024-12-27 02:05:57","2024-11-26 20:01:33","williamwen42",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141522",1
"285","2692283909","2692283909","141519","[SDPA-CPU] Fix Edge case w/ fused flash cpu kernel","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #141519

Fixes https://github.com/pytorch/pytorch/issues/141128


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @mikaylagawarecki","closed","Refatoração","{""module: cpu"",Merged,ciflow/trunk,""topic: not user facing"",""module: edge cases"",ciflow/inductor}",9,"2024-11-25 21:26:01","2024-12-28 02:03:45","2024-11-26 22:07:59","drisspg",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141519",1
"286","2692278470","2692278470","141518","[BE] Use `torch.special.expm1`","Instead of `torch.exp(x)-1`, as suggested by TorchFix","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",6,"2024-11-25 21:23:15","2024-12-12 22:30:05","2024-11-26 01:47:14","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141518",1
"333","2753597236","2753597236","143686","[Codemod][AddExplicitStrictExportArg] caffe2/benchmarks/dynamo","Reviewed By: avikchaudhuri


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""topic: not user facing"",test-config/default,""module: dynamo"",ciflow/inductor}",26,"2024-12-20 22:49:18","2024-12-23 22:33:26","2024-12-21 19:56:58","gmagogsfm",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143686",1
"334","2753579064","2753579064","143685","[BE] Remove gcc-5 workaround for unused args","ditto","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",6,"2024-12-20 22:29:15","2025-01-23 02:04:27","2024-12-21 00:18:17","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143685",1
"289","2702917002","2702917002","141780","[MPS] Convert `channels_last_3d` to `contiguous` for input tensor in `nn.Conv3d`","When the input tensor to Conv3d is in the channels_last_3d memory format the Conv3d op will generate incorrect output (see example image in #141471). This PR checks if the op is 3d, and then attempts to convert the input tensor to contiguous.

Added a regression test that verifies the output by running the same op on the CPU.

I'm unsure if Conv3d supports the channels last memory format after #128393. If it does, we should consider updating the logic to utilize this as it would be more efficient. Perhaps @DenisVieriu97 knows or has more context?

Fixes #141471","closed","Teste de Regressão","{""open source"",Merged,ciflow/trunk,""release notes: mps"",ciflow/mps}",7,"2024-11-28 18:12:08","2024-12-02 17:34:22","2024-12-01 18:36:57","hvaara",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141780",3
"290","2702478642","2702478642","141776","[MPS] Add autocast rule for SDPA","Fixes #141774


cc @mcarilli @ptrblck @leslie-fang-intel @jgong5","closed","Refatoração","{""open source"",""module: amp (automated mixed precision)"",Merged,""topic: bug fixes"",""release notes: mps"",ciflow/mps}",4,"2024-11-28 15:11:31","2024-11-30 23:48:31","2024-11-29 03:34:06","hvaara",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141776",1
"291","2713748736","2713748736","141928","[FP8] Expand MaskedSelect to float8","Needed for printing those.
Though I wonder if better solution would be to change those ops to use element size rather than actual type (to extend them automatically to unsigned integral types as well)

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10","closed","Refatoração","{""module: cpu"",Merged,ciflow/trunk,""release notes: python_frontend"",""topic: improvements""}",7,"2024-12-03 01:22:43","2024-12-12 22:30:11","2024-12-03 15:14:29","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141928",0
"292","2713661077","2713661077","141925","[CI] Add more tests to the numpy 2 CI","Related to #107302 

This PR adds all the tests that failed with NumPy 2, which all have been fixed, to the CI to test with NumPy 2 to prevent regression.","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",9,"2024-12-03 00:13:09","2024-12-13 23:28:20","2024-12-05 16:46:24","haifeng-jin",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141925",2
"294","2713632716","2713632716","141922","[64-bit Indexing][CUDA] Another 64-bit indexing fix for `distribution_elementwise_grid_stride_kernel `","Previous fix in #141613 is technically incomplete e.g.,

```
#include <iostream>

int main() {
        int a = 4456448;
        int b = 1024;
        int64_t c = a * b;
        int64_t d = (int64_t) a * b;
        std::cout << (c == d) << std::endl;
        std::cout << c << std::endl;
        std::cout << d << std::endl;
}
```

cc @ptrblck @msaroufim","closed","Refatoração","{""module: cuda"",""module: 64-bit"",""open source"",""topic: not user facing""}",3,"2024-12-02 23:46:14","2025-01-03 02:07:24","2024-12-03 00:01:43","eqy",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141922",1
"295","2713629502","2713629502","141921","Filter pattern matching tests based on ACL","There are a number of cases where pattern matching differs based on the presence of ACL, causing the tests to fail. This adds `TEST_ACL` and `skipIfACL` so that these tests can still run with different values or be entirely skipped if necessary.


cc @malfet @snadampal @milpuz01 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{triaged,""open source"",""module: arm"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/linux-aarch64}",13,"2024-12-02 23:43:08","2024-12-06 06:50:39","2024-12-06 04:19:44","Mousius",NULL,NULL,"https://github.com/pytorch/pytorch/pull/141921",4
"296","2718162337","2718162337","142051","Corrected AMSGrad max equation in Adam and AdamW","Fixes #142041","closed","Refatoração","{triaged,""open source"",Merged,ciflow/trunk,""topic: docs"",""release notes: optim""}",18,"2024-12-04 15:50:20","2024-12-06 21:56:31","2024-12-06 21:55:29","Uvi-12",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142051",2
"297","2721739104","2721739104","142187","add a basic `benchmark_many_cpu` implementation","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #142188
* __->__ #142187
* #142186
* #134089
* #133154
* #133121
* #133058
* #144315","closed","Refatoração","{""module: inductor"",ciflow/inductor}",2,"2024-12-06 00:16:16","2025-01-08 18:17:01","2025-01-08 18:17:01","nmacchioni",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142187",33
"299","2725022667","2725022667","142324","Dataloader distribute tasks to workers when in_order is False","Fixes #105203 and is a follow up PR to #141833

When `in_order` is True (the default), tasks are given out to workers in a round robin fashion. When `in_order` is False this is no longer needed, as we give up guarantees of reproducibility, and instead tasks should be given to workers that are able to perform work.
In this PR I've added tracking of the number of outstanding tasks for each worker (updated when tasks are added to their queue, and when data is returned to the main thread). When finding the next queue to add a task to, if `in_order` is False it will only add the task to the workers queue if it has fewer than `_prefetch_factor` tasks outstanding.
The current default behaviour is left as is.

Tests are also updated to assert on the worker IDs for each sample of data returned.
I've run the following to confirm they aren't flaky
```bash
for i in {1..20}; do python test/test_dataloader.py TestOutOfOrderDataLoader; done
```

cc @andrewkho @divyanshk @SsnL @VitalyFedyunin @dzhulgakov","closed","Refatoração","{""module: dataloader"",triaged,""open source"",Merged,ciflow/trunk,""release notes: dataloader""}",7,"2024-12-08 07:49:12","2025-01-22 20:15:13","2025-01-03 12:57:07","michael-diggin",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142324",26
"300","2724996763","2724996763","142323","Inconsistent description of AMSGrad with code","### 📚 The doc issue

https://github.com/pytorch/pytorch/blob/0bd7b7ae58970f565eb22766549ad9d6d2e23fe6/torch/optim/adam.py#L469-L476

In the code, the bias correction term $1-\beta_2^t$ is used after the max operation. However, [the documentation](https://pytorch.org/docs/main/generated/torch.optim.Adam.html#adam) describes that it is used before the max operation:
<img width=""254"" alt=""max operation"" src=""https://github.com/user-attachments/assets/77e6016d-b225-4aa3-b507-d8fc49d1d4fd"">


### Suggest a potential alternative/fix

<img width=""500"" alt=""amsgrad algo notes"" src=""https://github.com/user-attachments/assets/4aeedca2-457d-4bca-8f7e-a879bae45ef4"">

cc @svekars @brycebortree @sekyondaMeta @AlannaBurke @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar","closed","Refatoração","{""module: docs"",""module: optimizer"",triaged}",6,"2024-12-08 07:01:16","2024-12-19 16:24:22","2024-12-19 16:24:22","Tony-Y",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142323",11
"301","2728766343","2728766343","142449","[TorchGen] remove remove_non_owning_ref_types from valuetype_type","It is not used","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",suppress-bc-linter}",11,"2024-12-10 02:21:19","2025-01-04 03:14:04","2024-12-12 00:15:47","cyyever",NULL,NULL,"https://github.com/pytorch/pytorch/pull/142449",2
"349","2776226574","2776226574","144425","Link to transformer tutorial in transformer docs","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #144425

<img width=""1045"" alt=""Screenshot 2025-01-08 at 4 50 20 PM"" src=""https://github.com/user-attachments/assets/05adfecb-8a23-4c48-9a2c-50c5b3f886b0"" />","closed","Refatoração","{Merged,ciflow/trunk,""release notes: nn"",""topic: docs""}",5,"2025-01-08 20:02:42","2025-01-09 18:55:55","2025-01-09 17:42:11","mikaylagawarecki",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144425",1
"302","2728719141","2728719141","142448","[DataParallel] Skip for MPS device","As `torch._C._scatter` is only defined for CUDA/ROCm (and may be XPU?)

This is a regression introduced by https://github.com/pytorch/pytorch/pull/141098 that went unnoticed due to https://github.com/pytorch/pytorch/issues/142206

Test plan:
```
python test_autograd.py -v -k test_dataparallel_saved_tensors_hooks
```

Before this change it failed with
```
ERROR: test_dataparallel_saved_tensors_hooks (__main__.TestMultithreadAutograd.test_dataparallel_saved_tensors_hooks)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/malfet/git/pytorch/pytorch/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
    ~~~~~~^^^^^^^^^^^^^^^^^
  File ""/Users/malfet/git/pytorch/pytorch/test/test_autograd.py"", line 13074, in test_dataparallel_saved_tensors_hooks
    model = torch.nn.DataParallel(Model())
  File ""/Users/malfet/git/pytorch/pytorch/torch/nn/parallel/data_parallel.py"", line 153, in __init__
    raise RuntimeError(""no available devices were found"")
RuntimeError: no available devices were found
```

After this change it passes


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Teste de Regressão","{""oncall: distributed"",Merged,ciflow/trunk,""release notes: dataloader"",""topic: bug fixes""}",4,"2024-12-10 01:41:00","2025-01-21 19:40:21","2024-12-10 02:49:26","malfet",NULL,"2.6.0","https://github.com/pytorch/pytorch/pull/142448",0
"304","2731258772","2731258772","142579","DISABLED test_vmap_exhaustive_mul_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_mul_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive_mul_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_mul_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 25467904 on device 0. CUDA driver allocated memory was 192151552 and is now 234094592.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_mul_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 21:47:13","2024-12-19 20:21:33","2024-12-19 20:21:33","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142579",9
"306","2731290139","2731290139","142675","DISABLED test_op_has_batch_rule_special_zeta_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_op_has_batch_rule_special_zeta_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_op_has_batch_rule_special_zeta_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_special_zeta_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 25467904 on device 0. CUDA driver allocated memory was 230948864 and is now 231997440.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_special_zeta_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 22:01:25","2024-12-19 20:16:07","2024-12-19 20:16:07","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142675",9
"350","2776109172","2776109172","144418","[ONNX] Avoid overwriting overlapped decomposed functions","Fixes #141770 

The decomposed function in `torch.export.default_decompositions().items()` is overwritten by `torch._decomp.decomposition_table`. As from `torch.onnx.export()` perspective, we should rather respect the table of decompositions in `torch.export.default_decompositions().items()` and avoid overwriting it with `torch._decomp.decomposition_table`.","closed","Refatoração","{""open source"",""release notes: onnx""}",1,"2025-01-08 19:00:24","2025-01-10 18:44:24","2025-01-10 18:44:24","pytorchbot",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144418",2
"307","2731289223","2731289223","142674","DISABLED test_vmap_exhaustive_lerp_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_lerp_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive_lerp_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_lerp_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 17920 on device 0. CUDA driver allocated memory was 190054400 and is now 192151552.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_lerp_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",3,"2024-12-10 22:01:00","2024-12-19 20:16:07","2024-12-19 20:16:07","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142674",9
"308","2731289180","2731289180","142673","DISABLED test_vmap_exhaustive___rmod___cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive___rmod___cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive___rmod___cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive___rmod___cuda_float32! Caching allocator allocated memory was 0 and is now reported as 25467904 on device 0. CUDA driver allocated memory was 191102976 and is now 233046016.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive___rmod___cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 22:00:59","2024-12-19 20:16:07","2024-12-19 20:16:06","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142673",9
"310","2731325787","2731325787","142779","DISABLED test_vmap_exhaustive_sqrt_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_sqrt_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive_sqrt_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_sqrt_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 15758336 on device 0. CUDA driver allocated memory was 193200128 and is now 214171648.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_sqrt_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 22:21:05","2024-12-19 20:11:59","2024-12-19 20:11:59","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142779",9
"311","2731325095","2731325095","142772","DISABLED test_vmap_exhaustive_pow_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_pow_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34194787540).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 5 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive_pow_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_pow_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 25467904 on device 0. CUDA driver allocated memory was 213123072 and is now 234094592.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_pow_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-10 22:20:48","2024-12-19 20:11:56","2024-12-19 20:11:56","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142772",9
"312","2733721821","2733721821","142905","DISABLED test_op_has_batch_rule_flip_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_op_has_batch_rule_flip_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34248100017).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_op_has_batch_rule_flip_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_flip_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 5120 on device 0. CUDA driver allocated memory was 171180032 and is now 173277184.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_op_has_batch_rule_flip_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-11 18:45:42","2024-12-19 20:14:16","2024-12-19 20:14:15","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142905",8
"335","2759446630","2759446630","143851","[CI] Disable sccache for xpu test","WA for https://github.com/pytorch/pytorch/issues/143585","closed","Refatoração","{""open source"",Merged,""topic: not user facing"",ciflow/xpu}",3,"2024-12-26 08:12:49","2024-12-26 19:46:10","2024-12-26 19:45:06","chuanqi129",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143851",0
"313","2733721411","2733721411","142900","DISABLED test_vmap_exhaustive_t_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_t_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34248100017).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive_t_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_t_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 1536 on device 0. CUDA driver allocated memory was 187957248 and is now 190054400.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_t_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-11 18:45:27","2024-12-19 20:14:14","2024-12-19 20:14:14","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142900",8
"314","2733758651","2733758651","143002","DISABLED test_vmap_exhaustive_isfinite_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_isfinite_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34248100017).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive_isfinite_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_isfinite_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 15754752 on device 0. CUDA driver allocated memory was 190054400 and is now 208928768.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_isfinite_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-11 19:05:07","2024-12-19 20:10:51","2024-12-19 20:10:51","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/143002",8
"315","2733758557","2733758557","143001","DISABLED test_vmap_exhaustive_asinh_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_asinh_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34248100017).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive_asinh_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_asinh_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 15758336 on device 0. CUDA driver allocated memory was 185860096 and is now 208928768.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_asinh_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-11 19:05:04","2024-12-19 20:10:51","2024-12-19 20:10:51","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/143001",8
"316","2733758477","2733758477","143000","DISABLED test_vmap_exhaustive_logical_xor_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_logical_xor_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34248100017).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive_logical_xor_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_logical_xor_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 25462784 on device 0. CUDA driver allocated memory was 209977344 and is now 230948864.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_logical_xor_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-11 19:05:01","2024-12-19 20:10:51","2024-12-19 20:10:51","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/143000",8
"317","2733758397","2733758397","142999","DISABLED test_vmap_exhaustive_tile_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)","Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_tile_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/34248100017).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_vmap_exhaustive_tile_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 3107, in wrapper
    with policy():
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2489, in __exit__
    raise RuntimeError(msg)
RuntimeError: CUDA driver API confirmed a leak in __main__.TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_tile_cuda_float32! Caching allocator allocated memory was 0 and is now reported as 13824 on device 0. CUDA driver allocated memory was 187957248 and is now 190054400.

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_CUDA_MEM_LEAK_CHECK=1 python test/functorch/test_vmap.py TestVmapOperatorsOpInfoCUDA.test_vmap_exhaustive_tile_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `functorch/test_vmap.py`

cc @clee2000 @wdvr @zou3519 @Chillee @samdow @kshitij12345","closed","Refatoração","{triaged,""module: flaky-tests"",skipped,""module: functorch""}",1,"2024-12-11 19:04:58","2024-12-19 20:10:50","2024-12-19 20:10:50","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/142999",8
"318","2736339258","2736339258","143120","Fix search icon","Removing:

.pytorch-left-menu-search input[type=text] {
    background-image: none;
}
so that the search icon correctly appears in the sphinx searchbox

Also, fixing scrolling

cc @brycebortree @sekyondaMeta @AlannaBurke","closed","Refatoração","{""open source""}",1,"2024-12-12 16:16:01","2025-01-18 02:02:07","2024-12-17 23:11:48","pytorchbot",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143120",5
"322","2739403209","2739403209","143246","UNSTABLE slow / linux-focal-rocm6.2-py3.10 / test (slow)","Network issue on ROCM runners is causing all the download there to fail

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @seemethere @malfet @pytorch/pytorch-dev-infra","closed","Refatoração","{""module: rocm"",""module: ci"",triaged,unstable}",4,"2024-12-14 00:27:31","2024-12-24 07:46:11","2024-12-24 07:46:10","huydhn",NULL,NULL,"https://github.com/pytorch/pytorch/issues/143246",10
"323","2739397070","2739397070","143242","ROCm SDPA: Ensure attn_mask has the same dtype with q","This is required by current AOTriton's backend.

Fixes NaN when calling SDPA ME backend with `q.dtype() != attn_mask.dtype()` when training llama2 using transformers+deepspeed+pytorch

Corresponding CUDA check seems to be here: 
https://github.com/pytorch/pytorch/blob/708ce3c0082d670d9eaff84bc3c43cad4554a75d/aten/src/ATen/native/transformers/cuda/attention.cu#L1331-L1336

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd","closed","Refatoração","{""module: rocm"",""open source"",Merged,""topic: not user facing"",ciflow/rocm}",17,"2024-12-14 00:17:26","2025-01-08 16:28:54","2025-01-08 15:20:28","xinyazhang",NULL,"2.6.0","https://github.com/pytorch/pytorch/pull/143242",25
"326","2745568705","2745568705","143394","[CD] Test that all PyTorch wheels support OpenMP","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #143395
* __->__ #143394
* #143393

Together with https://github.com/pytorch/pytorch/pull/143393 fixes https://github.com/pytorch/pytorch/issues/123225","closed","Refatoração","{Merged,""topic: not user facing"",ciflow/binaries_wheel}",3,"2024-12-17 17:27:50","2025-01-18 02:04:59","2024-12-18 02:27:59","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143394",1
"329","2753681638","2753681638","143695","[ROCm] CK Flash Attention Backend","Replace https://github.com/pytorch/pytorch/pull/138947 for re-import. 

Replaces https://github.com/ROCm/pytorch/pull/1592

This PR contains the initial implementation of SDPA with composable_kernel backend. The CK path can be forced by simply calling torch.backends.cuda.preferred_rocm_fa_library(""ck""). Similarly, you can force the incumbent aotriton implementation by passing in ""aotriton"" or ""default"". As you'd expect, not setting this option will result in aotriton to be used as the backend. In the case of CK, if pytorch deems flash attention usable, then it will use the CK path in all the same places aotriton would have been used. This PR makes no changes to the heuristics which select which attention scheme to use (i.e. flash attention vs memory efficient attention vs math etc etc). It only gets called when flash attention is both enabled (via USE_FLASH_ATTENTION) and is selected at runtime by the existing heuristics.

Files located in pytorch/aten/src/ATen/native/transformers/hip/flash_attn/ck/mha* have been pulled from https://github.com/Dao-AILab/flash-attention courtesy of @tridao's hard work who is the co-author

NOTE: In order to use this backend, the user MUST set USE_CK_FLASH_ATTENTION=1 in their environment when they build PyTorch.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @albanD @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{""module: rocm"",Merged,ciflow/trunk,""release notes: distributed (c10d)"",""topic: not user facing"",skip-pr-sanity-checks,""module: dynamo"",ciflow/inductor,ciflow/rocm}",14,"2024-12-21 01:11:02","2025-01-06 04:23:32","2025-01-03 22:01:40","xw285cornell",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143695",13
"336","2759384359","2759384359","143849","Refine CUDA Stream priority","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #143849
* #143799
* #141123
* #141119
* #142347

# Motivation
As mentioned in https://github.com/pytorch/pytorch/pull/141119#discussion_r1897480515, we properly handle the priority value if it is outside of the priority range.

# Additional Context
If the value falls outside of the allowed priority range, it will automatically be mapped to the nearest valid priority(either lowest or highest).","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""release notes: cuda"",""topic: improvements""}",4,"2024-12-26 07:11:34","2024-12-31 11:17:16","2024-12-31 11:17:08","guangyey",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143849",5
"337","2759145849","2759145849","143842","subgraph rewriter supports matched pattern with no users","Fixes #143841


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""release notes: fx"",fx}",8,"2024-12-26 00:47:55","2024-12-27 12:46:44","2024-12-27 12:45:42","YangQun1",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143842",1
"354","2781252749","2781252749","144588","Avoid data-dependent errors in NJT tests via capture_scalar_outputs=True","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #144889
* __->__ #144588
* #144587
* #144586

Part of my BE project addressing NJT bugs surfaced via OpInfo tests.

There are several xfails related to data-dependent errors in torch.compile. This PR sets `torch._dynamo.config.capture_scalar_outputs=True` to avoid these, which tends to exercise unbacked SymInt logic and will require `torch._check()`-related fixes.","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing""}",3,"2025-01-10 21:51:49","2025-01-24 22:46:09","2025-01-24 22:45:03","jbschlosser",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144588",14
"338","2763815395","2763815395","144007","[ROCm] fix torch.layer_norm invalid configuration problem when input is large tensor","Fixes #136291

This PR is to fix the `invalid configuration argument` problem happened on ROCm when input is a large tensor when calling `torch.layer_norm`.

```
 File ""/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/functional.py"", line 2573, in layer_norm
    return torch.layer_norm
RuntimeError: HIP error: invalid configuration argument
```

After investigation, I found that the reason why this error happened is: The amd compute language runtime checks whether  `gridDim.x * blockDim.x` is greater than `std::numeric_limits<uint32_t>::max()` or not. If yes, it will error out with the ""invalid configuration argument"" message.
  
The fix is to split the whole task to several chunks so that each chunk will not trigger the failure condition. This will ensure the correctness and completeness given the current kernel implementation logic of `vectorized_layer_norm_kernel`.

Also added a largeTensor layer_norm unit test `test_layer_norm_large_tensor` with the same shape `[16, 3000, 3000, 16]` as the one used by the pytorch issue #136291 so that the unit test can check the expected output value to ensure correctness. 

The future work may include performance optimization of layer_norm and CK layer_norm integration.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @naromero77amd","closed","Refatoração","{""module: rocm"",""open source"",Merged,ciflow/trunk,""release notes: cuda"",ciflow/rocm}",5,"2024-12-31 00:00:36","2025-01-07 19:18:08","2025-01-07 19:17:06","hongxiayang",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144007",7
"351","2776100913","2776100913","144417","[ONNX] Handle list values as 0d inputs","Handle list values as 0d inputs instead of 1d, as the `SymInt`s are expected to be 0d tensors in ONNX.

This PR reshapes int64 values into 1D tensors in a list, assuming they are 0D tensors initially.","closed","Refatoração","{""open source"",""release notes: onnx""}",1,"2025-01-08 18:55:42","2025-01-10 18:43:39","2025-01-10 18:43:39","pytorchbot",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144417",2
"339","2763769352","2763769352","144006","[inductor] Add missing py312 xfail","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #144006

See #144006

```py
__________________________________________ CudaReproTests.test_repeated_masked_load __________________________________________
RuntimeError: First class dim doesn't work with python 3.12

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/jansel/conda/envs/pytorch/lib/python3.12/unittest/case.py"", line 58, in testPartExecutor
    yield
  File ""/home/jansel/conda/envs/pytorch/lib/python3.12/unittest/case.py"", line 634, in run
    self._callTestMethod(testMethod)
  File ""/home/jansel/conda/envs/pytorch/lib/python3.12/unittest/case.py"", line 589, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File ""/home/jansel/pytorch/torch/testing/_internal/common_utils.py"", line 3108, in wrapper
    method(*args, **kwargs)
  File ""/home/jansel/pytorch/test/inductor/test_cuda_repro.py"", line 1678, in test_repeated_masked_load
    from functorch.einops import rearrange
  File ""/home/jansel/pytorch/functorch/einops/__init__.py"", line 1, in <module>
    from .rearrange import rearrange
  File ""/home/jansel/pytorch/functorch/einops/rearrange.py"", line 7, in <module>
    from functorch._C import dim as _C
ImportError: initialization failed
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",3,"2024-12-30 22:48:20","2024-12-31 23:38:12","2024-12-31 23:37:07","jansel",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144006",1
"340","2763769278","2763769278","144005","[tp] propagate src_data_rank kwarg in TP API","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #144005
* #143883

as titled, this PR propagates the src_data_rank in the TP API, so that
module level APIs could leverage the flexibility to choose
src_data_rank, and avoid the communication if it does not need to

cc @H-Huang @awgu @kwen2501 @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,ciflow/trunk,""release notes: distributed (dtensor)""}",4,"2024-12-30 22:48:14","2025-01-02 05:37:00","2025-01-02 05:35:56","wanchaol",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144005",3
"341","2763617445","2763617445","143998","[MPSInductor] Fix multiple kernel generation","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #143966
* __->__ #143998
* #143977
* #143973
* #143949
* #143948

At the moment by generating multiple MetalLibraries

`pytest test/inductor/test_torchinductor.py -k _mps` score is 434 failed, 317 passed, 32 skipped

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,""topic: not user facing"",ciflow/mps,""module: inductor"",ciflow/inductor}",3,"2024-12-30 19:33:40","2024-12-31 13:52:58","2024-12-31 13:51:54","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/143998",1
"342","2767918290","2767918290","144155","[ez] Use strip for arg sanitization in upload_metadata_file to improve readability","Minor thing that improves readability.  I didn't realize you could specify characters for strip when I wrote this","closed","Refatoração","{Merged,""topic: not user facing""}",3,"2025-01-03 17:21:12","2025-01-03 19:26:34","2025-01-03 19:25:32","clee2000",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144155",0
"344","2767606919","2767606919","144151","fix a bug for fft c2c stride in cpu","Fixes #144150

Please see details in the issue.


cc @mruberry","closed","Refatoração","{triaged,""open source"",""module: fft""}",5,"2025-01-03 13:48:04","2025-01-08 09:56:24","2025-01-08 09:56:24","ywq880611",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144151",5
"345","2767154255","2767154255","144145","[reland][attempt2][AMD] Turn on TF32 for aten::mm","Summary:
https://github.com/pytorch/pytorch/pull/143549 was reverted due to some
internal/oss tooling issue. Relanding.

hipblaslt supports TF32, so adding the support.
Original PR https://github.com/pytorch/pytorch/pull/139869

Test Plan: CI

Differential Revision: D67785496




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""topic: not user facing"",""module: dynamo"",ciflow/inductor,ci-no-td}",5,"2025-01-03 08:11:45","2025-01-06 00:38:06","2025-01-06 00:37:03","xw285cornell",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144145",3
"346","2771443024","2771443024","144281","[MPSInductor] Add `nan` constant generation","If val is not equal to self, it's a nan (which is spelled as `NAN` in Metal)


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,""topic: not user facing"",ciflow/mps,""module: inductor"",ciflow/inductor}",3,"2025-01-06 21:03:22","2025-01-06 22:14:27","2025-01-06 22:13:26","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144281",0
"348","2776439263","2776439263","144430","Request for: torch-2.5.1-cp313-cp313-win_amd64.whl","I don't see the posting for 313 there.

I am guessing the Mac people would be interested as well?","closed","Refatoração","{}",1,"2025-01-08 22:07:00","2025-01-09 22:33:44","2025-01-09 22:33:44","tlh45342",NULL,NULL,"https://github.com/pytorch/pytorch/issues/144430",1
"359","2791393947","2791393947","144924","[MPSInductor] Fix codegen regression","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #144924
* #144917

Caused by https://github.com/pytorch/pytorch/pull/144649

Do not try to insert anything into the header if wrapper is not ready yet

Fixes `test_sort_mps`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Teste de Regressão","{Merged,""topic: not user facing"",ciflow/mps,""module: inductor"",ciflow/inductor}",3,"2025-01-16 01:00:40","2025-01-16 02:13:52","2025-01-16 02:12:46","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144924",0
"360","2791393380","2791393380","144922","DISABLED test_repeat_graph_capture_cublas_workspace_memory (__main__.TestCuda)","Platforms: rocm

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_repeat_graph_capture_cublas_workspace_memory&suite=TestCuda&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/35670628070).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 6 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_repeat_graph_capture_cublas_workspace_memory`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.



<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File ""/var/lib/jenkins/pytorch/test/test_cuda.py"", line 2048, in test_repeat_graph_capture_cublas_workspace_memory
    self.assertFalse(used_gb_before + 0.1 < used_gb_after)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/unittest/case.py"", line 681, in assertFalse
    raise self.failureException(msg)
AssertionError: True is not false

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test/test_cuda.py TestCuda.test_repeat_graph_capture_cublas_workspace_memory

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>


Test file path: `test_cuda.py`

cc @ptrblck @msaroufim @eqy @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @clee2000 @wdvr","closed","Refatoração","{""module: cuda"",""module: rocm"",triaged,""module: flaky-tests"",skipped}",4,"2025-01-16 01:00:24","2025-01-30 03:41:58","2025-01-30 03:41:58","pytorch-bot[bot]",NULL,NULL,"https://github.com/pytorch/pytorch/issues/144922",14
"361","2791338736","2791338736","144915","Update clickhouse-connect to 0.8.14","Corresponds to https://github.com/pytorch/test-infra/pull/6177

I only tested the slow test script but I also did testing on the new version with scripts in https://github.com/pytorch/test-infra/pull/6177","closed","Refatoração","{Merged,""topic: not user facing""}",3,"2025-01-16 00:30:57","2025-01-21 21:44:24","2025-01-21 21:43:20","clee2000",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144915",5
"362","2791283544","2791283544","144912","DISABLED test_flex_attention (__main__.TestCompiledAutograd)","Platforms: rocm

This test was disabled because it is failing on main branch ([recent examples](https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_compiled_autograd.py%3A%3ATestCompiledAutograd%3A%3Atest_flex_attention%22%5D)).

Caused by this PR: https://github.com/pytorch/pytorch/pull/144533

The MI200 CI runners were passing all inductor UTs prior to merge.  Post-merge on MI300 we see this failure.  Hopefully just the one test.

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @chauhang @penguinwu @zou3519 @ydwu4 @xmfan @yf225 @bdhirsh @Chillee @drisspg @yanboliang @BoyuanFeng","closed","Refatoração","{""module: rocm"",triaged,skipped,""oncall: pt2"",""module: higher order operators"",""module: compiled autograd"",""module: pt2-dispatcher"",""module: flex attention""}",1,"2025-01-16 00:01:44","2025-01-18 02:47:15","2025-01-18 02:47:15","jeffdaily","xmfan",NULL,"https://github.com/pytorch/pytorch/issues/144912",2
"363","2791263757","2791263757","144911","[Submodule] Upgrade to Cutlass 3.6 part deux","# Summary
Take 2 of [D67866269](https://www.internalfb.com/diff/D67866269)
Main change is that we identified and fixed the FA2 regression. More details can be found here https://github.com/pytorch/pytorch/issues/144729 and have landed that before this here: [D68194635](https://www.internalfb.com/diff/D68194635)

Differential Revision: D68194470","closed","Teste de Regressão","{fb-exported,Merged,ciflow/trunk,""release notes: sparse""}",7,"2025-01-15 23:51:01","2025-01-17 00:54:46","2025-01-17 00:53:44","drisspg",NULL,NULL,"https://github.com/pytorch/pytorch/pull/144911",2
"364","2791255925","2791255925","144910","cpp_wrapper: Properly handle scalars when input to tensor arguments","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145095
* __->__ #144910

Additionally, reduce code duplication in `cpp_wrapper_cpu_array_ref.py`.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",3,"2025-01-15 23:46:42","2025-01-24 02:07:42","2025-01-24 02:06:38","benjaminglass1","benjaminglass1",NULL,"https://github.com/pytorch/pytorch/pull/144910",9
"366","2795951958","2795951958","145086","`torch.distributions`: replace `numbers.Number` with `torch.types.Number`.","Fixes #144788 (partial)


cc @fritzo @neerajprad @alicanb @nikitaved @ezyang @malfet @xuzhao9 @gramster","closed","Refatoração","{""module: distributions"",""module: typing"",triaged,""open source"",Merged,ciflow/trunk,""release notes: python_frontend""}",7,"2025-01-17 17:18:37","2025-01-29 05:34:17","2025-01-27 20:24:58","randolf-scholz",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145086",10
"367","2801803076","2801803076","145270","[NVIDIA] RTX50 Blackwell Support codegen","cc @ptrblck @msaroufim @eqy","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""release notes: cuda"",""topic: new features""}",6,"2025-01-21 13:05:55","2025-01-21 21:11:10","2025-01-21 21:10:07","johnnynunez",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145270",0
"368","2801785959","2801785959","145269","Improve the caching allocator test for raw alloc","1 Prevent block allocated by torch._C._cuda_cudaCachingAllocator_raw_alloc from affecting torch.cuda.empty_cache() in other unit tests
2 Additionally, tested the changes to raw_delete in https://github.com/pytorch/pytorch/pull/131114   

@jeffdaily @albanD @houseroad @eqy @aaronenyeshi","closed","Refatoração","{""open source"",Merged,ciflow/trunk,""topic: not user facing""}",31,"2025-01-21 12:58:56","2025-01-24 21:08:21","2025-01-24 21:07:19","1274085042",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145269",3
"369","2805592336","2805592336","145431","[dynamo] Re-enable `test_torch_name_rule_map_updated`","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145431

This patch re-enables `test_torch_name_rule_map_updated` and adds
relevant fixes for the failures.

Fixes #114831.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames","closed","Refatoração","{""topic: not user facing"",""module: dynamo"",ciflow/inductor}",2,"2025-01-23 00:10:08","2025-01-25 00:02:53","2025-01-25 00:02:52","StrongerXi",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145431",2
"370","2805589977","2805589977","145430","[c10/metal] Add a vectype variant for `short`/`int`/`long`","Some of the kernels (exp_complex/atan_complex) need the specialization.

cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{Merged,""topic: not user facing"",""module: mps"",ciflow/mps,""module: inductor""}",4,"2025-01-23 00:08:12","2025-01-23 04:54:00","2025-01-23 04:52:58","dcci",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145430",0
"371","2805583543","2805583543","145428","torch._neg_view correctness","### 🐛 Describe the bug

```
import torch

device = ""cuda""
dtype = torch.float64

# Basic test for negative view
x = torch.randn(20, 20, device=device, dtype=dtype, requires_grad=False)
physical_neg = torch.neg(x)
view_neg = torch._neg_view(x)

assert torch.is_neg(view_neg), ""view_neg should be negative""
assert not torch.is_neg(x), ""x should not be negative""
assert torch.allclose(
    physical_neg, view_neg
), ""physical_neg and view_neg should be equal""

# Test in-place operations on negative view
x = torch.randn(20, 20, device=device, dtype=dtype, requires_grad=False)
neg_x = torch._neg_view(x)
neg_x.add_(1.0)
assert torch.is_neg(neg_x), ""neg_x should still be negative after in-place operation""
expected = -x + 1.0
assert torch.allclose(neg_x, expected), ""neg_x should match expected result""
```
The output of the above tests is 
```
% TORCH_LOGS=+inductor,dynamo python test_neg_view.py
Traceback (most recent call last):
  File ""test_neg_view.py"", line 23, in <module>
    assert torch.allclose(neg_x, expected), ""neg_x should match expected result""
AssertionError: neg_x should match expected result
```
I'm curious if this is expected result? 

possible related failed ci tests https://github.com/pytorch/pytorch/pull/145127 . 
cc @shunting314 @eellison @zou3519 @masnesral 

### Versions

nightly","closed","Refatoração","{""module: correctness (silent)"",""topic: not user facing""}",1,"2025-01-23 00:04:08","2025-01-23 00:28:47","2025-01-23 00:28:30","FindHao",NULL,NULL,"https://github.com/pytorch/pytorch/issues/145428",0
"372","2805533573","2805533573","145423","Implement deepcopy for AOTICompiledModel","Summary: 

Fix https://github.com/pytorch/pytorch/issues/145411

Support deepcopying AOTICompiledModel. The `loader` is shallow copied.

Test Plan:
```
buck2 run fbcode//mode/opt //caffe2/test/inductor:aot_inductor_package -- -r deepcopy
```

Differential Revision: D68524673




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Refatoração","{fb-exported,Merged,ciflow/trunk,""topic: not user facing"",""module: inductor"",ciflow/inductor}",6,"2025-01-22 23:18:42","2025-01-23 21:06:36","2025-01-23 21:05:34","yushangdi",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145423",1
"373","2805522917","2805522917","145421","[cp] override compute_log_sumexp to True for aten._scaled_dot_product_efficient_attention.default if False","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145421

## Description
Our current CP doesn't support efficient attention when `compute_log_sumexp=False`. `compute_log_sumexp=False` only if that `requires_grad=False` and since PP's [shape inference](https://github.com/pytorch/pytorch/blob/d95a6babcc581ff06d1b914ee9f92c81b2e850e2/torch/distributed/pipelining/stage.py#L1387) happens under `torch.no_grad()` context , we need to override `compute_log_sumexp` to `True` in our CP attention implementation.

## Test
- Test PP+FSDP+CP w/ `mixed_precision = ""float32""` in torchtitan

- `pytest test/distributed/tensor/test_attention.py -s -k test_ring_attention_sdpa`

Before:
<img width=""1880"" alt=""image"" src=""https://github.com/user-attachments/assets/872ff583-295e-4751-a280-cf7f2d41c61a"" />

After:
<img width=""2988"" alt=""image"" src=""https://github.com/user-attachments/assets/4bdcc2e5-22a5-427a-91a5-82206d5bd78f"" />


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed","Refatoração","{""oncall: distributed"",Merged,ciflow/trunk,""topic: not user facing"",ciflow/inductor,""module: context parallel""}",5,"2025-01-22 23:08:25","2025-01-24 06:19:00","2025-01-24 06:17:57","XilunWu",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145421",2
"375","2813952858","2813952858","145769","Revert D68232274","Summary:
This diff reverts D68232274
broken multiple tests - T213563826

Test Plan: NA

Differential Revision: D68725051","closed","Refatoração","{fb-exported,ciflow/inductor,""release notes: export""}",3,"2025-01-27 20:09:50","2025-01-27 20:11:31","2025-01-27 20:11:31","avikchaudhuri",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145769",0
"376","2813935628","2813935628","145767","Spammy Aot autograd cache warning","### 🐛 Describe the bug

```with-proxy python benchmarks/dynamo/torchbench.py --backend inductor --device cuda --only
 basic_gnn_edgecnn --amp --cold-start-latency --print-compilation-time --training --performance 2>&1
```

Gives

```
loading model: 0it [00:02, ?it/s]
cuda train basic_gnn_edgecnn                  
[WARNING]:Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[WARNING]:Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[WARNING]:Bypassing autograd cache due to: Cannot cache a graph with functional tensor
```

We're not similarly raising a warning with fx graph cache bypasses. I think this is a bit spammy but open to other opinions. It also is not user actionable. cc @ezyang @masnesral @jamesjwu 

### Versions

master","closed","Refatoração","{}",0,"2025-01-27 20:00:00","2025-01-28 19:25:39","2025-01-28 19:25:39","eellison","jamesjwu",NULL,"https://github.com/pytorch/pytorch/issues/145767",1
"377","2813903406","2813903406","145762","[export] fix non-strict pre_dispatch exporting while_loop","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145762

fix https://github.com/pytorch/pytorch/issues/145737.

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv","closed","Refatoração","{Merged,ciflow/trunk,""release notes: fx"",fx,keep-going}",6,"2025-01-27 19:41:50","2025-01-30 18:59:41","2025-01-30 18:58:37","ydwu4",NULL,NULL,"https://github.com/pytorch/pytorch/pull/145762",3
"378","2819753071","2819753071","146002","[ONNX] Delete `rename_dynamic_shapes_with_model_inputs`","Basically, this function brings more cons than pros. 

It was nice to have an automation help users to convert top-level key of dynamic shapes to arg names. However, this function has a bug when the model input has the same amount as dynamic_shapes in coincidence:

```python
input_names
# 'input_ids', 'past_key_values.0.key', 'past_key_values.0.value', 'past_key_values.1.key', 'past_key_values.1.value', 'past_key_values.2.key', 'past_key_values.2.value', 'past_key_values.3.key', 'past_key_values.3.value', 'past_key_values.4.key', 'past_key_values.4.value', 'attention_mask', 'position_ids'

inspect.sig(model.forward).parameters
# mappingproxy(OrderedDict([('input_ids', <Parameter ""input_ids: Optional[torch.LongTensor] = None"">), ('past_key_values', <Parameter ""past_key_values: Union[transformers.cache_utils.Cache, Tuple[Tuple[torch.Tensor]], NoneType] = None"">), ('attention_mask', <Parameter ""attention_mask: Optional[torch.FloatTensor] = None"">), ('token_type_ids', <Parameter ""token_type_ids: Optional[torch.LongTensor] = None"">), ('position_ids', <Parameter ""position_ids: Optional[torch.LongTensor] = None"">), ('head_mask', <Parameter ""head_mask: Optional[torch.FloatTensor] = None"">), ('inputs_embeds', <Parameter ""inputs_embeds: Optional[torch.FloatTensor] = None"">), ('labels', <Parameter ""labels: Optional[torch.LongTensor] = None"">), ('use_cache', <Parameter ""use_cache: Optional[bool] = None"">), ('output_attentions', <Parameter ""output_attentions: Optional[bool] = None"">), ('output_hidden_states', <Parameter ""output_hidden_states: Optional[bool] = None"">), ('return_dict', <Parameter ""return_dict: Optional[bool] = None"">), ('cache_position', <Parameter ""cache_position: Optional[torch.LongTensor] = None"">)]))
```

In the above case, the given input_names is following onnx graph, while it has the same length as torch model forward call. This kind of case makes it difficult to detect, and automate for users.

On the other hand, the error message from torch.export.export is quite informative that I believe users will know how to go from there:

```python

import torch


class Model(torch.nn.Module):
    def forward(self, x=None, y=None):
        return x + y

dim = torch.export.Dim(""x"", min=1, max=6)
onnx_program = torch.export.export(
    Model(),
    (),
    kwargs={""x"": torch.randn(2, 3), ""y"": torch.randn(2, 3)},
    dynamic_shapes={""custom_input_x"": {0: dim}, ""custom_input_y"": {0: dim}},
)

# torch._dynamo.exc.UserError: When `dynamic_shapes` is specified as a dict, its top-level keys must be the arg names ['x', 'y'] of `inputs`, but here they are ['custom_input_x', 'custom_input_y']. Alternatively, you could also ignore arg names entirely and specify `dynamic_shapes` as a list/tuple matching `inputs`. For more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#dynamic-shapes-validation
```","closed","Refatoração","{""module: onnx"",""open source"",Merged,ciflow/trunk,""release notes: onnx"",""topic: bug fixes""}",6,"2025-01-30 02:23:56","2025-01-30 16:02:43","2025-01-30 16:01:41","titaiwangms",NULL,NULL,"https://github.com/pytorch/pytorch/pull/146002",0
"379","2819703893","2819703893","146001","Allow replacing unbacked with very large upperbound by returning no-op for FloorToInt(int)","* Let's say x is an integer beyond 2^53 where Python floats lose precision i.e. can't increment by 1.
* Therefore, float(x) will lose precision and won't retain the exact value of x even though it's an integer.
* That means `FloorToInt(very_large_number)` will lose precision if we cast it to float
```
>>> int(float(1000000007999999992))
1000000008000000000
```

This means when we try to do this in set_replacement():
https://github.com/pytorch/pytorch/blob/32bb6f83d5e9819560c2a074a193740c989f765d/torch/fx/experimental/symbolic_shapes.py#L6011-L6019

We run into this:
```
TORCH_LOGS=""+torch.fx.experimental.symbolic_shapes"" pytest -s test_export.py -k test_replace_unbacked_with_very_large_upperbound

  File ""/data/users/colinpeppler/pytorch/torch/fx/experimental/symbolic_shapes.py"", line 6258, in _maybe_guard_rel
    self._set_replacement(rhs, self._find(lhs), ""trivial_rhs"")
  File ""/data/users/colinpeppler/pytorch/torch/fx/experimental/symbolic_shapes.py"", line 6039, in _set_replacement
    assert tgt_bound.issubset(
torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function add>(*(FakeTensor(..., size=(2*s0,)), FakeTensor(..., size=(u0,))), **{}):
tgt_bound=VR[4, 1000000008000000000] not a subset of src_bound=VR[4, 1000000007999999992]
```



Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146001



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @SherlockNoMad @EikanWang @wenzhe-nrv","closed","Refatoração","{""module: cpu"",Merged,ciflow/trunk,""release notes: fx"",""topic: not user facing"",fx,ciflow/inductor}",15,"2025-01-30 01:37:04","2025-01-31 16:56:33","2025-01-31 00:25:22","ColinPeppler",NULL,NULL,"https://github.com/pytorch/pytorch/pull/146001",1
"380","2822175157","2822175157","146114","[fsdp2] mixed precision missing `buffer_dtype`","Hi Andrew @awgu 😊,

As a big fan of FSDP2, I found an potential missing feature in its mixed precision, when compared with your FSDP1:

Recall the mixed precision of FSDP1 has the [`buffer_dtype`](https://github.com/pytorch/pytorch/blob/e6704a2447a04349e6b021817a2bf2f601215e67/torch/distributed/fsdp/api.py#L124):

```python

@dataclass
class MixedPrecision:
    """"""
    ...
    
        buffer_dtype (Optional[torch.dtype]): This specifies the dtype for
            buffers. FSDP does not shard buffers. Rather, FSDP casts them to
            ``buffer_dtype`` in the first forward pass and keeps them in that
            dtype thereafter. For model checkpointing, the buffers are saved
            in full precision except for ``LOCAL_STATE_DICT``. (Default:
            ``None``)
    """"""
    ...
    buffer_dtype: Optional[torch.dtype] = None

```
 
However, in FSDP2, we no longer have such [`buffer_dtype`](https://github.com/pytorch/pytorch/blob/e6704a2447a04349e6b021817a2bf2f601215e67/torch/distributed/fsdp/_fully_shard/_fsdp_api.py#L9):

```python
class MixedPrecisionPolicy:
    param_dtype: Optional[torch.dtype] = None
    reduce_dtype: Optional[torch.dtype] = None
    output_dtype: Optional[torch.dtype] = None
    cast_forward_inputs: bool = True
```

In FSDP2, buffers are treated as ignored tensors and are not sharded, thus staying in the initialized precision during forward and backward, which can violate the semantics of mixed precision. E.g., 

- During initialization, a model's parameters and buffers are in `float32` and then `fully_shard`ed with `MixedPrecisionPolicy(param_dtype=bfloat16, cast_forward_inputs=True)`
- During forward, we expect this model runs in `bfloat16` for each intermediate tensors (including those tensors within a submodule)
- Indeed, parameters are unsharded in `param_dtype=bfloat16` and input to submodules are casted to `bfloat16`
- However, buffers stays in `float32` and joins forward compute with parameters and inputs (e.g., `intermediate = parameters * input + buffers`)
- Then, due to the dtype promotion, intermediate tensor (`intermediate`) will be in `float32`, so does all following intermediate tensors in this submodule (before next submodule's `cast_forward_inputs`).
- So mixed precision ends up in full precision (`float32`) compute within each submodule 

How should we solve the `buffer`? Bringing back FSDP1's `buffer_dtype` or there is more elegant design?

Looking forward 😄","closed","Refatoração","{}",3,"2025-01-31 00:52:03","2025-01-31 03:44:54","2025-01-31 03:44:54","leonardo0lyj",NULL,NULL,"https://github.com/pytorch/pytorch/issues/146114",0
"381","2821879527","2821879527","146085","[MPS] Fix regression in con-contig bitwise ops","Caused by https://github.com/pytorch/pytorch/pull/128393 that change semantic of `needsGather`, which resulted in silent correctness errors on MacOS-15+ if output tensor is non-contiguous

Fixes https://github.com/pytorch/pytorch/issues/145203","closed","Teste de Regressão","{Merged,""topic: bug fixes"",""release notes: mps"",ciflow/mps}",3,"2025-01-30 21:43:38","2025-01-30 22:38:02","2025-01-30 22:37:00","malfet",NULL,NULL,"https://github.com/pytorch/pytorch/pull/146085",0
"382","2821557769","2821557769","146058","Turn on local caches for fbcode","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Differential Revision: [D68908317](https://our.internmc.facebook.com/intern/diff/D68908317/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov","closed","Teste de Regressão","{""module: inductor"",""module: dynamo"",ciflow/inductor}",2,"2025-01-30 18:40:26","2025-01-30 18:41:24","2025-01-30 18:41:24","oulgen",NULL,NULL,"https://github.com/pytorch/pytorch/pull/146058",0
"436","2821823521","2821823521","146072","add WaitCounter type interface and get rid of type errors","Summary: as titled.","closed","Refatoração","{fb-exported,ciflow/trunk,""topic: not user facing""}",11,"2025-01-30 21:07:29","2025-01-31 17:32:38","2025-01-31 17:32:38","burak-turk",NULL,NULL,"https://github.com/pytorch/pytorch/pull/146072",1
